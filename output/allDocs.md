# Hasura PromptQL Documentation

URL: https://hasura.io/docs/promptql/


# Hasura PromptQL Documentation

<div className={'front-matter'}>
  <div>
    Talk to your all your data accurately with natural language. 
    
    PromptQL is a novel agent approach to enable high-trust LLM interaction with business data & systems. It composes 
    tool calls and LLM tasks in a way that provides a high degree of explainability, accuracy and repeatability, for 
    arbitrarily complex tasks.

    PromptQL's data layer (Hasura DDN) comes with out of the box connectors to a wide range of data sources. These
    connectors introspect your data sources and code and help you build a high-quality realtime data product.

    <ul>
      <li>Semantic metadata with relationships</li>
      <li>Fine-grained entitlements and access control</li>
    </ul>

    <Link to="/quickstart/">Quickstart.</Link>

  </div>
  <div className={'video-wrapper'}>
    <div className={'video-aspect-ratio'}>
      <iframe
        src={"https://www.youtube.com/embed/nGcf09iVQbk"}
        allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
        allowFullScreen
      />
    </div>
  </div>
</div>

## Demos

Check out these demos to get a feel for how PromptQL works. Each of these will take you to a README outlining the
connected data sources and will suggest some ways of interacting with the application.

<div className="homepage-demo-grid">
  <a href="https://promptql.console.hasura.io/public/sandbox-gtm/readme" className="demo-box" target="_blank">
  <div className="icon-wrapper">
    <Handshake className="demo-icon" />
  </div>
  <h3>GTM</h3>
  <p>
    See how sales and marketing teams leverage PromptQL to understand customer journeys, analyze pipeline data, and
    drive strategic business decisions.
  </p>
  <div class="demo-cta-button">Explore the GTM demo</div>
</a>

<a href="https://promptql.console.hasura.io/public/sandbox-healthcare/readme" className="demo-box" target="_blank">
  <div className="icon-wrapper">
    <Healthcare className="demo-icon" />
  </div>
  <h3>Healthcare</h3>
  <p>
    Discover how healthcare providers use PromptQL to analyze patient data, optimize care pathways, and improve
    operational efficiency while maintaining compliance.
  </p>
  <div class="demo-cta-button">Explore the Healthcare demo</div>
</a>

<a href="https://promptql.console.hasura.io/public/sandbox-aml/readme" className="demo-box" target="_blank">
  <div className="icon-wrapper">
    <Banking className="demo-icon" />
  </div>
  <h3>AML</h3>
  <p>
    Explore how financial institutions use PromptQL to streamline anti-money laundering (AML) efforts by analyzing
    transaction data, uncovering suspicious patterns, and enhancing compliance.
  </p>
  <div class="demo-cta-button">Explore the AML demo</div>
</a>

</div>



==============================



# quickstart.mdx

URL: https://hasura.io/docs/promptql/quickstart


# Quickstart with PromptQL

In this getting started guide, you'll get hands-on with PromptQL by deploying it and connecting to a sample dataset.
You'll use a hosted PostgreSQL database pre-loaded with IMDb movie data. You’ll be able to chat with the data to explore
information about different movies. Then, you'll add custom business logic that lets PromptQL take action — like renting
a movie on a user's behalf.


## Prerequisites

**Install the DDN CLI**

:::info Minimum version requirements

To use this guide, ensure you've installed/updated your CLI to at least `v2.28.0`.

:::


<Tabs groupId="os-preference" className="api-tabs">

<TabItem value="macOS-or-linux" label="macOS and Linux">

Simply run the installer script in your terminal:

<CodeBlock language="bash">
  {`curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/get.sh | bash`}
</CodeBlock>

<Admonition type="info" title="ARM-based Linux Machines">
  Currently, the CLI does not support installation on ARM-based Linux systems.
</Admonition>

</TabItem>

<TabItem value="windows" label="Windows">
- Download the latest <a href={`https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/latest/DDN_CLI_Setup.exe`}>DDN CLI installer for Windows.</a> 
- Run the `DDN_CLI_Setup.exe` installer file and follow the instructions. This will only take a minute.
- By default, the DDN CLI is installed under `C:\Users\{Username}\AppData\Local\Programs\DDN_CLI`
- The DDN CLI is added to your `%PATH%` environment variable so that you can use the `ddn` command from your terminal.
<br />

</TabItem>
</Tabs>


**Install [Docker](https://docs.docker.com/engine/install/)**

The Docker-based workflow helps you iterate and develop locally without deploying any changes to Hasura DDN, making the
development experience faster and your feedback loops shorter. **You'll need Docker Compose `v2.20` or later.**

**Validate the installation**

You can verify that the DDN CLI is installed correctly by running:

```ddn
ddn doctor
```


## Build your First PromptQL app


### Step 1. Authenticate your CLI

```ddn title="Before you can create a new Hasura DDN project, you need to authenticate your CLI:"
ddn auth login
```

This will launch a browser window prompting you to log in or sign up for Hasura Cloud. After you log in, the CLI will
acknowledge your login, giving you access to Hasura Cloud resources.

### Step 2. Scaffold out a new local project

```ddn title="Next, create a new local project:"
ddn supergraph init imdb-promptflix --with-promptql && cd imdb-promptflix
```

Now that you're in this directory, you'll see your project scaffolded out for you. You can view the structure by either
running `ls` in your terminal, or by opening the directory in your preferred editor.

### Step 3. Initialize your PostgreSQL connector

```ddn title="In your project directory, run:"
ddn connector init imdb -i
```

From the dropdown, select `hasura/postgres-promptql` (you can type to filter the list). Then, enter the following
connection URI to an existing PostgreSQL database of IMDb data:

```plaintext
jdbc:postgresql://35.236.11.122:5432/imdb?user=read_only_user&password=readonlyuser
```

For the `JDBC_SCHEMAS` environment variable, enter the following:

```plaintext
public
```

### Step 4. Introspect the PostgreSQL database

```ddn title="Next, use the CLI to introspect your PostgreSQL database:"
ddn connector introspect imdb
```

After running this, you should see a representation of your database's schema in the
`app/connector/imdb/configuration.json` file; you can view this using `cat` or open the file in your editor.

### Step 5. Track your tables

Based on our sample database, a SQL schema will be generated. Let's track all the models to get started quickly.

```ddn title="Run the following command:"
ddn model add imdb "*"
```

Open the `app/metadata` directory. You'll find Hasura Metadata Language (HML) files for each table in your database.  
In this case, there's only one: `public_movies.hml`.

The DDN CLI uses these HML files to represent PostgreSQL tables in your API as
[models](/reference/metadata-reference/models.mdx).

### Step 6. Create a new build

```ddn title="To create a local build, run:"
ddn supergraph build local
```

The build is stored as a set of JSON files in `engine/build`.

### Step 7. Start your local services

```ddn title="Start your local Hasura DDN Engine and PostgreSQL connector:"
ddn run docker-start
```

Your terminal will be taken over by logs for the different services.

## Talk to your data

### Step 1. Open the PromptQL Playground

```ddn title="In another terminal, run:"
ddn console --local
```

### Step 2. Ask questions about your dataset

The console is a web app hosted at [`promptql.console.hasura.io`](https://promptql.console.hasura.io) that connects to
your local PromptQL API and data sources. Your data is processed in the DDN PromptQL runtime but isn't persisted
externally.

Head over to the console and ask a few questions about your data.

```plaintext title="For example:"
Hi, what are some questions you can answer?
```

PromptQL will respond with information about the dataset and make suggestions for your first query on the data!

:::info Try it out

Go ahead and ask a question! In the next steps, we'll take a deeper look at interacting with the data.

:::

## Act on your data

### Step 1. Create a command

While it's great that we can ask questions about the data set, what if the application could take action on our behalf?
Such as rent a movie for us? We can easily do this by adding custom business logic using one of our
[lambda connectors](/business-logic/overview.mdx). In the example below, we'll use TypeScript.

```ddn title="Like before, let's initialize this connector:"
ddn connector init promptflix -i
```

From the list of choices, select `hasura/nodejs`. Like with the PostgreSQL connector, the CLI will scaffold out a set of
configuration files, including a file called `app/connector/promptflix/functions.ts` — this is where we'll add our logic
so the application can take action on our behalf.

```typescript title="Replace the content of your functions.ts file with the following:"
/*
 * This interface defines the structure of the response object returned by the
 * function that rents a movie. It includes a success status and a message.
 * The success status is a boolean indicating whether the operation was
 * successful or not.
 */
interface RentMovieResponse {
  success: boolean;
  message: string;
}

/*
 * PromptFlix - A movie rental service
 *
 * @param {string} seriesTitle - The title of the movie series to rent
 * @returns {RentMovieResponse} - The response object containing success status and message
 */
export function rentSingleMovieBySeriesTitle(seriesTitle: string): RentMovieResponse {
  console.log(`Renting movie series: ${seriesTitle}`);
  return {
    success: true,
    message: `Successfully rented the movie series: ${seriesTitle}`,
  };
}
```

This function simulates the action of renting a movie on behalf of a user. The JSDoc comments aid in providing context
to the underlying LLM so that the application knows how to use the function.

```ddn title="Introspect your connector:"
ddn connector introspect promptflix
```

```ddn title="Add your function as a command:"
ddn command add promptflix "*"
```

Before we create a new build, let's add a description to the metadata for our command located in
`app/metadata/rent_single_movie_by_series_title.hml`

```yaml title="Open the newly-generated rent_single_movie_by_series_title.hml file and add the following description:" {6-7}
---
kind: Command
version: v1
definition:
  name: rent_single_movie_by_series_title
  description:
    This function allows users to rent movies from PromptFlix and should be used for any request to rent a movie.
  outputType: rent_movie_response!
  arguments:
    - name: series_title
      type: String!
  source:
    dataConnectorName: promptflix
    dataConnectorCommand:
      procedure: rentSingleMovieBySeriesTitle
    argumentMapping:
      series_title: seriesTitle
  graphql:
    rootFieldName: rent_single_movie_by_series_title
    rootFieldKind: Mutation
```

```ddn title="Create a new build:"
ddn supergraph build local
```

```ddn title="Bring your services down with CTRL+C and start them back up:"
ddn run docker-start
```

### Step 2. Approve an action

```plaintext title="From the PromptQL playground, ask the following question:"
I really like Apollo 13; can you recommend a single movie with the same actors that is a historical story?
```

PromptQL will analyze the dataset by classifying movies and will suggest Saving Private Ryan. Likely, it will also ask
if you'd like to rent it. Answer `yes` and you'll then be prompted to either `Deny` or `Approve` the action.

If you click `Approve`, PromptQL will confirm that the rental was successful! 🍿

## Deploy and share your project

### Step 1. Create a new cloud build

Until this point, we've been developing locally. However, we can easily create a cloud build of our project and then
share it with others.

```ddn title="Run the following from your project's directory:"
ddn supergraph build create
```

The CLI will respond with information about your new build.

### Step 2. Share your project

You can visit the cloud project's console and share your new PromptQL app!

<Thumbnail src="/img/get-started/share-project.png" alt="Share your app" width="1000px" />

Users with any access level, including “Read only” can access your PromptQL app. Read only users cannot modify your
project or invite additional users.

You can also choose “Request Access” so that anyone who arrives at the project URL can request access.



==============================



# overview.mdx

URL: https://hasura.io/docs/promptql/how-to-build-with-promptql/overview


# Common Data Source & Logic Patterns

## Introduction

PromptQL, powered by Hasura DDN, makes it simple to build AI-ready APIs on your data — no matter where it lives.

You'll connect your data using **native connectors**, configure your API using **Hasura Metadata Language (HML)**, and
create an immutable **build** that will serve your application.

Then, you'll use the **PromptQL Playground UI**, or the **PromptQL API** to talk to your data.

## Let's build something

To get started, it's helpful to look at common patterns for building PromptQL applications based on the datasource(s)
you want to include:

- [With a persistent datasource and custom business logic](with-a-datasource-and-custom-business-logic.mdx)
- [With API endpoints](with-api-endpoints.mdx)
- [With bulk API data](with-apis-with-bulk-data.mdx)

:::info Mix and match

Keep in mind: these are simply broad buckets for you to get started. Any sources we support can be added to the same
project to create a unified supergraph which feeds data to PromptQL.

:::

## Guides by data source

You can also dive straight into a guide for your preferred source:

- [Amazon Athena](/how-to-build-with-promptql/with-amazon-athena.mdx)
- [Amazon Redshift](/how-to-build-with-promptql/with-amazon-redshift.mdx)
- [BigQuery](/how-to-build-with-promptql/with-bigquery.mdx)
- [Databricks](/how-to-build-with-promptql/with-databricks.mdx)
- [MySQL](/how-to-build-with-promptql/with-mysql.mdx)
- [PostgreSQL](/how-to-build-with-promptql/with-postgresql.mdx)
- [Snowflake](/how-to-build-with-promptql/with-snowflake.mdx)



==============================



# with-a-datasource-and-custom-business-logic.mdx

URL: https://hasura.io/docs/promptql/how-to-build-with-promptql/with-a-datasource-and-custom-business-logic


# With Persistent Datasources

The content of this guide is identical to our [quickstart](/quickstart.mdx). It provides you with a quick and easy guide
to get up-and-running with single datasource while also incorporating custom business logic so that PromptQL can take
action on a user's behalf.

You'll use a hosted PostgreSQL database pre-loaded with IMDb movie data. You'll be able to chat with the data to explore
information about different movies. Then, you'll add custom business logic that lets PromptQL take action — like renting
a movie on a user's behalf.


## Prerequisites

**Install the DDN CLI**

:::info Minimum version requirements

To use this guide, ensure you've installed/updated your CLI to at least `v2.28.0`.

:::


<Tabs groupId="os-preference" className="api-tabs">

<TabItem value="macOS-or-linux" label="macOS and Linux">

Simply run the installer script in your terminal:

<CodeBlock language="bash">
  {`curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/get.sh | bash`}
</CodeBlock>

<Admonition type="info" title="ARM-based Linux Machines">
  Currently, the CLI does not support installation on ARM-based Linux systems.
</Admonition>

</TabItem>

<TabItem value="windows" label="Windows">
- Download the latest <a href={`https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/latest/DDN_CLI_Setup.exe`}>DDN CLI installer for Windows.</a> 
- Run the `DDN_CLI_Setup.exe` installer file and follow the instructions. This will only take a minute.
- By default, the DDN CLI is installed under `C:\Users\{Username}\AppData\Local\Programs\DDN_CLI`
- The DDN CLI is added to your `%PATH%` environment variable so that you can use the `ddn` command from your terminal.
<br />

</TabItem>
</Tabs>


**Install [Docker](https://docs.docker.com/engine/install/)**

The Docker-based workflow helps you iterate and develop locally without deploying any changes to Hasura DDN, making the
development experience faster and your feedback loops shorter. **You'll need Docker Compose `v2.20` or later.**

**Validate the installation**

You can verify that the DDN CLI is installed correctly by running:

```ddn
ddn doctor
```


## Tutorial


### Step 1. Authenticate your CLI

```ddn title="Before you can create a new Hasura DDN project, you need to authenticate your CLI:"
ddn auth login
```

This will launch a browser window prompting you to log in or sign up for Hasura Cloud. After you log in, the CLI will
acknowledge your login, giving you access to Hasura Cloud resources.

### Step 2. Scaffold out a new local project

```ddn title="Next, create a new local project:"
ddn supergraph init imdb-promptflix --with-promptql && cd imdb-promptflix
```

Now that you're in this directory, you'll see your project scaffolded out for you. You can view the structure by either
running `ls` in your terminal, or by opening the directory in your preferred editor.

### Step 3. Initialize your PostgreSQL connector

```ddn title="In your project directory, run:"
ddn connector init imdb -i
```

From the dropdown, select `hasura/postgres-promptql` (you can type to filter the list). Then, enter the following
connection URI to an existing PostgreSQL database of IMDb data:

```plaintext
jdbc:postgresql://35.236.11.122:5432/imdb?user=read_only_user&password=readonlyuser
```

For the `JDBC_SCHEMAS` environment variable, enter the following:

```plaintext
public
```

### Step 4. Introspect the PostgreSQL database

```ddn title="Next, use the CLI to introspect your PostgreSQL database:"
ddn connector introspect imdb
```

After running this, you should see a representation of your database's schema in the
`app/connector/imdb/configuration.json` file; you can view this using `cat` or open the file in your editor.

### Step 5. Track your tables

Based on our sample database, a SQL schema will be generated. Let's track all the models to get started quickly.

```ddn title="Run the following command:"
ddn model add imdb "*"
```

Open the `app/metadata` directory. You'll find Hasura Metadata Language (HML) files for each table in your database.  
In this case, there's only one: `public_movies.hml`.

The DDN CLI uses these HML files to represent PostgreSQL tables in your API as
[models](/reference/metadata-reference/models.mdx).

### Step 6. Create a new build

```ddn title="To create a local build, run:"
ddn supergraph build local
```

The build is stored as a set of JSON files in `engine/build`.

### Step 7. Start your local services

```ddn title="Start your local Hasura DDN Engine and PostgreSQL connector:"
ddn run docker-start
```

Your terminal will be taken over by logs for the different services.

## Talk to your data

### Step 1. Open the PromptQL Playground

```ddn title="In another terminal, run:"
ddn console --local
```

### Step 2. Ask questions about your dataset

The console is a web app hosted at [`promptql.console.hasura.io`](https://promptql.console.hasura.io) that connects to
your local PromptQL API and data sources. Your data is processed in the DDN PromptQL runtime but isn't persisted
externally.

Head over to the console and ask a few questions about your data.

```plaintext title="For example:"
Hi, what are some questions you can answer?
```

PromptQL will respond with information about the dataset and make suggestions for your first query on the data!

:::info Try it out

Go ahead and ask a question! In the next steps, we'll take a deeper look at interacting with the data.

:::

## Act on your data

### Step 1. Create a command

While it's great that we can ask questions about the data set, what if the application could take action on our behalf?
Such as rent a movie for us? We can easily do this by adding custom business logic using one of our
[lambda connectors](/business-logic/overview.mdx). In the example below, we'll use TypeScript.

```ddn title="Like before, let's initialize this connector:"
ddn connector init promptflix -i
```

From the list of choices, select `hasura/nodejs`. Like with the PostgreSQL connector, the CLI will scaffold out a set of
configuration files, including a file called `app/connector/promptflix/functions.ts` — this is where we'll add our logic
so the application can take action on our behalf.

```typescript title="Replace the content of your functions.ts file with the following:"
/*
 * This interface defines the structure of the response object returned by the
 * function that rents a movie. It includes a success status and a message.
 * The success status is a boolean indicating whether the operation was
 * successful or not.
 */
interface RentMovieResponse {
  success: boolean;
  message: string;
}

/*
 * PromptFlix - A movie rental service
 *
 * @param {string} seriesTitle - The title of the movie series to rent
 * @returns {RentMovieResponse} - The response object containing success status and message
 */
export function rentSingleMovieBySeriesTitle(seriesTitle: string): RentMovieResponse {
  console.log(`Renting movie series: ${seriesTitle}`);
  return {
    success: true,
    message: `Successfully rented the movie series: ${seriesTitle}`,
  };
}
```

This function simulates the action of renting a movie on behalf of a user. The JSDoc comments aid in providing context
to the underlying LLM so that the application knows how to use the function.

```ddn title="Introspect your connector:"
ddn connector introspect promptflix
```

```ddn title="Add your function as a command:"
ddn command add promptflix "*"
```

Before we create a new build, let's add a description to the metadata for our command located in
`app/metadata/rent_single_movie_by_series_title.hml`

```yaml title="Open the newly-generated rent_single_movie_by_series_title.hml file and add the following description:" {6-7}
---
kind: Command
version: v1
definition:
  name: rent_single_movie_by_series_title
  description:
    This function allows users to rent movies from PromptFlix and should be used for any request to rent a movie.
  outputType: rent_movie_response!
  arguments:
    - name: series_title
      type: String!
  source:
    dataConnectorName: promptflix
    dataConnectorCommand:
      procedure: rentSingleMovieBySeriesTitle
    argumentMapping:
      series_title: seriesTitle
  graphql:
    rootFieldName: rent_single_movie_by_series_title
    rootFieldKind: Mutation
```

```ddn title="Create a new build:"
ddn supergraph build local
```

```ddn title="Bring your services down with CTRL+C and start them back up:"
ddn run docker-start
```

### Step 2. Approve an action

```plaintext title="From the PromptQL playground, ask the following question:"
I really like Apollo 13; can you recommend a single movie with the same actors that is a historical story?
```

PromptQL will analyze the dataset by classifying movies and will suggest Saving Private Ryan. Likely, it will also ask
if you'd like to rent it. Answer `yes` and you'll then be prompted to either `Deny` or `Approve` the action.

If you click `Approve`, PromptQL will confirm that the rental was successful! 🍿

## Deploy and share your project

### Step 1. Create a new cloud build

Until this point, we've been developing locally. However, we can easily create a cloud build of our project and then
share it with others.

```ddn title="Run the following from your project's directory:"
ddn supergraph build create
```

The CLI will respond with information about your new build.

### Step 2. Share your project

You can visit the cloud project's console and share your new PromptQL app!

<Thumbnail src="/img/get-started/share-project.png" alt="Share your app" width="1000px" />

Users with any access level, including “Read only” can access your PromptQL app. Read only users cannot modify your
project or invite additional users.

You can also choose “Request Access” so that anyone who arrives at the project URL can request access.



==============================



# with-api-endpoints.mdx

URL: https://hasura.io/docs/promptql/how-to-build-with-promptql/with-api-endpoints


# With API Endpoints

We can connect PromptQL to API endpoints to query them directly.

Sometimes bulk loading data from API services might be overkill. In such cases, connecting to API endpoints directly is
useful or a convenient way to test your API integration.

However, if you expect to run into API rate limits, or need more flexible access to data that would require filtering
data in ways that the API doesn't natively allow for, consider
[bulk loading the data instead](/how-to-build-with-promptql/with-apis-with-bulk-data.mdx).


## Prerequisites

**Install the DDN CLI**

:::info Minimum version requirements

To use this guide, ensure you've installed/updated your CLI to at least `v2.28.0`.

:::


<Tabs groupId="os-preference" className="api-tabs">

<TabItem value="macOS-or-linux" label="macOS and Linux">

Simply run the installer script in your terminal:

<CodeBlock language="bash">
  {`curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/get.sh | bash`}
</CodeBlock>

<Admonition type="info" title="ARM-based Linux Machines">
  Currently, the CLI does not support installation on ARM-based Linux systems.
</Admonition>

</TabItem>

<TabItem value="windows" label="Windows">
- Download the latest <a href={`https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/latest/DDN_CLI_Setup.exe`}>DDN CLI installer for Windows.</a> 
- Run the `DDN_CLI_Setup.exe` installer file and follow the instructions. This will only take a minute.
- By default, the DDN CLI is installed under `C:\Users\{Username}\AppData\Local\Programs\DDN_CLI`
- The DDN CLI is added to your `%PATH%` environment variable so that you can use the `ddn` command from your terminal.
<br />

</TabItem>
</Tabs>


**Install [Docker](https://docs.docker.com/engine/install/)**

The Docker-based workflow helps you iterate and develop locally without deploying any changes to Hasura DDN, making the
development experience faster and your feedback loops shorter. **You'll need Docker Compose `v2.20` or later.**

**Validate the installation**

You can verify that the DDN CLI is installed correctly by running:

```ddn
ddn doctor
```


## Tutorial

Instead of adding a connector that is backed by a persistent data store (like a database, or DuckDB), we'll add a
**lambda** connector.

After creating your supergraph project with:

```ddn
ddn supergraph init myproject --with-promptql
```

Load one of the following connectors:

1. TypeScript + DuckDB: `hasura/duckduckapi`
2. TypeScript with NodeJS runtime: `hasura/nodejs`
3. Python: `hasura/python`

All of these connectors follow the same design, but for the purposes of this guide we'll follow the TypeScript guide.

### Step 1. Initialize the connector

**Install the connector and say hello!**

```ddn
ddn connector init typescript -i
```

### Step 2. Introspect the connector

```ddn
ddn connector introspect typescript
ddn commands list typescript
```

You should see a default `hello` command being `AVAILABLE` which means that it’s not yet added to the supergraph.

### Step 3. Call an external API

Open the `app/connector/typescript/functions.ts` file.

```ts title="Replace the contents with the following:"
/**
 * This is an API to say hello from httpbin for a given name
 * @readonly
 */
export async function helloFromHttpBin(name?: string): Promise<{ greeting?: string }> {
  const greeting = { greeting: name };

  const response = await fetch("https://httpbin.org/post", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
    },
    body: JSON.stringify({ greeting: `Hello ${name}!` }),
  });

  const data: any = await response.json();
  return { greeting: data?.json?.greeting };
}
```

### Step 4. Add the metadata

```ddn
ddn connector introspect typescript
ddn commands list typescript
ddn commands add typescript helloFromHttpBin
```

### Step 5. Create and run a new build

```ddn title="Create a new local build:"
ddn supergraph build local
```

```ddn title="Run your services:"
ddn run docker-start
```

```ddn title="In a new terminal tab, open the devlopment console:"
ddn console --local
```

Head over to the PromptQL Playground and see if the AI assistant is able to call your API integration.

```plaintext
say hello from httpBin for everyone
```



==============================



# with-apis-with-bulk-data.mdx

URL: https://hasura.io/docs/promptql/how-to-build-with-promptql/with-apis-with-bulk-data


# With Bulk API Data

In this tutorial we'll see how to connect to an API source that has some bulk data we want to bring into PromptQL.

This is what we'll do:

- We will set up a connector that has a DuckDB source
- We will set up a job to load data from our API source

We're loading data into DuckDB for this example, but you could load data into any database that has a supported
connector (e.g., PostgreSQL, MongoDB, ClickHouse). We're going to use TypeScript to write a loading script to load data
— but how you choose to load data is completely up to you.


## Prerequisites

**Install the DDN CLI**

:::info Minimum version requirements

To use this guide, ensure you've installed/updated your CLI to at least `v2.28.0`.

:::


<Tabs groupId="os-preference" className="api-tabs">

<TabItem value="macOS-or-linux" label="macOS and Linux">

Simply run the installer script in your terminal:

<CodeBlock language="bash">
  {`curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/get.sh | bash`}
</CodeBlock>

<Admonition type="info" title="ARM-based Linux Machines">
  Currently, the CLI does not support installation on ARM-based Linux systems.
</Admonition>

</TabItem>

<TabItem value="windows" label="Windows">
- Download the latest <a href={`https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/latest/DDN_CLI_Setup.exe`}>DDN CLI installer for Windows.</a> 
- Run the `DDN_CLI_Setup.exe` installer file and follow the instructions. This will only take a minute.
- By default, the DDN CLI is installed under `C:\Users\{Username}\AppData\Local\Programs\DDN_CLI`
- The DDN CLI is added to your `%PATH%` environment variable so that you can use the `ddn` command from your terminal.
<br />

</TabItem>
</Tabs>


**Install [Docker](https://docs.docker.com/engine/install/)**

The Docker-based workflow helps you iterate and develop locally without deploying any changes to Hasura DDN, making the
development experience faster and your feedback loops shorter. **You'll need Docker Compose `v2.20` or later.**

**Validate the installation**

You can verify that the DDN CLI is installed correctly by running:

```ddn
ddn doctor
```


## Tutorial

### Step 1. Scaffold out a new local project

```ddn
ddn supergraph init bulk-data --with-promptql && cd bulk-data
```

### Step 2. Add the `hasura/duckduckapi` connector

```ddn title="Initialize the connector and choose hasura/duckduckapi from the list:"
ddn connector init github -i
```

```bash title="Move into the connector directory and install the dependencies:"
cd app/connector/github && npm i
```

### Step 3. Initialize a table and sample data

Open the file `app/connector/github/index.ts` and define your DuckDB schema there.

```ts title="We'll start by creating a repoistories entity:"
// ...

const connectorConfig: duckduckapi = {
  dbSchema: \`
    -- Create repositories table with commonly needed fields
    DROP TABLE IF EXISTS repositories;
    CREATE TABLE repositories (
        id INTEGER PRIMARY KEY,
        name VARCHAR NOT NULL,
        description TEXT
    );

    -- Sample data
    INSERT INTO repositories (id, name, description)
    VALUES (1, 'my-project', 'A sample repository');
  \`,
  functionsFilePath: path.resolve(__dirname, './functions.ts'),
};

// ...
```

:::info Why are we doing this?

The `hasura/duckduckapi` connector uses a convention wherein you'll create a schema in an underlying DuckDB database
that represents your API's bulk data. Later, we'll create a loader function that will then use this schema to persist
bulk data from your API in this DuckDB instance.

:::

### Step 4. Add the metadata

Once we create new entities in our sources, we need to get them into our project's metadata. This allows the AI
assistant to access that data via PromptQL.

```ddn
# Grab the model definitions
ddn connector introspect github

# Check out what models are available to track. You'll see some sample ones which you can ignore for now.
ddn model list github

# Add the repositories model
ddn model add github repositories
```

### Step 5. Create and run a new build

```ddn title="Create a new local build:"
ddn supergraph build local
```

```ddn title="Run your services:"
ddn run docker-start
```

```ddn title="In a new terminal tab, open the devlopment console:"
ddn console --local
```

Head over to the PromptQL Playground and see if the AI assistant is able to access your repositories.

```plaintext
What repositories do I have?
```

PromptQL will return something like:

```
You have one repository in your account. This repository is named "my-project" and is described as "A sample repository".
```

### Step 6. Set up a job to continuously load data (optional)

Adding a job to load data can be done by kicking off an async task from our DuckDuckAPI connector.

Head over to `app/connector/github/index.ts` and add the following code right after the connector starts:

```ts
// import statements...
// schema initialization...

async function insertData() {
  const db = await getDB();

  setInterval(async () => {
    try {
      const timestamp = new Date().toISOString();
      await db.all(\`
        INSERT INTO repositories (id, name, description)
        VALUES (
          (SELECT COALESCE(MAX(id), 0) + 1 FROM repositories),
          'project-\${timestamp}',
          'Automatically inserted at \${timestamp}'
        )
      \`);
      console.log(\`Inserted new repository at \${timestamp}\`);
    } catch (err) {
      console.error('Error inserting data:', err);
    }
  }, 1000);
}

(async () => {
  const connector = await makeConnector(connectorConfig);
  start(connector);

  // Kick off an insert data job
  insertData();
})();
```

## A real-world example

The steps above help you get started by understanding how to setup DuckDB, how to get a connection to it and how to
start inserting data into it that comes from another source.

In a production ready example, you'll need to:

1. Connect to another API securely
2. Incrementally pull in updates after the initial sync is done
3. Handle API rate limits
4. Persist data incrementally
5. Recover from failures and process restarts

Check out the code at [PromptQL Github example](https://github.com/hasura/example-promptql-github) and start reading
through the code at
[app/connector/github/index.ts](https://github.com/hasura/example-promptql-github/blob/main/app/connector/github/index.ts)
to see how to put together a real-world bulk data from an API connector!



==============================



# with-amazon-athena.mdx

URL: https://hasura.io/docs/promptql/how-to-build-with-promptql/with-amazon-athena


# Get Started with PromptQL and Amazon Athena

<BoilerplateOverview dataSourceName="Amazon Athena" time="twenty" />

This tutorial assumes you're starting from scratch; you should use an existing Amazon Athena database.


## Prerequisites

**Install the DDN CLI**

:::info Minimum version requirements

To use this guide, ensure you've installed/updated your CLI to at least `v2.28.0`.

:::


<Tabs groupId="os-preference" className="api-tabs">

<TabItem value="macOS-or-linux" label="macOS and Linux">

Simply run the installer script in your terminal:

<CodeBlock language="bash">
  {`curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/get.sh | bash`}
</CodeBlock>

<Admonition type="info" title="ARM-based Linux Machines">
  Currently, the CLI does not support installation on ARM-based Linux systems.
</Admonition>

</TabItem>

<TabItem value="windows" label="Windows">
- Download the latest <a href={`https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/latest/DDN_CLI_Setup.exe`}>DDN CLI installer for Windows.</a> 
- Run the `DDN_CLI_Setup.exe` installer file and follow the instructions. This will only take a minute.
- By default, the DDN CLI is installed under `C:\Users\{Username}\AppData\Local\Programs\DDN_CLI`
- The DDN CLI is added to your `%PATH%` environment variable so that you can use the `ddn` command from your terminal.
<br />

</TabItem>
</Tabs>


**Install [Docker](https://docs.docker.com/engine/install/)**

The Docker-based workflow helps you iterate and develop locally without deploying any changes to Hasura DDN, making the
development experience faster and your feedback loops shorter. **You'll need Docker Compose `v2.20` or later.**

**Validate the installation**

You can verify that the DDN CLI is installed correctly by running:

```ddn
ddn doctor
```


## Tutorial

### Step 1. Authenticate your CLI

```ddn title="Before you can create a new Hasura DDN project for PromptQL, you need to authenticate your CLI:"
ddn auth login
```

This will launch a browser window prompting you to log in or sign up for Hasura DDN. After you log in, the CLI will
acknowledge your login, giving you access to Hasura Cloud resources.

### Step 2. Scaffold out a new local project

```ddn title="Next, create a new local project:"
ddn supergraph init my-project --with-promptql && cd my-project
```

Once you move into this directory, you'll see your project scaffolded out for you. You can view the structure by either
running `ls` in your terminal, or by opening the directory in your preferred editor.

### Step 3. Initialize your connector

```ddn title="In your project directory, run:"
ddn connector init my_connector -i
```


Select `hasura/athena` (you can type to filter the list). Then, enter the following required environment variables:

| ENV            | Example                                                                      | Description                                            |
| -------------- | ---------------------------------------------------------------------------- | ------------------------------------------------------ |
| `JDBC_URL`     | `jdbc:athena://<host>:<port>/<database>?user=<username>&password=<password>` | The JDBC URL to connect to the Amazon Athena database. |
| `JDBC_SCHEMAS` | `public,app`                                                                 | The schemas to use for the database.                   |

:::info Syntax for schemas

When entering schemas, ensure there's no whitespace as in the example above.

:::

Hasura will never modify your source schema.


### Step 4. Introspect your source

```ddn title="Next, use the CLI to introspect your source:"
ddn connector introspect my_connector
```

After running this, you should see a representation of your source's schema in the
`app/connector/my_connector/configuration||config.json` file; you can view this using `cat` or open the file in your
editor.

```ddn title="Additionally, you can check which resources are available — and their status — at any point using the CLI:"
ddn connector show-resources my_connector
```

### Step 5. Add your resources

```ddn title="Add your resources to create metadata:"
ddn model add my_connector "*"
ddn command add my_connector "*"
ddn relationship add my_connector "*"
```

Open the `app/metadata` directory and you'll find newly-generated file(s) ending in `.hml`. The DDN CLI will use these
Hasura Metadata Language files to represent your data source to PromptQL as
[models](/reference/metadata-reference/models.mdx), [commands](/reference/metadata-reference/commands.mdx), and
[relationships](/reference/metadata-reference/relationships.mdx).

### Step 6. Add semantic information to your metadata (optional)

It is highly recommended to provide extra natural language descriptions of the resources in your project so that the LLM
can better understand your data and create appropriate query plans.

The description field can be added to `Model`, `Command` and `Relationship` metadata objects to provide semantic
context. See more about [semantic information here](/data-modeling/semantic-information.mdx).

### Step 7. Create a new build

```ddn title="To create a local build, run:"
ddn supergraph build local
```

The build is stored as a set of JSON files in `engine/build`.

### Step 8. Start your local services

```ddn title="Start your local Hasura DDN Engine and connector:"
ddn run docker-start
```

Your terminal will be taken over by logs for the different services.

### Step 9. Chat with your data

```ddn title="In a new terminal tab, open your local console:"
ddn console --local
```

Once the PromptQL interface is open, ask a question about your data. For example:

<PqlChat>
  <User>Hi, what are some questions you can answer?</User>
  <Pql>
    I can help you analyze data from what appears to be an e-commerce system. Here are some types of questions I can answer:

    <ol>
        <li>Product-related:</li>
        <ul>
            <li>Find popular products based on orders or reviews</li>
            <li>Analyze product pricing across categories</li>
            <li>Look up product details and descriptions</li>
            <li>Find products from specific manufacturers or countries</li>
        </ul>
        <li>Order and Purchase Analysis:</li>
        <ul>
            <li>Track order statuses</li>
            <li>Analyze delivery times</li>
            <li>Look at purchase patterns</li>
            <li>Find abandoned carts</li>
        </ul>
        <li>Customer Behavior:</li>
        <ul>
            <li>Analyze customer reviews and ratings</li>
            <li>Look at shopping patterns</li>
            <li>Track cart abandonment</li>
            <li>Study product category preferences</li>
        </ul>
        <li>Business Metrics:</li>
        <ul>
            <li>Calculate sales metrics</li>
            <li>Analyze review scores</li>
            <li>Track coupon usage</li>
            <li>Monitor cart completion rates</li>
        </ul>
    </ol>

    Would you like me to explore any of these areas in particular?

  </Pql>
</PqlChat>

### Step 10. Iterate on your source's schema

If something changes in your data source's schema, you can iterate on your data model by following the steps in the
[iteration guide](/data-modeling/iterate.mdx).


<BoilerplateSummary dataSourceName="Amazon Athena" />



==============================



# with-amazon-redshift.mdx

URL: https://hasura.io/docs/promptql/how-to-build-with-promptql/with-amazon-redshift


# Get Started with PromptQL and Amazon Redshift

<BoilerplateOverview dataSourceName="Amazon Redshift" time="twenty" />

This tutorial assumes you're starting from scratch; you should use an existing Amazon Redshift database.


## Prerequisites

**Install the DDN CLI**

:::info Minimum version requirements

To use this guide, ensure you've installed/updated your CLI to at least `v2.28.0`.

:::


<Tabs groupId="os-preference" className="api-tabs">

<TabItem value="macOS-or-linux" label="macOS and Linux">

Simply run the installer script in your terminal:

<CodeBlock language="bash">
  {`curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/get.sh | bash`}
</CodeBlock>

<Admonition type="info" title="ARM-based Linux Machines">
  Currently, the CLI does not support installation on ARM-based Linux systems.
</Admonition>

</TabItem>

<TabItem value="windows" label="Windows">
- Download the latest <a href={`https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/latest/DDN_CLI_Setup.exe`}>DDN CLI installer for Windows.</a> 
- Run the `DDN_CLI_Setup.exe` installer file and follow the instructions. This will only take a minute.
- By default, the DDN CLI is installed under `C:\Users\{Username}\AppData\Local\Programs\DDN_CLI`
- The DDN CLI is added to your `%PATH%` environment variable so that you can use the `ddn` command from your terminal.
<br />

</TabItem>
</Tabs>


**Install [Docker](https://docs.docker.com/engine/install/)**

The Docker-based workflow helps you iterate and develop locally without deploying any changes to Hasura DDN, making the
development experience faster and your feedback loops shorter. **You'll need Docker Compose `v2.20` or later.**

**Validate the installation**

You can verify that the DDN CLI is installed correctly by running:

```ddn
ddn doctor
```


## Tutorial

### Step 1. Authenticate your CLI

```ddn title="Before you can create a new Hasura DDN project for PromptQL, you need to authenticate your CLI:"
ddn auth login
```

This will launch a browser window prompting you to log in or sign up for Hasura DDN. After you log in, the CLI will
acknowledge your login, giving you access to Hasura Cloud resources.

### Step 2. Scaffold out a new local project

```ddn title="Next, create a new local project:"
ddn supergraph init my-project --with-promptql && cd my-project
```

Once you move into this directory, you'll see your project scaffolded out for you. You can view the structure by either
running `ls` in your terminal, or by opening the directory in your preferred editor.

### Step 3. Initialize your connector

```ddn title="In your project directory, run:"
ddn connector init my_connector -i
```


Select `hasura/redshift` (you can type to filter the list). Then, enter the following environment variables:

| ENV            | Example                                                                        | Description                                                                                        |
| -------------- | ------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------- |
| `JDBC_URL`     | `jdbc:redshift://<host>:<port>/<database>?user=<username>&password=<password>` | The JDBC URL to connect to the Amazon Redshift database.                                           |
| `JDBC_SCHEMAS` | `public,app`                                                                   | The schemas to use for the database. Optional. This can also be included in the connection string. |

:::info Syntax for schemas

When entering schemas, ensure there's no whitespace as in the example above.

:::

Hasura will never modify your source schema.


### Step 4. Introspect your source

```ddn title="Next, use the CLI to introspect your source:"
ddn connector introspect my_connector
```

After running this, you should see a representation of your source's schema in the
`app/connector/my_connector/configuration||config.json` file; you can view this using `cat` or open the file in your
editor.

```ddn title="Additionally, you can check which resources are available — and their status — at any point using the CLI:"
ddn connector show-resources my_connector
```

### Step 5. Add your resources

```ddn title="Add your resources to create metadata:"
ddn model add my_connector "*"
ddn command add my_connector "*"
ddn relationship add my_connector "*"
```

Open the `app/metadata` directory and you'll find newly-generated file(s) ending in `.hml`. The DDN CLI will use these
Hasura Metadata Language files to represent your data source to PromptQL as
[models](/reference/metadata-reference/models.mdx), [commands](/reference/metadata-reference/commands.mdx), and
[relationships](/reference/metadata-reference/relationships.mdx).

### Step 6. Add semantic information to your metadata (optional)

It is highly recommended to provide extra natural language descriptions of the resources in your project so that the LLM
can better understand your data and create appropriate query plans.

The description field can be added to `Model`, `Command` and `Relationship` metadata objects to provide semantic
context. See more about [semantic information here](/data-modeling/semantic-information.mdx).

### Step 7. Create a new build

```ddn title="To create a local build, run:"
ddn supergraph build local
```

The build is stored as a set of JSON files in `engine/build`.

### Step 8. Start your local services

```ddn title="Start your local Hasura DDN Engine and connector:"
ddn run docker-start
```

Your terminal will be taken over by logs for the different services.

### Step 9. Chat with your data

```ddn title="In a new terminal tab, open your local console:"
ddn console --local
```

Once the PromptQL interface is open, ask a question about your data. For example:

<PqlChat>
  <User>Hi, what are some questions you can answer?</User>
  <Pql>
    I can help you analyze data from what appears to be an e-commerce system. Here are some types of questions I can answer:

    <ol>
        <li>Product-related:</li>
        <ul>
            <li>Find popular products based on orders or reviews</li>
            <li>Analyze product pricing across categories</li>
            <li>Look up product details and descriptions</li>
            <li>Find products from specific manufacturers or countries</li>
        </ul>
        <li>Order and Purchase Analysis:</li>
        <ul>
            <li>Track order statuses</li>
            <li>Analyze delivery times</li>
            <li>Look at purchase patterns</li>
            <li>Find abandoned carts</li>
        </ul>
        <li>Customer Behavior:</li>
        <ul>
            <li>Analyze customer reviews and ratings</li>
            <li>Look at shopping patterns</li>
            <li>Track cart abandonment</li>
            <li>Study product category preferences</li>
        </ul>
        <li>Business Metrics:</li>
        <ul>
            <li>Calculate sales metrics</li>
            <li>Analyze review scores</li>
            <li>Track coupon usage</li>
            <li>Monitor cart completion rates</li>
        </ul>
    </ol>

    Would you like me to explore any of these areas in particular?

  </Pql>
</PqlChat>

### Step 10. Iterate on your source's schema

If something changes in your data source's schema, you can iterate on your data model by following the steps in the
[iteration guide](/data-modeling/iterate.mdx).


<BoilerplateSummary dataSourceName="Amazon Redshift" />



==============================



# with-bigquery.mdx

URL: https://hasura.io/docs/promptql/how-to-build-with-promptql/with-bigquery


# Get Started with PromptQL and BigQuery

<BoilerplateOverview dataSourceName="BigQuery" time="twenty" />

This tutorial assumes you're starting from scratch; you should use an existing BigQuery database.


## Prerequisites

**Install the DDN CLI**

:::info Minimum version requirements

To use this guide, ensure you've installed/updated your CLI to at least `v2.28.0`.

:::


<Tabs groupId="os-preference" className="api-tabs">

<TabItem value="macOS-or-linux" label="macOS and Linux">

Simply run the installer script in your terminal:

<CodeBlock language="bash">
  {`curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/get.sh | bash`}
</CodeBlock>

<Admonition type="info" title="ARM-based Linux Machines">
  Currently, the CLI does not support installation on ARM-based Linux systems.
</Admonition>

</TabItem>

<TabItem value="windows" label="Windows">
- Download the latest <a href={`https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/latest/DDN_CLI_Setup.exe`}>DDN CLI installer for Windows.</a> 
- Run the `DDN_CLI_Setup.exe` installer file and follow the instructions. This will only take a minute.
- By default, the DDN CLI is installed under `C:\Users\{Username}\AppData\Local\Programs\DDN_CLI`
- The DDN CLI is added to your `%PATH%` environment variable so that you can use the `ddn` command from your terminal.
<br />

</TabItem>
</Tabs>


**Install [Docker](https://docs.docker.com/engine/install/)**

The Docker-based workflow helps you iterate and develop locally without deploying any changes to Hasura DDN, making the
development experience faster and your feedback loops shorter. **You'll need Docker Compose `v2.20` or later.**

**Validate the installation**

You can verify that the DDN CLI is installed correctly by running:

```ddn
ddn doctor
```


## Tutorial

### Step 1. Authenticate your CLI

```ddn title="Before you can create a new Hasura DDN project for PromptQL, you need to authenticate your CLI:"
ddn auth login
```

This will launch a browser window prompting you to log in or sign up for Hasura DDN. After you log in, the CLI will
acknowledge your login, giving you access to Hasura Cloud resources.

### Step 2. Scaffold out a new local project

```ddn title="Next, create a new local project:"
ddn supergraph init my-project --with-promptql && cd my-project
```

Once you move into this directory, you'll see your project scaffolded out for you. You can view the structure by either
running `ls` in your terminal, or by opening the directory in your preferred editor.

### Step 3. Initialize your connector

```ddn title="In your project directory, run:"
ddn connector init my_connector -i
```


Select `hasura/bigquery-jdbc` (you can type to filter the list). Then, enter the following required environment
variable:

| ENV        | Example                                                                                                                                                                                               | Description                                       |
| ---------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------- |
| `JDBC_URL` | `jdbc:bigquery://https://www.googleapis.com/bigquery/v2:443;ProjectId=project-id;DefaultDataset=dataset;OAuthType=0;OAuthServiceAcctEmail=service-account-email;OAuthPvtKey=/etc/connector/key.json;` | The JDBC URL to connect to the BigQuery database. |

Hasura will never modify your source schema.

You should also create and download a JSON key file for the IAM service account.

The JSON key file should look like this example:

```json
{
  "type": "service_account",
  "project_id": "project-id",
  "private_key_id": "private-key-id",
  "private_key": "-----BEGIN PRIVATE KEY-----\nprivate-key\n-----END PRIVATE KEY-----\n",
  "client_email": "service-account-email",
  "client_id": "client-id",
  "auth_uri": "https://accounts.google.com/o/oauth2/auth",
  "token_uri": "https://oauth2.googleapis.com/token",
  "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
  "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/service-account-email"
}
```

Move your downloaded service account key file to your connector folder which is already referenced in the `JDBC_URL`
environment variable.

```bash
mv /path/to/your/key.json app/connector/my_bigquery/key.json
```


### Step 4. Introspect your source

```ddn title="Next, use the CLI to introspect your source:"
ddn connector introspect my_connector
```

After running this, you should see a representation of your source's schema in the
`app/connector/my_connector/configuration||config.json` file; you can view this using `cat` or open the file in your
editor.

```ddn title="Additionally, you can check which resources are available — and their status — at any point using the CLI:"
ddn connector show-resources my_connector
```

### Step 5. Add your resources

```ddn title="Add your resources to create metadata:"
ddn model add my_connector "*"
ddn command add my_connector "*"
ddn relationship add my_connector "*"
```

Open the `app/metadata` directory and you'll find newly-generated file(s) ending in `.hml`. The DDN CLI will use these
Hasura Metadata Language files to represent your data source to PromptQL as
[models](/reference/metadata-reference/models.mdx), [commands](/reference/metadata-reference/commands.mdx), and
[relationships](/reference/metadata-reference/relationships.mdx).

### Step 6. Add semantic information to your metadata (optional)

It is highly recommended to provide extra natural language descriptions of the resources in your project so that the LLM
can better understand your data and create appropriate query plans.

The description field can be added to `Model`, `Command` and `Relationship` metadata objects to provide semantic
context. See more about [semantic information here](/data-modeling/semantic-information.mdx).

### Step 7. Create a new build

```ddn title="To create a local build, run:"
ddn supergraph build local
```

The build is stored as a set of JSON files in `engine/build`.

### Step 8. Start your local services

```ddn title="Start your local Hasura DDN Engine and connector:"
ddn run docker-start
```

Your terminal will be taken over by logs for the different services.

### Step 9. Chat with your data

```ddn title="In a new terminal tab, open your local console:"
ddn console --local
```

Once the PromptQL interface is open, ask a question about your data. For example:

<PqlChat>
  <User>Hi, what are some questions you can answer?</User>
  <Pql>
    I can help you analyze data from what appears to be an e-commerce system. Here are some types of questions I can answer:

    <ol>
        <li>Product-related:</li>
        <ul>
            <li>Find popular products based on orders or reviews</li>
            <li>Analyze product pricing across categories</li>
            <li>Look up product details and descriptions</li>
            <li>Find products from specific manufacturers or countries</li>
        </ul>
        <li>Order and Purchase Analysis:</li>
        <ul>
            <li>Track order statuses</li>
            <li>Analyze delivery times</li>
            <li>Look at purchase patterns</li>
            <li>Find abandoned carts</li>
        </ul>
        <li>Customer Behavior:</li>
        <ul>
            <li>Analyze customer reviews and ratings</li>
            <li>Look at shopping patterns</li>
            <li>Track cart abandonment</li>
            <li>Study product category preferences</li>
        </ul>
        <li>Business Metrics:</li>
        <ul>
            <li>Calculate sales metrics</li>
            <li>Analyze review scores</li>
            <li>Track coupon usage</li>
            <li>Monitor cart completion rates</li>
        </ul>
    </ol>

    Would you like me to explore any of these areas in particular?

  </Pql>
</PqlChat>

### Step 10. Iterate on your source's schema

If something changes in your data source's schema, you can iterate on your data model by following the steps in the
[iteration guide](/data-modeling/iterate.mdx).


<BoilerplateSummary dataSourceName="BigQuery" />



==============================



# with-databricks.mdx

URL: https://hasura.io/docs/promptql/how-to-build-with-promptql/with-databricks


# Get Started with PromptQL and Databricks

<BoilerplateOverview dataSourceName="Databricks" time="twenty" />


## Prerequisites

**Install the DDN CLI**

:::info Minimum version requirements

To use this guide, ensure you've installed/updated your CLI to at least `v2.28.0`.

:::


<Tabs groupId="os-preference" className="api-tabs">

<TabItem value="macOS-or-linux" label="macOS and Linux">

Simply run the installer script in your terminal:

<CodeBlock language="bash">
  {`curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/get.sh | bash`}
</CodeBlock>

<Admonition type="info" title="ARM-based Linux Machines">
  Currently, the CLI does not support installation on ARM-based Linux systems.
</Admonition>

</TabItem>

<TabItem value="windows" label="Windows">
- Download the latest <a href={`https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/latest/DDN_CLI_Setup.exe`}>DDN CLI installer for Windows.</a> 
- Run the `DDN_CLI_Setup.exe` installer file and follow the instructions. This will only take a minute.
- By default, the DDN CLI is installed under `C:\Users\{Username}\AppData\Local\Programs\DDN_CLI`
- The DDN CLI is added to your `%PATH%` environment variable so that you can use the `ddn` command from your terminal.
<br />

</TabItem>
</Tabs>


**Install [Docker](https://docs.docker.com/engine/install/)**

The Docker-based workflow helps you iterate and develop locally without deploying any changes to Hasura DDN, making the
development experience faster and your feedback loops shorter. **You'll need Docker Compose `v2.20` or later.**

**Validate the installation**

You can verify that the DDN CLI is installed correctly by running:

```ddn
ddn doctor
```


## Tutorial

### Step 1. Authenticate your CLI

```ddn title="Before you can create a new Hasura DDN project for PromptQL, you need to authenticate your CLI:"
ddn auth login
```

This will launch a browser window prompting you to log in or sign up for Hasura DDN. After you log in, the CLI will
acknowledge your login, giving you access to Hasura Cloud resources.

### Step 2. Scaffold out a new local project

```ddn title="Next, create a new local project:"
ddn supergraph init my-project --with-promptql && cd my-project
```

Once you move into this directory, you'll see your project scaffolded out for you. You can view the structure by either
running `ls` in your terminal, or by opening the directory in your preferred editor.

### Step 3. Initialize your connector

```ddn title="In your project directory, run:"
ddn connector init my_connector -i
```


Select `hasura/databricks` (you can type to filter the list). Then, enter the following environment variables:

| ENV            | Example                                                                                                                                                                  | Description                                                                                                                        |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------- |
| `JDBC_URL`     | `jdbc:databricks://<host>:<port>/default;transportMode=http;ssl=1;AuthMech=3;httpPath=/sql/1.0/warehouses/<warehouse-id>;UID=token;PWD=<access-token>;ConnCatalog=main;` | You can construct the base of this using your Databricks UI under `SQL Warehouses` » `<name-of-warehouse>` » `Connection details`. |
| `JDBC_SCHEMAS` | `default,public`                                                                                                                                                         | A comma-separated list of schemas within the referenced catalog.                                                                   |


### Step 4. Introspect your source

```ddn title="Next, use the CLI to introspect your source:"
ddn connector introspect my_connector
```

After running this, you should see a representation of your source's schema in the
`app/connector/my_connector/configuration||config.json` file; you can view this using `cat` or open the file in your
editor.

```ddn title="Additionally, you can check which resources are available — and their status — at any point using the CLI:"
ddn connector show-resources my_connector
```

### Step 5. Add your resources

```ddn title="Add your resources to create metadata:"
ddn model add my_connector "*"
ddn command add my_connector "*"
ddn relationship add my_connector "*"
```

Open the `app/metadata` directory and you'll find newly-generated file(s) ending in `.hml`. The DDN CLI will use these
Hasura Metadata Language files to represent your data source to PromptQL as
[models](/reference/metadata-reference/models.mdx), [commands](/reference/metadata-reference/commands.mdx), and
[relationships](/reference/metadata-reference/relationships.mdx).

### Step 6. Add semantic information to your metadata (optional)

It is highly recommended to provide extra natural language descriptions of the resources in your project so that the LLM
can better understand your data and create appropriate query plans.

The description field can be added to `Model`, `Command` and `Relationship` metadata objects to provide semantic
context. See more about [semantic information here](/data-modeling/semantic-information.mdx).

### Step 7. Create a new build

```ddn title="To create a local build, run:"
ddn supergraph build local
```

The build is stored as a set of JSON files in `engine/build`.

### Step 8. Start your local services

```ddn title="Start your local Hasura DDN Engine and connector:"
ddn run docker-start
```

Your terminal will be taken over by logs for the different services.

### Step 9. Chat with your data

```ddn title="In a new terminal tab, open your local console:"
ddn console --local
```

Once the PromptQL interface is open, ask a question about your data. For example:

<PqlChat>
  <User>Hi, what are some questions you can answer?</User>
  <Pql>
    I can help you analyze data from what appears to be an e-commerce system. Here are some types of questions I can answer:

    <ol>
        <li>Product-related:</li>
        <ul>
            <li>Find popular products based on orders or reviews</li>
            <li>Analyze product pricing across categories</li>
            <li>Look up product details and descriptions</li>
            <li>Find products from specific manufacturers or countries</li>
        </ul>
        <li>Order and Purchase Analysis:</li>
        <ul>
            <li>Track order statuses</li>
            <li>Analyze delivery times</li>
            <li>Look at purchase patterns</li>
            <li>Find abandoned carts</li>
        </ul>
        <li>Customer Behavior:</li>
        <ul>
            <li>Analyze customer reviews and ratings</li>
            <li>Look at shopping patterns</li>
            <li>Track cart abandonment</li>
            <li>Study product category preferences</li>
        </ul>
        <li>Business Metrics:</li>
        <ul>
            <li>Calculate sales metrics</li>
            <li>Analyze review scores</li>
            <li>Track coupon usage</li>
            <li>Monitor cart completion rates</li>
        </ul>
    </ol>

    Would you like me to explore any of these areas in particular?

  </Pql>
</PqlChat>

### Step 10. Iterate on your source's schema

If something changes in your data source's schema, you can iterate on your data model by following the steps in the
[iteration guide](/data-modeling/iterate.mdx).


<BoilerplateSummary dataSourceName="Databricks" />



==============================



# with-mysql.mdx

URL: https://hasura.io/docs/promptql/how-to-build-with-promptql/with-mysql


# Get Started with PromptQL and MySQL

<BoilerplateOverview dataSourceName="MySQL" time="twenty" />


## Prerequisites

**Install the DDN CLI**

:::info Minimum version requirements

To use this guide, ensure you've installed/updated your CLI to at least `v2.28.0`.

:::


<Tabs groupId="os-preference" className="api-tabs">

<TabItem value="macOS-or-linux" label="macOS and Linux">

Simply run the installer script in your terminal:

<CodeBlock language="bash">
  {`curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/get.sh | bash`}
</CodeBlock>

<Admonition type="info" title="ARM-based Linux Machines">
  Currently, the CLI does not support installation on ARM-based Linux systems.
</Admonition>

</TabItem>

<TabItem value="windows" label="Windows">
- Download the latest <a href={`https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/latest/DDN_CLI_Setup.exe`}>DDN CLI installer for Windows.</a> 
- Run the `DDN_CLI_Setup.exe` installer file and follow the instructions. This will only take a minute.
- By default, the DDN CLI is installed under `C:\Users\{Username}\AppData\Local\Programs\DDN_CLI`
- The DDN CLI is added to your `%PATH%` environment variable so that you can use the `ddn` command from your terminal.
<br />

</TabItem>
</Tabs>


**Install [Docker](https://docs.docker.com/engine/install/)**

The Docker-based workflow helps you iterate and develop locally without deploying any changes to Hasura DDN, making the
development experience faster and your feedback loops shorter. **You'll need Docker Compose `v2.20` or later.**

**Validate the installation**

You can verify that the DDN CLI is installed correctly by running:

```ddn
ddn doctor
```


## Tutorial

### Step 1. Authenticate your CLI

```ddn title="Before you can create a new Hasura DDN project for PromptQL, you need to authenticate your CLI:"
ddn auth login
```

This will launch a browser window prompting you to log in or sign up for Hasura DDN. After you log in, the CLI will
acknowledge your login, giving you access to Hasura Cloud resources.

### Step 2. Scaffold out a new local project

```ddn title="Next, create a new local project:"
ddn supergraph init my-project --with-promptql && cd my-project
```

Once you move into this directory, you'll see your project scaffolded out for you. You can view the structure by either
running `ls` in your terminal, or by opening the directory in your preferred editor.

### Step 3. Initialize your connector

```ddn title="In your project directory, run:"
ddn connector init my_connector -i
```


Select `hasura/mysql` (you can type to filter the list). Then, enter the following environment variables:

| ENV        | Example                                        | Description                         |
| ---------- | ---------------------------------------------- | ----------------------------------- |
| `JDBC_URL` | `jdbc:mysql://user:password@host:3306/db_name` | This connector requires a JDBC URL. |


### Step 4. Introspect your source

```ddn title="Next, use the CLI to introspect your source:"
ddn connector introspect my_connector
```

After running this, you should see a representation of your source's schema in the
`app/connector/my_connector/configuration||config.json` file; you can view this using `cat` or open the file in your
editor.

```ddn title="Additionally, you can check which resources are available — and their status — at any point using the CLI:"
ddn connector show-resources my_connector
```

### Step 5. Add your resources

```ddn title="Add your resources to create metadata:"
ddn model add my_connector "*"
ddn command add my_connector "*"
ddn relationship add my_connector "*"
```

Open the `app/metadata` directory and you'll find newly-generated file(s) ending in `.hml`. The DDN CLI will use these
Hasura Metadata Language files to represent your data source to PromptQL as
[models](/reference/metadata-reference/models.mdx), [commands](/reference/metadata-reference/commands.mdx), and
[relationships](/reference/metadata-reference/relationships.mdx).

### Step 6. Add semantic information to your metadata (optional)

It is highly recommended to provide extra natural language descriptions of the resources in your project so that the LLM
can better understand your data and create appropriate query plans.

The description field can be added to `Model`, `Command` and `Relationship` metadata objects to provide semantic
context. See more about [semantic information here](/data-modeling/semantic-information.mdx).

### Step 7. Create a new build

```ddn title="To create a local build, run:"
ddn supergraph build local
```

The build is stored as a set of JSON files in `engine/build`.

### Step 8. Start your local services

```ddn title="Start your local Hasura DDN Engine and connector:"
ddn run docker-start
```

Your terminal will be taken over by logs for the different services.

### Step 9. Chat with your data

```ddn title="In a new terminal tab, open your local console:"
ddn console --local
```

Once the PromptQL interface is open, ask a question about your data. For example:

<PqlChat>
  <User>Hi, what are some questions you can answer?</User>
  <Pql>
    I can help you analyze data from what appears to be an e-commerce system. Here are some types of questions I can answer:

    <ol>
        <li>Product-related:</li>
        <ul>
            <li>Find popular products based on orders or reviews</li>
            <li>Analyze product pricing across categories</li>
            <li>Look up product details and descriptions</li>
            <li>Find products from specific manufacturers or countries</li>
        </ul>
        <li>Order and Purchase Analysis:</li>
        <ul>
            <li>Track order statuses</li>
            <li>Analyze delivery times</li>
            <li>Look at purchase patterns</li>
            <li>Find abandoned carts</li>
        </ul>
        <li>Customer Behavior:</li>
        <ul>
            <li>Analyze customer reviews and ratings</li>
            <li>Look at shopping patterns</li>
            <li>Track cart abandonment</li>
            <li>Study product category preferences</li>
        </ul>
        <li>Business Metrics:</li>
        <ul>
            <li>Calculate sales metrics</li>
            <li>Analyze review scores</li>
            <li>Track coupon usage</li>
            <li>Monitor cart completion rates</li>
        </ul>
    </ol>

    Would you like me to explore any of these areas in particular?

  </Pql>
</PqlChat>

### Step 10. Iterate on your source's schema

If something changes in your data source's schema, you can iterate on your data model by following the steps in the
[iteration guide](/data-modeling/iterate.mdx).


<BoilerplateSummary dataSourceName="MySQL" />



==============================



# with-postgresql.mdx

URL: https://hasura.io/docs/promptql/how-to-build-with-promptql/with-postgresql


# Get Started with PromptQL and PostgreSQL

<BoilerplateOverview dataSourceName="PostgreSQL" time="twenty" />

This tutorial assumes you're starting from scratch; you can use an existing PostgreSQL database that you have or the
PostgreSQL docker image that ships with the data connector.


## Prerequisites

**Install the DDN CLI**

:::info Minimum version requirements

To use this guide, ensure you've installed/updated your CLI to at least `v2.28.0`.

:::


<Tabs groupId="os-preference" className="api-tabs">

<TabItem value="macOS-or-linux" label="macOS and Linux">

Simply run the installer script in your terminal:

<CodeBlock language="bash">
  {`curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/get.sh | bash`}
</CodeBlock>

<Admonition type="info" title="ARM-based Linux Machines">
  Currently, the CLI does not support installation on ARM-based Linux systems.
</Admonition>

</TabItem>

<TabItem value="windows" label="Windows">
- Download the latest <a href={`https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/latest/DDN_CLI_Setup.exe`}>DDN CLI installer for Windows.</a> 
- Run the `DDN_CLI_Setup.exe` installer file and follow the instructions. This will only take a minute.
- By default, the DDN CLI is installed under `C:\Users\{Username}\AppData\Local\Programs\DDN_CLI`
- The DDN CLI is added to your `%PATH%` environment variable so that you can use the `ddn` command from your terminal.
<br />

</TabItem>
</Tabs>


**Install [Docker](https://docs.docker.com/engine/install/)**

The Docker-based workflow helps you iterate and develop locally without deploying any changes to Hasura DDN, making the
development experience faster and your feedback loops shorter. **You'll need Docker Compose `v2.20` or later.**

**Validate the installation**

You can verify that the DDN CLI is installed correctly by running:

```ddn
ddn doctor
```


## Tutorial

### Step 1. Authenticate your CLI

```ddn title="Before you can create a new Hasura DDN project for PromptQL, you need to authenticate your CLI:"
ddn auth login
```

This will launch a browser window prompting you to log in or sign up for Hasura DDN. After you log in, the CLI will
acknowledge your login, giving you access to Hasura Cloud resources.

### Step 2. Scaffold out a new local project

```ddn title="Next, create a new local project:"
ddn supergraph init my-project --with-promptql && cd my-project
```

Once you move into this directory, you'll see your project scaffolded out for you. You can view the structure by either
running `ls` in your terminal, or by opening the directory in your preferred editor.

### Step 3. Initialize your connector

```ddn title="In your project directory, run:"
ddn connector init my_connector -i
```


Select `hasura/postgres-promptql` (you can type to filter the list). Then, enter the following environment variables:

| ENV            | Example                                                                          | Description                                                                                        |
| -------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- |
| `JDBC_URL`     | `jdbc:postgresql://<host>:<port>/<database>?user=<username>&password=<password>` | The JDBC URL to connect to the PostgreSQL database.                                                |
| `JDBC_SCHEMAS` | `public,app`                                                                     | The schemas to use for the database. Optional. This can also be included in the connection string. |

:::info Syntax for schemas

When entering schemas, ensure there's no whitespace as in the example above.

:::

Hasura will never modify your source schema.


### Step 4. Introspect your source

```ddn title="Next, use the CLI to introspect your source:"
ddn connector introspect my_connector
```

After running this, you should see a representation of your source's schema in the
`app/connector/my_connector/configuration||config.json` file; you can view this using `cat` or open the file in your
editor.

```ddn title="Additionally, you can check which resources are available — and their status — at any point using the CLI:"
ddn connector show-resources my_connector
```

### Step 5. Add your resources

```ddn title="Add your resources to create metadata:"
ddn model add my_connector "*"
ddn command add my_connector "*"
ddn relationship add my_connector "*"
```

Open the `app/metadata` directory and you'll find newly-generated file(s) ending in `.hml`. The DDN CLI will use these
Hasura Metadata Language files to represent your data source to PromptQL as
[models](/reference/metadata-reference/models.mdx), [commands](/reference/metadata-reference/commands.mdx), and
[relationships](/reference/metadata-reference/relationships.mdx).

### Step 6. Add semantic information to your metadata (optional)

It is highly recommended to provide extra natural language descriptions of the resources in your project so that the LLM
can better understand your data and create appropriate query plans.

The description field can be added to `Model`, `Command` and `Relationship` metadata objects to provide semantic
context. See more about [semantic information here](/data-modeling/semantic-information.mdx).

### Step 7. Create a new build

```ddn title="To create a local build, run:"
ddn supergraph build local
```

The build is stored as a set of JSON files in `engine/build`.

### Step 8. Start your local services

```ddn title="Start your local Hasura DDN Engine and connector:"
ddn run docker-start
```

Your terminal will be taken over by logs for the different services.

### Step 9. Chat with your data

```ddn title="In a new terminal tab, open your local console:"
ddn console --local
```

Once the PromptQL interface is open, ask a question about your data. For example:

<PqlChat>
  <User>Hi, what are some questions you can answer?</User>
  <Pql>
    I can help you analyze data from what appears to be an e-commerce system. Here are some types of questions I can answer:

    <ol>
        <li>Product-related:</li>
        <ul>
            <li>Find popular products based on orders or reviews</li>
            <li>Analyze product pricing across categories</li>
            <li>Look up product details and descriptions</li>
            <li>Find products from specific manufacturers or countries</li>
        </ul>
        <li>Order and Purchase Analysis:</li>
        <ul>
            <li>Track order statuses</li>
            <li>Analyze delivery times</li>
            <li>Look at purchase patterns</li>
            <li>Find abandoned carts</li>
        </ul>
        <li>Customer Behavior:</li>
        <ul>
            <li>Analyze customer reviews and ratings</li>
            <li>Look at shopping patterns</li>
            <li>Track cart abandonment</li>
            <li>Study product category preferences</li>
        </ul>
        <li>Business Metrics:</li>
        <ul>
            <li>Calculate sales metrics</li>
            <li>Analyze review scores</li>
            <li>Track coupon usage</li>
            <li>Monitor cart completion rates</li>
        </ul>
    </ol>

    Would you like me to explore any of these areas in particular?

  </Pql>
</PqlChat>

### Step 10. Iterate on your source's schema

If something changes in your data source's schema, you can iterate on your data model by following the steps in the
[iteration guide](/data-modeling/iterate.mdx).


<BoilerplateSummary dataSourceName="PostgreSQL" />



==============================



# with-snowflake.mdx

URL: https://hasura.io/docs/promptql/how-to-build-with-promptql/with-snowflake


# Get Started with PromptQL and Snowflake

<BoilerplateOverview dataSourceName="Snowflake" time="twenty" />


## Prerequisites

**Install the DDN CLI**

:::info Minimum version requirements

To use this guide, ensure you've installed/updated your CLI to at least `v2.28.0`.

:::


<Tabs groupId="os-preference" className="api-tabs">

<TabItem value="macOS-or-linux" label="macOS and Linux">

Simply run the installer script in your terminal:

<CodeBlock language="bash">
  {`curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/get.sh | bash`}
</CodeBlock>

<Admonition type="info" title="ARM-based Linux Machines">
  Currently, the CLI does not support installation on ARM-based Linux systems.
</Admonition>

</TabItem>

<TabItem value="windows" label="Windows">
- Download the latest <a href={`https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/latest/DDN_CLI_Setup.exe`}>DDN CLI installer for Windows.</a> 
- Run the `DDN_CLI_Setup.exe` installer file and follow the instructions. This will only take a minute.
- By default, the DDN CLI is installed under `C:\Users\{Username}\AppData\Local\Programs\DDN_CLI`
- The DDN CLI is added to your `%PATH%` environment variable so that you can use the `ddn` command from your terminal.
<br />

</TabItem>
</Tabs>


**Install [Docker](https://docs.docker.com/engine/install/)**

The Docker-based workflow helps you iterate and develop locally without deploying any changes to Hasura DDN, making the
development experience faster and your feedback loops shorter. **You'll need Docker Compose `v2.20` or later.**

**Validate the installation**

You can verify that the DDN CLI is installed correctly by running:

```ddn
ddn doctor
```


## Tutorial

### Step 1. Authenticate your CLI

```ddn title="Before you can create a new Hasura DDN project for PromptQL, you need to authenticate your CLI:"
ddn auth login
```

This will launch a browser window prompting you to log in or sign up for Hasura DDN. After you log in, the CLI will
acknowledge your login, giving you access to Hasura Cloud resources.

### Step 2. Scaffold out a new local project

```ddn title="Next, create a new local project:"
ddn supergraph init my-project --with-promptql && cd my-project
```

Once you move into this directory, you'll see your project scaffolded out for you. You can view the structure by either
running `ls` in your terminal, or by opening the directory in your preferred editor.

### Step 3. Initialize your connector

```ddn title="In your project directory, run:"
ddn connector init my_connector -i
```


Select `hasura/snowflake` (you can type to filter the list). Then, enter the following environment variables:

| ENV        | Example                                                                                                                                                                                       | Description                         |
| ---------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------- |
| `JDBC_URL` | `jdbc:snowflake://<account-identifier.<region>.snowflakecomputing.com?user=YOUR_USERNAME&&password=YOUR_PASSWORD&db=YOUR_DATABASE&warehouse=YOUR_WAREHOUSE&schema=YOUR_SCHEMA&role=YOUR_ROLE` | This connector requires a JDBC URL. |

:::info Optional parameters for Snowflake

Snowflake allows you to set a number of defaults. Your JDBC can minimal (i.e., only including the account identifier,
username, and password) or more granular depending on your settings within Snowflake.

:::


### Step 4. Introspect your source

```ddn title="Next, use the CLI to introspect your source:"
ddn connector introspect my_connector
```

After running this, you should see a representation of your source's schema in the
`app/connector/my_connector/configuration||config.json` file; you can view this using `cat` or open the file in your
editor.

```ddn title="Additionally, you can check which resources are available — and their status — at any point using the CLI:"
ddn connector show-resources my_connector
```

### Step 5. Add your resources

```ddn title="Add your resources to create metadata:"
ddn model add my_connector "*"
ddn command add my_connector "*"
ddn relationship add my_connector "*"
```

Open the `app/metadata` directory and you'll find newly-generated file(s) ending in `.hml`. The DDN CLI will use these
Hasura Metadata Language files to represent your data source to PromptQL as
[models](/reference/metadata-reference/models.mdx), [commands](/reference/metadata-reference/commands.mdx), and
[relationships](/reference/metadata-reference/relationships.mdx).

### Step 6. Add semantic information to your metadata (optional)

It is highly recommended to provide extra natural language descriptions of the resources in your project so that the LLM
can better understand your data and create appropriate query plans.

The description field can be added to `Model`, `Command` and `Relationship` metadata objects to provide semantic
context. See more about [semantic information here](/data-modeling/semantic-information.mdx).

### Step 7. Create a new build

```ddn title="To create a local build, run:"
ddn supergraph build local
```

The build is stored as a set of JSON files in `engine/build`.

### Step 8. Start your local services

```ddn title="Start your local Hasura DDN Engine and connector:"
ddn run docker-start
```

Your terminal will be taken over by logs for the different services.

### Step 9. Chat with your data

```ddn title="In a new terminal tab, open your local console:"
ddn console --local
```

Once the PromptQL interface is open, ask a question about your data. For example:

<PqlChat>
  <User>Hi, what are some questions you can answer?</User>
  <Pql>
    I can help you analyze data from what appears to be an e-commerce system. Here are some types of questions I can answer:

    <ol>
        <li>Product-related:</li>
        <ul>
            <li>Find popular products based on orders or reviews</li>
            <li>Analyze product pricing across categories</li>
            <li>Look up product details and descriptions</li>
            <li>Find products from specific manufacturers or countries</li>
        </ul>
        <li>Order and Purchase Analysis:</li>
        <ul>
            <li>Track order statuses</li>
            <li>Analyze delivery times</li>
            <li>Look at purchase patterns</li>
            <li>Find abandoned carts</li>
        </ul>
        <li>Customer Behavior:</li>
        <ul>
            <li>Analyze customer reviews and ratings</li>
            <li>Look at shopping patterns</li>
            <li>Track cart abandonment</li>
            <li>Study product category preferences</li>
        </ul>
        <li>Business Metrics:</li>
        <ul>
            <li>Calculate sales metrics</li>
            <li>Analyze review scores</li>
            <li>Track coupon usage</li>
            <li>Monitor cart completion rates</li>
        </ul>
    </ol>

    Would you like me to explore any of these areas in particular?

  </Pql>
</PqlChat>

### Step 10. Iterate on your source's schema

If something changes in your data source's schema, you can iterate on your data model by following the steps in the
[iteration guide](/data-modeling/iterate.mdx).


<BoilerplateSummary dataSourceName="Snowflake" />



==============================



# with-others.mdx

URL: https://hasura.io/docs/promptql/how-to-build-with-promptql/with-others


# Get Started with PromptQL and other Sources

On the [Connector Hub](https://hasura.io/connectors), you can find information about our wealth of data connectors.
These data connectors allow you to connect nearly any data source — be them relational, vector, custom business logic,
or even a third-party API — to Hasura DDN and build an API on top of it.

We recommend using the [Quickstart](/quickstart.mdx) and referencing the individual connector's docs from ☝️ to get
started with a connector-specific setup.



==============================



# overview.mdx

URL: https://hasura.io/docs/promptql/data-sources/overview

# Basics

## What is a data source?

In PromptQL, a data source is anything you can ask questions about.

That includes traditional databases like Postgres and MySQL, but also REST APIs, GraphQL endpoints, and even custom
functions or services. If it returns data, it can be a data source.

## How do I connect my data (and what happens next)?

We use **native data connectors** to make your data accessible through PromptQL. These connectors are designed for
specific systems: relational databases, NoSQL, REST APIs, GraphQL APIs, and more.

Once your data sources are connected, PromptQL acts as the interface. You ask a question in plain language, and PromptQL
figures out what data it needs, pulls the right pieces from one or more sources, and assembles an accurate, reliable
result.

You don't need to worry about where the data lives — PromptQL can traverse across your connected sources to answer your
question.

You can get started by choosing one of the supported connectors:

- Amazon Athena
- Amazon Redshift
- BigQuery
- Databricks
- MySQL
- PostgreSQL
- Snowflake
- TypeScript Lambda
- Python Lambda
- Go Lambda

## Building your own connector

If you're working with a custom data source, you can create your own connector. The
[NDC Spec](https://github.com/hasura/ndc-spec) describes how this works, and we offer SDKs and a full tutorial to guide
you.

- [Tutorial](https://hasura.github.io/ndc-spec/tutorial/index.html)
- SDKs:
  - [Rust](https://github.com/hasura/ndc-sdk-rs)
  - [TypeScript](https://github.com/hasura/ndc-sdk-typescript)
  - [Python](https://github.com/hasura/ndc-sdk-python)
  - [Go](https://github.com/hasura/ndc-sdk-go)

## Next steps

- [Connect your data](/data-sources/connect-to-a-source.mdx)



==============================



# connect-to-a-source.mdx

URL: https://hasura.io/docs/promptql/data-sources/connect-to-a-source


# Connect to a source

## Introduction

This guide explains how to initialize a connector, configure its environment variables, and link it to your data source.
Once initialized, you'll be ready to introspect the source and integrate it into your PromptQL application.

You'll need a [project](/reference/cli/commands/ddn_supergraph_init.mdx) before initializing a connector.

## Step 1. Initialize a connector

Regardless which connector you're using, you'll always begin by initializing it with a unique name:

```ddn title="Initialize a new connector in a project directory:"
ddn connector init <your_name_for_the_connector> -i
```

A wizard will appear with a dropdown list. If you know the name of the connector, start typing the name. Otherwise, use
the arrows to scroll through the list. Hit `ENTER` when you've selected your desired connector; you'll then be prompted
to enter some values.

:::info Customization

You can customize which subgraph this connector is added to by
[changing your project's context](/reference/cli/commands/ddn_context.mdx) or using flags. More information can be found
in the [CLI docs](/reference/cli/commands/ddn_connector_init.mdx) for the `ddn connector init` command.

:::

## Step 2. Add environment variables

The CLI will assign a random port for the connector to use during local development. You can hit `ENTER` to accept the
suggested value or enter your own. Then, depending on your connector, there may be a set of environment variables that
it requires:

<Tabs groupId="source-preference" className="api-tabs">

<TabItem value="Athena" label="Athena">

| ENV            | Example                                                                      | Description                                            |
| -------------- | ---------------------------------------------------------------------------- | ------------------------------------------------------ |
| `JDBC_URL`     | `jdbc:athena://<host>:<port>/<database>?user=<username>&password=<password>` | The JDBC URL to connect to the Amazon Athena database. |
| `JDBC_SCHEMAS` | `public,app`                                                                 | The schemas to use for the database.                   |

</TabItem>

<TabItem value="BigQuery" label="BigQuery">

| ENV        | Example                                                                                                                                                                                               | Description                                       |
| ---------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------- |
| `JDBC_URL` | `jdbc:bigquery://https://www.googleapis.com/bigquery/v2:443;ProjectId=project-id;DefaultDataset=dataset;OAuthType=0;OAuthServiceAcctEmail=service-account-email;OAuthPvtKey=/etc/connector/key.json;` | The JDBC URL to connect to the BigQuery database. |

</TabItem>

<TabItem value="Databricks" label="Databricks">

| ENV            | Example                                                                                                                                                                  | Description                                                                                                                        |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------- |
| `JDBC_URL`     | `jdbc:databricks://<host>:<port>/default;transportMode=http;ssl=1;AuthMech=3;httpPath=/sql/1.0/warehouses/<warehouse-id>;UID=token;PWD=<access-token>;ConnCatalog=main;` | You can construct the base of this using your Databricks UI under `SQL Warehouses` » `<name-of-warehouse>` » `Connection details`. |
| `JDBC_SCHEMAS` | `default,public`                                                                                                                                                         | A comma-separated list of schemas within the referenced catalog.                                                                   |

</TabItem>
<TabItem value="MySQL" label="MySQL">

| ENV        | Example                                        | Description                         |
| ---------- | ---------------------------------------------- | ----------------------------------- |
| `JDBC_URL` | `jdbc:mysql://user:password@host:3306/db_name` | This connector requires a JDBC URL. |

</TabItem>
<TabItem value="PostgreSQL" label="PostgreSQL">

| ENV            | Example                                                                          | Description                                                                                        |
| -------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- |
| `JDBC_URL`     | `jdbc:postgresql://<host>:<port>/<database>?user=<username>&password=<password>` | The JDBC URL to connect to the PostgreSQL database.                                                |
| `JDBC_SCHEMAS` | `public, app`                                                                    | The schemas to use for the database. Optional. This can also be included in the connection string. |

</TabItem>
  <TabItem value="Redshift" label="Redshift">

| ENV            | Example                                                                        | Description                                              |
| -------------- | ------------------------------------------------------------------------------ | -------------------------------------------------------- |
| `JDBC_URL`     | `jdbc:redshift://<host>:<port>/<database>?user=<username>&password=<password>` | The JDBC URL to connect to the Amazon Redshift database. |
| `JDBC_SCHEMAS` | `public,app`                                                                   | The schemas to use for the database.                     |

</TabItem>

<TabItem value="Snowflake" label="Snowflake">

| ENV        | Example                                                                                                                                                                                       | Description                         |
| ---------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------- |
| `JDBC_URL` | `jdbc:snowflake://<account-identifier.<region>.snowflakecomputing.com?user=YOUR_USERNAME&&password=YOUR_PASSWORD&db=YOUR_DATABASE&warehouse=YOUR_WAREHOUSE&schema=YOUR_SCHEMA&role=YOUR_ROLE` | This connector requires a JDBC URL. |

:::info Optional parameters for Snowflake

Snowflake allows you to set a number of defaults. Your JDBC can minimal (i.e., only including the account identifier,
username, and password) or more granular depending on your settings within Snowflake.

:::

</TabItem>
</Tabs>

:::note Escape characters

If your connection string contains special characters, you may need to escape them.

:::

If your data source requires a connection string or endpoint, the CLI will confirm that it successfully tested the
connection to your source. Additionally, it generates configuration files, which you can find in the `connector`
directory of the subgraph where you added the connector (default: `app`). Finally, the CLI will create a
[DataConnectorLink](/reference/metadata-reference/data-connector-links.mdx) in your connector's `metadata` directory.

## Next steps

Now that you've initialized a connector and connected it to your data, you're ready to introspect the source and
populate the configuration files with source-specific information that Hasura will need to build your application. Check
out the [introspection page](/data-sources/introspect-a-source.mdx) to learn more.



==============================



# introspect-a-source.mdx

URL: https://hasura.io/docs/promptql/data-sources/introspect-a-source

# Introspect a source

## Introduction

This guide explains how to use a connector to introspect a data source. After introspection, you'll be ready to begin
generating Hasura metadata that represents resources (such as tables or collections) in your data source.

This metadata will act as the blueprint for your application and will empower PromptQL with the structure it needs to
understand your data, generate accurate responses, and take meaningful action.

You'll need an [initialized data connector](/data-sources/connect-to-a-source.mdx) before attempting to introspect a
source.

## Step 1. Run the introspect command

```ddn title="All connectors use the same command to introspect a data source:"
ddn connector introspect <name_of_connector>
```

:::tip Is your Docker daemon running?

When you run the introspection command, the connector will start. As it runs as a Docker service, the Docker daemon must
be running.

:::

The connector will reach out to your data source and update the configuration files with information about your source
schema. This will vary depending on the connector but, at a minimum, you'll see source-specific information updated in
two files, both located in your connector's directory:

| File                      | Description                                                                                                                                                                                                                           |
| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `configuration.json`      | An [Open Data Domain Specification](https://github.com/hasura/open-data-domain-specification) description of your data source. The CLI uses this file to update the DataConnectorLink object with information about your data source. |
| `<name_of_connector>.hml` | Contains the [DataConnectorLink](/reference/metadata-reference/data-connector-links.mdx) for your connector.                                                                                                                          |

:::info When to use this command

This command will be used at least once with every connector; Hasura **must** introspect your data source in order to
understand the resources that are present.

**Additionally, any time you modify your source schema, you should re-run this command to ensure your Hasura metadata is
up-to-date with your source schema.**

:::

## Next steps

With your source introspected and the configuration files populated, you can now start creating Hasura metadata that
will represent your source's schema in your application. Check out the [Data Modeling](/data-modeling/overview.mdx)
section to begin creating models, commands, and relationships for your data.



==============================



# troubleshooting.mdx

URL: https://hasura.io/docs/promptql/data-sources/troubleshooting

# Troubleshooting

## Common issues

### My connector can't connect to the data source

When initializing a new connector, we verify the connection string you provide to ensure your data source is reachable.
If this process fails, try the following solutions:

#### Your source isn't accessible to the public internet

If your data source is hosted behind a firewall or restricts access to specific IP addresses, it may block external
connections. Unless you're using [Private DDN](https://hasura.io/pricing), you need to allow access from all IP
addresses. You can do this by adding the following to your allowlist:

```plaintext
0.0.0.0/0
```

#### Docker networking issues

Hasura DDN resolves `local.hasura.dev` to your machine's `localhost`. This avoids Docker networking conflicts, where
`localhost` might point to your source's container instead of your local machine. To fix this, update any `localhost`
references in your connection strings to `local.hasura.dev`.

### My connector won't introspect my data source

If you encounter an error like the following:

```plaintext
ERR Failed building the container: exit status 17
```

You'll likely see the following in the logs:

```plaintext
Failed to resolve source metadata for ghcr.io/hasura/<connector>: failed to authorize: failed to fetch oauth token: unexpected status from GET request to https://ghcr.io/token?scope=repository%3Ahasura%2F<connector>%3Apull&service=ghcr.io: 403 Forbidden
```

When this happens, your GitHub access token located in your Docker credentials store is likely out-of-date.

To resolve this, generate a new PAT with `package` scope and run the following:

```sh
echo "<GH_PAT>" | docker login ghcr.io -u <YOUR_GITHUB_USERNAME> --password-stdin
```

Your terminal should return `Login Succeeded`.

### There's a port conflict for my connector

Use `docker ps` to check for any running processes that would conflict with your source's container. If you find one,
kill it and then try initializing the connector again.

### My connector isn't reflecting schema changes in my data source

You may have not re-run the introspection steps. Remember, each time your data source's schema changes, you'll need to
re-run this command to make Hasura DDN aware of the new schema. Follow the guide
[here](/data-sources/introspect-a-source.mdx).

### My connector isn't reflecting schema changes in my application

If you've modified your metadata and are getting errors when trying to use PromptQL, you may need to force a rebuild of
the connector.

Identify the connector's container and image; kill the container and delete the image. Then, re-run the
`ddn run docker-start` command to rebuild the connector's image in addition to your other services.

### My data source operations time out

If a data source operation like `ddn introspect` hangs, eventually times out, and you encounter a log entry like the
following:

```plaintext
INF Waiting for the Connector service to be healthy...
```

Follow these steps to resolve the issue:

#### Step 1: Enable Debug Logging

Add the `--log-level DEBUG` flag to your command to generate more detailed logs. If the debug logs include the following
entry:

```plaintext
dial tcp: lookup local.hasura.dev: no such host
```

Then the issue is related to DNS resolution.

#### Step 2: Diagnose the DNS Issue

The most common cause of this DNS error is that your DHCP server is setting the Domain Search option. This is specified
in [DHCP Option 119/RFC 3397](https://www.rfc-editor.org/rfc/rfc3397).

#### Step 3: Resolve the DNS Issue

To resolve this issue, you have two options:

- **Disable the Domain Search Option**

  Disabling this option in your DHCP server settings will prevent the DNS error. Consult your network administrator if
  you are unsure how to make this change.

- **Add a Static Hostname Entry**

  If disabling the Domain Search option is not feasible, you can manually add a static entry to your hosts file. This
  approach will override DNS lookup for the hostname `local.hasura.dev`.

  **Windows:** Follow the instructions
  [here](https://www.howtogeek.com/27350/beginner-geek-how-to-edit-your-hosts-file/) to edit your hosts file.

  **Linux/Unix/MacOS:** Add the following line to your `/etc/hosts` file:

  ```plaintext
  127.0.0.1 local.hasura.dev
  ```

  This entry maps the hostname `local.hasura.dev` to the IP address `127.0.0.1` and does not interfere with other
  applications.

By following these steps, you should be able to resolve the introspection hang or timeout issue with your data source.

### I have a lot of tables

Typically, you won't experience any issues when connecting to or introspecting a database with a large number of tables.
However, _builds_ of your supergraph may exceed timeouts given the size of your project.

```ddn title="If possible, consider removing unused models via the CLI:"
ddn model remove <table>
```

```ddn title="Then, retry your supergraph build:"
ddn supergraph build <local|create>
```

:::info I need all of my tables

If all tracked tables are needed, and you're running into build issues, [raise a ticket](/help/overview.mdx).

:::

## Get help

Each connector has a public repository on GitHub. Typically, they'll follow the naming convention of
`hasura/ndc-<connector-name>`. Create issues on these repositories to get help directly from the teams responsible for
the connectors.

You can search the list of public repositories [here](https://github.com/orgs/hasura/repositories).



==============================



# publish-your-own-connector.mdx

URL: https://hasura.io/docs/promptql/data-sources/publish-your-own-connector

# Publish a Data Connector

## Introduction

This guide is for **connector authors** and details the automated process for publishing new connectors and updating
existing versions on the [Hasura Connector Hub](https://hasura.io/connectors), which is powered by
[the `ndc-hub` repository](https://github.com/hasura/ndc-hub).

The automated publication process simplifies and accelerates the deployment of NDC connectors by:

- Managing documentation updates
- Streamlining the approval workflow
- Uploading connector packages to Hasura DDN
- Updating the Hasura Hub Registry database so connectors can be deployed on DDN
- Making a new connector/connector version available in the DDN ecosystem

:::info How do I build a connector?

You can find information about building a connector in the [NDC Spec](https://github.com/hasura/ndc-spec) and the
rendered [NDC Spec data connector tutorial](https://hasura.github.io/ndc-spec/tutorial/index.html) as well as the SDKs
for various languages:

- [Rust](https://github.com/hasura/ndc-sdk-rs)
- [TypeScript](https://github.com/hasura/ndc-sdk-typescript)
- [Python](https://github.com/hasura/ndc-sdk-python)
- [Go](https://github.com/hasura/ndc-sdk-go)

:::

## Publish a new connector

### Step 1. Clone the repository

```sh title="Clone the ndc-hub repository:"
git clone https://github.com/hasura/ndc-hub.git
```

### Step 2. Create a new directory for your connector in the registry {#step-2}

Create a new folder structure in [the `registry` directory](https://github.com/hasura/ndc-hub/tree/main/registry) of the
`ndc-hub` repo:

```plaintext
registry/
  └── [namespace]/
      └── [connector-name]/
```

Replace `[namespace]` with your organization's namespace and `[connector-name]` with the name of your connector.

### Step 3. Create a `connector-packaging.json` {#step-3}

Within your newly created connector's directory in the registry, create a new `releases` directory with a folder for
your first release and a `connector-packaging.json` for it:

```plaintext {4-6}
registry/
  └── [namespace]/
      └── [connector-name]/
          └── releases/
              └── v1.0.0/
                  └── connector-packaging.json
```

#### Format

Below is the required shape for the `connector-packaging.json` file for each connector version:

```json
{
  "version": "1.0.0",
  "uri": "https://github.com/hasura/ndc-mongodb/releases/download/v0.0.1/connector-definition.tgz",
  "checksum": {
    "type": "sha256",
    "value": "2cd3584557be7e2870f3488a30cac6219924b3f7accd9f5f473285323843a0f4"
  },
  "source": {
    "hash": "c32adbde478147518f65ff465c40a0703239288a"
  }
}
```

#### Required fields:

- **version**: The version of the connector (e.g., "1.0.0").
- **uri**: The URL to download the connector package. This should be a tarball containing the connector package
  definition and must be accessible without authentication.
- **checksum**: The checksum of the connector package.
  - **type**: The algorithm used for the checksum (currently only "sha256" is supported).
  - **value**: The actual checksum value.
- **source**: Information about the source code used to build the package.
  - **hash**: The commit hash of the source code used to build the connector package.

Ensure all fields are correctly filled out for each new connector version you publish.

### Step 4. Add the other required files

Add the following files to your connector's directory in the registry; details are below.

```plaintext {5-7}
registry/
  └── [namespace]/
      └── [connector-name]/
          ├─ releases/
          ├─ logo.(png|svg)
          ├─ metadata.json
          └─ README.md
```

- `logo.png` or `logo.svg`: The logo for your connector (preferably in SVG format).
- `README.md`: Documentation for your connector, including description, usage instructions, and any other relevant
  information.
- `metadata.json`: Metadata file containing information about the connector.

The `metadata.json` file in your connector's folder contains crucial information about your connector:

```json
{
  "overview": {
    "namespace": "your_namespace",
    "description": "A brief description of your connector",
    "title": "Your Connector Title",
    "logo": "logo.png",
    "tags": [],
    "latest_version": "v1.0.0"
  },
  "author": {
    "support_email": "support@example.com",
    "homepage": "https://www.example.com",
    "name": "Your Organization Name"
  },
  "is_verified": false,
  "is_hosted_by_hasura": false,
  "source_code": {
    "is_open_source": true,
    "repository": "https://github.com/your-org/your-connector-repo"
  }
}
```

**Field explanations:**

- **overview**: General information about your connector.

  - **namespace**: Your organization's namespace (e.g., `sqlserver` for the SQL Server connector).
  - **description**: A brief description of your connector.
  - **title**: The title of your connector (e.g., BigQuery, SQL Server).
  - **logo**: Filename containing your connector logo (acceptable formats: PNG, SVG).
  - **tags**: Keywords related to your connector.
  - **latest_version**: The most recent version of your connector.

- **author**: Information about the connector's author or organization.
- **is_verified**: Set this to `false` as only connectors developed by Hasura can be tagged as verified.
- **is_hosted_by_hasura**: Set this to `false` as connectors not developed by Hasura cannot be hosted by Hasura.

### Step 5. Submit for review

1. Commit all your changes to a new branch.
2. Create a pull request targeting the `main` branch of the `ndc-hub` repository.
3. In the pull request description provide any additional context about your new connector.
4. Wait for review and approval from the Hasura team.

#### Post-approval process

Once your pull request is approved and merged:

- The new connector will be added to the Hasura Hub Registry.
- It will become available in the staging environment for testing.
- After successful testing, it will be published to the production environment.

## Create a new release for existing connectors

### Step 1. Check repository structure

Ensure your connector and connector versions follow this directory structure in the `ndc-hub` repository:

```
registry/
  ├── [namespace]/
  │   ├── [connector-name]/
  │   │   ├── releases/
  │   │   │   ├── [version]/
  │   │   │   │   └── connector-packaging.json
  │   │   ├── logo.(png|svg)
  │   │   └── README.md
```

### Step 2. Create a new connector version

1. Create a new folder under `registry/[namespace]/[connector-name]/releases/` with the version number.
2. The version must start with the letter 'v', for example: `v1.0.0`.
3. Add a new `connector-packaging.json` file in this folder with the connector metadata.

### Step 3. Update connector information

- To update the logo: Modify the `logo.png` or `logo.svg` file in the connector's root folder.
- To update the documentation: Modify the `README.md` file in the connector's root folder.

### Step 4. Create a PR

1. Commit your changes to a new branch.
2. Create a pull request targeting the `main` branch of the `ndc-hub` repository.
3. Wait for approval from a Hasura team member.

#### Post-approval process

Once the pull request is approved, the GitHub workflow will automatically:

- Run the registry automation program.
- Upload connector packages.
- Update the Hasura Hub Registry database.

## Development workflow

The connector publication automation is designed to run automatically for every commit made to a pull request targeting
the `main` branch. This process ensures that your changes are continuously validated and updated in our staging
environment. Here's how it works:

### PR creation

When you create a pull request against the `main` branch with changes in the registry folder, it triggers the automation
process.

### Commit-based triggers

Every new commit to the pull request will trigger
[the `registry-updates` GitHub Actions workflow](https://github.com/hasura/ndc-hub/blob/main/.github/workflows/registry-updates.yaml).
This includes:

- Initial commits when opening the PR.
- Additional commits pushed to the same PR.
- Commits made in response to review comments.

### Staging environment updates

For each commit:

- The `registry-updates` workflow runs automatically.
- It validates the changes in the registry folder, including the `connector-packaging.json` file.
- If validation is successful, it updates the connector information on the `staging` Hasura DDN environment.
- Each new commit overwrites the previous version of that connector on the `staging` environment.

### Continuous updates

This process allows for continuous iteration and testing in the staging environment:

- You can push multiple commits to refine your connector changes.
- Each commit provides a new opportunity to test and verify your changes in the staging environment.
- Reviewers can check the latest version of your connector in the staging DDN at any time during the review process.

### Production deployment

Once the pull request is approved and merged into the `main` branch:

- The final version of the connector (based on the last commit in the PR) is automatically published to the `production`
  Hasura DDN.
- This process ensures that only thoroughly reviewed and tested connector versions reach the `production` environment.

### Important Notes

- The automation only triggers for changes made in the `registry` folder.
- Ensure your `connector-packaging.json` file is valid for each commit to avoid automation failures.
- Multiple connector versions or updates to multiple connectors can be included in a single PR, and the automation will
  handle all changes appropriately.

## Troubleshooting

### Pull request automation doesn't trigger.

Ensure the PR targets the main branch and modifies files under the `registry/` directory.

### Automation fails due to missing environment variables.

Ping `@scriptnull` or `@codingkarthik` in the PR.



==============================



# Business Logic

URL: https://hasura.io/docs/promptql/business-logic/overview

# Business Logic

You can use lambda connectors to give PromptQL abilities to execute custom business logic on behalf of the user. We can
add a lamdba connector runtime in Node.js with **TypeScript**, **Python**, or **Go**, and expose functions to PromptQL
as tools it can use. This means PromptQL isn't limited to querying data — it can **trigger logic**, **run workflows**,
or **transform inputs into actions**.

- [Lambda Connector Basics](/business-logic/add-a-lambda-connector.mdx)
- [Add Custom Environment Variables](/business-logic/add-env-vars-to-a-lambda.mdx)
- [Live Reloading](/business-logic/dev-mode.mdx)
- [Debug and Handle Errors](/business-logic/errors.mdx)



==============================



# add-a-lambda-connector.mdx

URL: https://hasura.io/docs/promptql/business-logic/add-a-lambda-connector


# Lambda Connector Basics

## Introduction

You can use lambda connectors to give PromptQL abilities to execute custom business logic on behalf of the user. We can
add a lamdba connector runtime in Node.js with **TypeScript**, **Python**, or **Go**, and expose functions to PromptQL
as tools it can use. This means PromptQL isn't limited to querying data — it can **trigger logic**, **run workflows**,
or **transform inputs into actions**, all within a secure and consistent API environment.

By treating logic like a first-class data source, PromptQL ensures your application has a unified surface for
interacting with databases, API services, and whatever actions you want your application to be able to take. You define
how the system should respond to user queries, apply business rules, or even call third-party APIs.

## Initialize a lambda connector

```ddn title="Initialize a new connector in a project directory:"
ddn connector init your_name_for_the_connector -i
```

Choose the lambda connector to correspond with the language you'd like to use for your functions.

<Tabs groupId="source-preference" className="api-tabs">

<TabItem value="TypeScript" label="TypeScript">

When you add the `hasura/nodejs` connector, the CLI will generate a Node.js package with a `functions.ts` file. This
file is the entrypoint for your connector.

As this is a Node.js project, you can easily add any dependencies you desire by running `npm i <package-name>` from this
connector's directory.

</TabItem>
<TabItem value="Python" label="Python">

When you add the `hasura/python` connector, the CLI will generate a Python application with a `functions.py` file. This
file is the entrypoint for your connector.

As this is a Python project, you can easily add any dependencies you desire by adding them to the `requirements.txt` in
this connector's directory.

</TabItem>
<TabItem value="Go" label="Go">

When you add the `hasura/go` connector, the CLI will generate a Go application with a `/functions` directory. The
connector will use this directory — and any `*.go` file in it — as the entrypoint for your connector.

As this is a Go project, you can easily add any dependencies you desire by adding them to the `go.mod` file and running
`go mod tidy` from this connector's directory.

</TabItem>

</Tabs>

## Write a function

There are two types of lambdas you can write, functions and procedures.

- Functions are read-only. Queries without side effects. PromptQL will not ask for confirmation before calling them.
- Procedures can mutate data and have side effects. PromptQL will ask for confirmation before calling them.

### Examples

The following examples show how to create basic lambda functions in each language. You can replace the contents of the
`functions.ts`, `functions.py`, or any `.go` file in the `/functions` directory of the Go connector with the following
examples.

<Tabs groupId="source-preference" className="api-tabs">

<TabItem value="TypeScript" label="TypeScript">

```ts title="<subgraph-name>/connector/<connector-name>/functions.ts"
/**
 * Takes an optional name parameter and returns a friendly greeting string
 * @param {string} [name] - Optional name to personalize the greeting
 * @returns {string} A greeting in the format "hello {name}" or "hello world" if no name provided
 * @readonly
 */
export function hello(name?: string) {
  return `hello ${name ?? "world"}`;
}
```

The JSDoc comments are optional, but the first general comment is highly recommended to help PromptQL understand the
function's purpose and parameters and will be added to the function's metadata.

The `@readonly` tag indicates that the function does not modify any data, and PromptQL will be able to call this without
asking for confirmation. Under the hood, DDN will create an NDC function for `@readonly` lambdas and an NDC procedure
for functions that are not marked as `@readonly`.

</TabItem>
<TabItem value="Python" label="Python">

```python title="<subgraph-name>/connector/<connector-name>/functions.py"
from hasura_ndc import start
from hasura_ndc.function_connector import FunctionConnector
from typing import Annotated

connector = FunctionConnector()

@connector.register_query
def hello(name: str | None = None) -> str:
    """
    Takes an optional name parameter and returns a friendly greeting string
    """
    return f"hello {name or 'world'}"

if __name__ == "__main__":
    start(connector)
```

The docstring comments are optional, but they're highly recommended to help PromptQL understand the function's purpose
and parameters and will be added to the function's metadata.

The `register_query` decorator indicates that the function does not modify any data, and PromptQL will be able to call
this without asking for confirmation. To create functions that modify data, use the `register_mutation` decorator
instead.

</TabItem>
<TabItem value="Go" label="Go">

```go title="<subgraph-name>/connector/<connector-name>/functions/hello.go"
package functions

	"context"
	"fmt"

	"hasura-ndc.dev/ndc-go/types"
)

// HelloArguments defines the input parameters
type HelloArguments struct {
	Name *string `json:"name"` // Pointer makes it optional
}

// HelloResult defines the return type
type HelloResult struct {
	Greeting string `json:"greeting"`
}

// FunctionHello takes an optional name parameter and returns a friendly greeting string
func FunctionHello(ctx context.Context, state *types.State, args *HelloArguments) (*HelloResult, error) {
	name := "world"
	if args.Name != nil {
		name = *args.Name
	}

	return &HelloResult{
		Greeting: fmt.Sprintf("hello %s", name),
	}, nil
}
```

Function names are important as they determine how the function will be exposed in the API:

- Functions starting with `Function` (like `FunctionHello`) are treated as queries (read-only)
- Functions starting with `Procedure` (like `ProcedureCreateUser`) are treated as mutations (data modifications)

The function documentation is highly recommended to help PromptQL understand the function's purpose and parameters and
will be added to the function's metadata.

</TabItem>

</Tabs>

## Exposing your lambda functions {#exposing-your-lambda-functions}

Once you've created your lambda functions, you need to expose them to PromptQL by generating metadata for them.

### Step 1. Introspect the connector and add the metadata

```ddn
ddn connector introspect <connector-name>
```

```ddn title="List the commands discovered during introspection:"
ddn connector show-resources <connector-name>
```

You should see the command being `AVAILABLE` which means that there's not yet metadata representing it.

```ddn
ddn commands add <connector-name> <command-name>
```

:::info Add semantic metadata

If you did not add comments to your function, we highly recommend adding a `description` to the command object added
above.

PromptQL's performance is improved by providing more context; if you guide its understanding of what a particular
function does and how it should be called, you'll get better results and fewer errors.

:::

### Step 2. Create and run a new build and test the function

```ddn title="Create a new local build:"
ddn supergraph build local
```

```ddn title="Run your services:"
ddn run docker-start
```

```ddn title="In a new terminal tab, open the devlopment console:"
ddn console --local
```

Head over to the PromptQL Playground and ask PromptQL to call your lambda function.

```plaintext
say hello to everyone
```

## Retrieve information

You can add a lambda function which reaches out to external APIs to retrieve data which PromptQL can use.

### Step 1. Call an external API

<Tabs groupId="source-preference" className="api-tabs">

<TabItem value="TypeScript" label="TypeScript">

Open the `app/connector/typescript/functions.ts` file.

```ts title="Replace the contents with the following:"
/**
 * Calls httpbin.org API to create a personalized greeting for the given name. Takes an optional name parameter and returns a friendly greeting string.
 * @param {string} [name] - Optional name to personalize the greeting
 * @returns {Promise<{ greeting?: string }>} A Promise resolving to an object containing the optional greeting message
 * @readonly
 */
export async function helloFromHttpBin(name?: string): Promise<{ greeting?: string }> {
  const greeting = { greeting: name };

  const response = await fetch("https://httpbin.org/post", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
    },
    body: JSON.stringify({ greeting: `Hello ${name}!` }),
  });

  const data: any = await response.json();
  return { greeting: data?.json?.greeting };
}
```

</TabItem>
<TabItem value="Python" label="Python">

Open the `app/connector/python/functions.py` file.

```python title="Replace the contents with the following:"
from hasura_ndc import start
from hasura_ndc.function_connector import FunctionConnector

connector = FunctionConnector()

@connector.register_query
async def hello_from_http_bin(name: str | None = None) -> dict:
    """
    Calls httpbin.org API to create a personalized greeting for the given name.
    Takes an optional name parameter and returns a friendly greeting string.
    """
    response = requests.post(
        "https://httpbin.org/post",
        json={"greeting": f"Hello {name}!"}
    )

    data = response.json()
    return {"greeting": data.get("json", {}).get("greeting")}

if __name__ == "__main__":
    start(connector)
```

</TabItem>
<TabItem value="Go" label="Go">

Open a new file in the functions directory:

```go title="<subgraph-name>/connector/<connector-name>/functions/hello_from_http_bin.go"
package functions

	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"

	"hasura-ndc.dev/ndc-go/types"
)

// HelloFromHttpBinArguments defines the input parameters
type HelloFromHttpBinArguments struct {
	Name *string `json:"name"` // Optional name parameter
}

// HelloFromHttpBinResponse defines the return type
type HelloFromHttpBinResponse struct {
	Greeting *string `json:"greeting"` // Optional greeting response
}

// HTTPBinResponse represents the response from httpbin.org
type HTTPBinResponse struct {
	JSON struct {
		Greeting string `json:"greeting"`
	} `json:"json"`
}

// FunctionHelloFromHttpBin calls httpbin.org API to create a personalized greeting
func FunctionHelloFromHttpBin(ctx context.Context, state *types.State, args *HelloFromHttpBinArguments) (*HelloFromHttpBinResponse, error) {
	// Prepare the name to use
	name := "world"
	if args.Name != nil {
		name = *args.Name
	}

	// Create the request payload
	payload, err := json.Marshal(map[string]string{
		"greeting": fmt.Sprintf("Hello %s!", name),
	})
	if err != nil {
		return nil, fmt.Errorf("failed to create request payload: %w", err)
	}

	// Send the POST request to httpbin
	resp, err := http.Post("https://httpbin.org/post", "application/json", bytes.NewBuffer(payload))
	if err != nil {
		return nil, fmt.Errorf("HTTP request failed: %w", err)
	}
	defer resp.Body.Close()

	// Read and parse the response
	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("failed to read response body: %w", err)
	}

	var httpBinResp HTTPBinResponse
	if err := json.Unmarshal(body, &httpBinResp); err != nil {
		return nil, fmt.Errorf("failed to parse response: %w", err)
	}

	// Extract the greeting from the response
	greeting := httpBinResp.JSON.Greeting
	return &HelloFromHttpBinResponse{
		Greeting: &greeting,
	}, nil
}
```

</TabItem>

</Tabs>

Remember to expose your lambda functions to PromptQL by following the steps in
[Exposing your lambda functions](#exposing-your-lambda-functions).

## Take action

You can use lambda connectors to add custom business logic to your application that takes action on behalf of a user.

### Step 1. Add custom logic

<Tabs groupId="source-preference" className="api-tabs">

<TabItem value="TypeScript" label="TypeScript">

```typescript title="Replace the contents of functions.ts with the following:"
/*
 * Interface for the response when taking action on behalf of a user.
 * Contains success status and message.
 */
interface UserActionResponse {
  success: boolean;
  message: string;
}

/*
 * This function simulates taking an action on behalf of a user. It logs the request made by the user
 * and returns a response object indicating the success status and a message.
 *
 * @param {string} request - What the user wants to do
 * @returns {UserActionResponse} - The response object containing success status and message
 */
export function takeActionOnBehalfOfUser(request: string): UserActionResponse {
  // In a real application, you'd replace this with your custom business logic.
  // You could update data in a database or use an API to update another service.
  console.log(`Taking action on behalf of user: ${request}`);
  return {
    success: true,
    message: `Successfully took action on user's behalf: ${request}`,
  };
}
```

The absence of the `@readonly` tag indicates that this function will modify data. PromptQL will ask for confirmation
before calling it.

</TabItem>

<TabItem value="Python" label="Python">

```python title="Replace the contents of functions.py with the following:"
from hasura_ndc import start
from hasura_ndc.function_connector import FunctionConnector
from pydantic import BaseModel, Field
from hasura_ndc.errors import UnprocessableContent
from typing import Annotated

connector = FunctionConnector()

class UserActionArguments(BaseModel):
    request: Annotated[str, Field(description="What the user wants to do")]

class UserActionResponse(BaseModel):
    success: bool
    message: str

@connector.register_mutation
def take_action_on_behalf_of_user(args: UserActionArguments) -> UserActionResponse:
    # In a real application, you'd replace this with business logic
    # You could update data in a database or use an API to update another service.
    print("Taking action on behalf of user")
    return UserActionResponse(
        success=True,
        message=f"Successfully took action on user's behalf: {args.request}"
    )

if __name__ == "__main__":
    start(connector)
```

The addition of the `@connector.register_mutation` decorator indicates that the function will modify data, and PromptQL
will ask for confirmation before calling it.

</TabItem>

<TabItem value="Go" label="Go">

```go title="Add the following to a new file called app/connector/my_go/functions/take_action_on_behalf_of_user.go:"
package functions

	"context"
	"fmt"

	"hasura-ndc.dev/ndc-go/types"
)

// TakeActionArguments represents the input arguments for a user action.
type TakeActionArguments struct {
	Request string `json:"request"`
}

// TakeActionResponse represents the response after performing a user action.
type TakeActionResponse struct {
	Success bool   `json:"success"`
	Message string `json:"message"`
}

// ProcedureTakeActionOnBehalfOfUser simulates taking an action for the user.
func ProcedureTakeActionOnBehalfOfUser(
	ctx context.Context,
	state *types.State,
	args *TakeActionArguments,
) (*TakeActionResponse, error) {
	// In a real application, you'd replace this with your custom business logic
  // You could update data in a database or use an API to update another service.
	fmt.Println("Taking action on behalf of user")

	return &TakeActionResponse{
		Success: true,
		Message: fmt.Sprintf("Successfully took action on user's behalf: %s", args.Request),
	}, nil
}
```

By prefixing the function name with `Procedure`, we indicate that the function will modify data, and PromptQL will ask
for confirmation before calling it.

</TabItem>

</Tabs>

Remember to expose your lambda functions to PromptQL by following the steps in
[Exposing your lambda functions](#exposing-your-lambda-functions).

<Thumbnail src="/img/business-logic/take-action.png" alt="Take action on behalf of user" />

:::info What about custom native operations?

Many complex reads and writes to your data sources can be accomplished with custom native queries and mutations. Check
out the [connector-specific reference docs](/reference/connectors/index.mdx) for generating queries and mutations using
the native capabilities of your data source.

:::



==============================



# add-env-vars-to-a-lambda.mdx

URL: https://hasura.io/docs/promptql/business-logic/add-env-vars-to-a-lambda

# Add Custom Environment Variables

## Introduction

Custom environment variables allow you to define configuration values specific to your deployment. These variables can
be used to set API keys, connection strings, or other parameters that your application or connectors need. When you run
the `ddn connector env add` command, it automatically updates multiple files to ensure the environment variables are
available across your project:

- **`.env`**: Adds the variable to the [current context's](/reference/cli/commands/ddn_context.mdx) environment
  configuration file for easy access during development.
- **`connector.yaml`**: Updates the connector's configuration to include the variable for deployment.
- **`compose.yaml`**: Ensures the variable is included in the Docker Compose configuration for runtime.

## Add an environment variable

```ddn title="From a project directory, run:"
ddn connector env add <connector_name> --env FOO=bar
```

Repeat these steps for each additional variable.

## Use an environment variable

After creating a new build and running your connector, you can access your environment variable(s) via whatever
conventions your language of choice prefers. For detailed instructions, refer to the following resources:

- **Node.js**: Using [dotenv](https://github.com/motdotla/dotenv#readme)
- **Python**: Using [python-dotenv](https://pypi.org/project/python-dotenv/)
- **Go**: Using [godotenv](https://github.com/joho/godotenv)



==============================



# lambda-connector-types.mdx

URL: https://hasura.io/docs/promptql/business-logic/lambda-connector-types


# Lambda Connector Types

Lambda connectors written in TypeScript, Python, or Go can accept and return native types that map to NDC data model
types. Below are examples of supported types in each language.

For information about setting up your lambda connectors, see
[Add a Lambda Connector](/business-logic/add-a-lambda-connector.mdx).

<Tabs groupId="source-preference" className="api-tabs">

<TabItem value="TypeScript" label="TypeScript">

When creating TypeScript lambda functions, you can use the following types for parameters and return values:

### Basic scalar types

- `string` - Maps to NDC scalar type: `String`
- `number` - Maps to NDC scalar type: `Float`
- `boolean` - Maps to NDC scalar type: `Boolean`
- `bigint` - Maps to NDC scalar type: `BigInt` (represented as a string in JSON)
- `Date` - Maps to NDC scalar type: `DateTime` (represented as an ISO formatted string in JSON)

```ts
/**
 * Example of basic scalar types
 * @readonly
 */
export function calculateAge(birthDate: Date, currentYear: number): number {
  return currentYear - birthDate.getFullYear();
}
```

### Object types and interfaces

You can define custom object types and interfaces:

```ts
type User = {
  id: string;
  name: string;
  age: number;
};

interface Response {
  success: boolean;
  data: string;
}

/**
 * Example using custom types
 * @readonly
 */
export function processUser(user: User): Response {
  return {
    success: true,
    data: `User ${user.name} is ${user.age} years old`,
  };
}
```

### Arrays

Arrays of a single type are supported:

```ts
/**
 * Example using array types
 * @readonly
 */
export function sumNumbers(numbers: number[]): number {
  return numbers.reduce((sum, num) => sum + num, 0);
}
```

### Null, undefined, and optional properties

You can use null, undefined, or make properties optional:

```ts
/**
 * Example with optional and nullable parameters
 * @readonly
 */
export function formatName(firstName: string, lastName?: string, title: string | null = null): string {
  const formattedTitle = title ? `${title} ` : "";
  const formattedLastName = lastName ? ` ${lastName}` : "";
  return `${formattedTitle}${firstName}${formattedLastName}`;
}
```

### Arbitrary JSON

You can import `JSONValue` from the SDK to accept and return arbitrary JSON:

```ts

/**
 * Example using JSONValue for arbitrary JSON
 * @readonly
 */
export function transformData(data: sdk.JSONValue): sdk.JSONValue {
  // Process the JSON data
  return new sdk.JSONValue({ processed: true, original: data.value });
}
```

### Error handling

You can throw custom errors for better error handling:

```ts

/**
 * Example with error handling
 * @readonly
 */
export function divide(a: number, b: number): number {
  if (b === 0) {
    throw new sdk.UnprocessableContent("Cannot divide by zero", { a, b });
  }
  return a / b;
}
```

</TabItem>
<TabItem value="Python" label="Python">

When creating Python lambda functions, you can use the following types for parameters and return values:

### Basic scalar types

- `str` - Maps to NDC scalar type: `String`
- `int` - Maps to NDC scalar type: `Int`
- `float` - Maps to NDC scalar type: `Float`
- `bool` - Maps to NDC scalar type: `Boolean`
- `datetime` - Maps to NDC scalar type: `DateTime` (represented as an ISO formatted string in JSON)

```python
from datetime import datetime

@connector.register_query
def calculate_rough_age(birth_year: int, current_year: int) -> int:
    """
    Example of basic scalar types
    """
    return current_year - birth_year
```

### Object types with Pydantic

You can define custom object types using Pydantic models with field descriptions:

```python
from pydantic import BaseModel, Field
from typing import Annotated

class User(BaseModel):
    id: str = Field(..., description="Unique identifier for the user")
    name: Annotated[str, "User's full name"]
    age: int

class Response(BaseModel):
    success: bool
    data: str

@connector.register_query
def process_user(user: User) -> Response:
    """Example using custom types"""
    return Response(
        success=True,
        data=f"User {user.name} is {user.age} years old"
    )
```

### Lists and nested types

Lists and nested types are supported:

```python
from pydantic import BaseModel

class Pet(BaseModel):
    name: str

class Person(BaseModel):
    name: str
    pets: list[Pet] | None = None

@connector.register_query
def greet_person(person: Person) -> str:
    greeting = f"Hello {person.name}!"
    if person.pets is not None:
        for pet in person.pets:
            greeting += f" And hello to {pet.name}."
    else:
        greeting += f" I see you don't have any pets."
    return greeting
```

### Union types and optional properties

You can use union types to indicate multiple possible types:

```python
@connector.register_query
def format_name(first_name: str, last_name: str | None = None, title: str | None = None) -> str:
    """Example with optional and nullable parameters"""
    formatted_title = f"{title} " if title else ""
    formatted_last_name = f" {last_name}" if last_name else ""
    return f"{formatted_title}{first_name}{formatted_last_name}"
```

### Arbitrary JSON

You can use untyped parameters for arbitrary JSON:

```python
@connector.register_mutation
def transform_data(data) -> dict:
    """Example using untyped parameter for arbitrary JSON"""
    # Process the JSON data
    return {"processed": True, "original": data}
```

### Parallel execution

You can configure functions to run in parallel:

```python

@connector.register_query(parallel_degree=5)
async def parallel_query(name: str) -> str:
    """
    This function will be executed in parallel in batches of 5
    when joined in a query
    """
    await asyncio.sleep(1)
    return f"Hello {name}"
```

### Error handling

You can raise custom errors for better error handling:

```python
from hasura_ndc.errors import UnprocessableContent

@connector.register_query
def divide(a: float, b: float) -> float:
    """Example with error handling"""
    if b == 0:
        raise UnprocessableContent(message="Cannot divide by zero", details={"a": a, "b": b})
    return a / b
```

### Tracing support

Add additional OpenTelemetry tracing spans to your functions:

```python
from hasura_ndc.instrumentation import with_active_span
from opentelemetry.trace import get_tracer

tracer = get_tracer("ndc-sdk-python.server")

@connector.register_query
async def with_tracing(name: str) -> str:
    async def do_some_work(_span):
        # This could be an async network call or other operation
        return f"Hello {name}, tracing is active!"

    return await with_active_span(
        tracer,
        "Root Span",
        do_some_work,
        {"tracing-attr": "This attribute is added to the trace"}
    )
```

</TabItem>
<TabItem value="Go" label="Go">

When creating Go lambda functions, you can use the following types for parameters and return values:

### Basic scalar types

- `string` - Maps to NDC scalar type: `String`
- `int`, `int32`, `int64` - Maps to NDC scalar type: `Int`
- `float32`, `float64` - Maps to NDC scalar type: `Float`
- `bool` - Maps to NDC scalar type: `Boolean`
- `time.Time` - Maps to NDC scalar type: `DateTime` (represented as an ISO formatted string in JSON)

```go
package functions

	"context"

	"hasura-ndc.dev/ndc-go/types"
)

type CalculateAgeArguments struct {
	BirthYear   int `json:"birthYear"`
	CurrentYear int `json:"currentYear"`
}

type CalculateAgeResult struct {
	Age int `json:"age"`
}

// FunctionCalculateAge calculates a person's age
func FunctionCalculateAge(ctx context.Context, state *types.State, args *CalculateAgeArguments) (*CalculateAgeResult, error) {
	return &CalculateAgeResult{
		Age: args.CurrentYear - args.BirthYear,
	}, nil
}
```

### Object types and structs

You can define custom struct types:

```go
package functions

	"context"
	"fmt"

	"hasura-ndc.dev/ndc-go/types"
)

type User struct {
	ID   string `json:"id"`
	Name string `json:"name"`
	Age  int    `json:"age"`
}

type ProcessUserArguments struct {
	User User `json:"user"`
}

type ProcessUserResponse struct {
	Success bool   `json:"success"`
	Data    string `json:"data"`
}

// FunctionProcessUser demonstrates using custom struct types
func FunctionProcessUser(ctx context.Context, state *types.State, args *ProcessUserArguments) (*ProcessUserResponse, error) {
	return &ProcessUserResponse{
		Success: true,
		Data:    fmt.Sprintf("User %s is %d years old", args.User.Name, args.User.Age),
	}, nil
}
```

### Nullable and optional properties

You can use pointers to make properties optional:

```go
package functions

	"context"
	"fmt"

	"hasura-ndc.dev/ndc-go/types"
)

type FormatNameArguments struct {
	FirstName string  `json:"firstName"`
	LastName  *string `json:"lastName"`   // Optional
	Title     *string `json:"title"`      // Optional
}

type FormatNameResult struct {
	FormattedName string `json:"formattedName"`
}

// FunctionFormatName demonstrates using optional parameters
func FunctionFormatName(ctx context.Context, state *types.State, args *FormatNameArguments) (*FormatNameResult, error) {
	formattedTitle := ""
	if args.Title != nil {
		formattedTitle = *args.Title + " "
	}

	formattedLastName := ""
	if args.LastName != nil {
		formattedLastName = " " + *args.LastName
	}

	return &FormatNameResult{
		FormattedName: fmt.Sprintf("%s%s%s", formattedTitle, args.FirstName, formattedLastName),
	}, nil
}
```

### Generic JSON

You can use `map[string]interface{}` or the `any` type for arbitrary JSON:

```go
package functions

	"context"

	"hasura-ndc.dev/ndc-go/types"
)

type TransformDataArguments struct {
	Data map[string]interface{} `json:"data"`
}

type TransformDataResult struct {
	Processed bool                   `json:"processed"`
	Original  map[string]interface{} `json:"original"`
}

// FunctionTransformData demonstrates handling arbitrary JSON
func FunctionTransformData(ctx context.Context, state *types.State, args *TransformDataArguments) (*TransformDataResult, error) {
	return &TransformDataResult{
		Processed: true,
		Original:  args.Data,
	}, nil
}
```

### Error handling

You can return custom errors for better error handling:

```go
package functions

	"context"
	"fmt"

	"hasura-ndc.dev/ndc-go/types"
)

type DivideArguments struct {
	A float64 `json:"a"`
	B float64 `json:"b"`
}

type DivideResult struct {
	Result float64 `json:"result"`
}

// FunctionDivide demonstrates error handling
func FunctionDivide(ctx context.Context, state *types.State, args *DivideArguments) (*DivideResult, error) {
	if args.B == 0 {
		return nil, fmt.Errorf("cannot divide by zero")
	}
	return &DivideResult{Result: args.A / args.B}, nil
}
```

</TabItem>

</Tabs>



==============================



# dev-mode.mdx

URL: https://hasura.io/docs/promptql/business-logic/dev-mode

# Live Reloading with Compose Watch

## Introduction

When working with lambda connectors, you'll often want to make quick edits to your custom business logic and don't want
to go through the hassle of bringing down all your services, rebuilding them all, and restarting them. Instead you can
leverage [Compose Watch](https://docs.docker.com/compose/how-tos/file-watch/) to listen for changes to your connector's
entrypoint, and rebuild the connector on the fly.

:::info Are you adding new functions or modifying the signature of existing functions?

The workflow described in the guide below helps when you're making changes to your connector's internal logic without
altering the exposed function interfaces or metadata.

If you're adding new functions that will need to be exposed as [commands](/reference/metadata-reference/commands.mdx) in
your application, or modifying existing functions' arguments or return types, you'll still need to regenerate your
metadata and create a new build of your application.

Follow the steps [here](/data-modeling/iterate.mdx) for more information.

:::

## Guide

### Step 1. Update your start script

Open your `.hasura/context.yaml` file and locate your `docker-start` script.

```yaml title="Add the --watch flag to your start script:" {4,7}
docker-start:
  bash:
    HASURA_DDN_PAT=$(ddn auth print-access-token) PROMPTQL_SECRET_KEY=$(ddn auth print-promptql-secret-key) docker
    compose -f compose.yaml --env-file .env up --build --pull always --watch
  powershell:
    $Env:HASURA_DDN_PAT = ddn auth print-access-token; $Env:PROMPTQL_SECRET_KEY = ddn auth print-promptql-secret-key;
    docker compose -f compose.yaml --env-file .env up --build --pull always --watch
```

By adding the `--watch` flag to the `docker compose up` command, you're telling Docker to monitor the files specified in
your service's `develop` section for changes. When changes are detected, Docker automatically triggers the action you
configured (in this case, we'll use `rebuild`) without requiring you to manually stop and restart your services. This
creates a much faster feedback loop while developing.

### Step 2. Update your connector's `compose.yaml`

```yaml title="Add the following adjusting the path as necessary:" {14-17}
services:
  my_lambda_connector:
    build:
      context: .
      dockerfile: .hasura-connector/Dockerfile
    environment:
      HASURA_SERVICE_TOKEN_SECRET: $MY_LAMBDA_CONNECTOR_HASURA_SERVICE_TOKEN_SECRET
      OTEL_EXPORTER_OTLP_ENDPOINT: $MY_LAMBDA_CONNECTOR_OTEL_EXPORTER_OTLP_ENDPOINT
      OTEL_SERVICE_NAME: $MY_LAMBDA_CONNECTOR_OTEL_SERVICE_NAME
    extra_hosts:
      - local.hasura.dev:host-gateway
    ports:
      - 8203:8080
    develop:
      watch:
        - action: rebuild
          path: .
```

The `develop` section with `watch` tells Docker Compose which files or directories to monitor during development.

- `path: .` means "watch the entire context directory". You can narrow this down to the entrypoint file(s) if you want
  finer control. For example, you can isolate on the `functions.ts` file for the TypeScript connector or the
  `./functions` directory for the Go connector.

- `action: rebuild` means that if any file changes within the watched path, Docker will automatically rebuild and
  restart the container. This setup ensures that your lambda connector is instantly rebuilt whenever you modify its code
  — speeding up iteration dramatically without manual intervention.

You can verify this by running `ddn run docker-start` from the root of your project and then making a modification to
your lambda connector's logic. In your Docker logs, you should see the container for your connector being rebuilt and
the changes you've made instantly reflected in your API.



==============================



# errors.mdx

URL: https://hasura.io/docs/promptql/business-logic/errors


# Debug and Handle Errors with Lambda Connectors

## Introduction

When developing with lambda connectors, understanding what went wrong — and why — is just as important as handling the
error itself. By default for security reasons, lambda connectors return a generic `internal error` message when
exceptions occur, while the full details are logged in the [OpenTelemetry trace](#access-opentelemetry-traces) for that
request.

This page covers strategies for both **debugging issues** during development and **handling errors** effectively in
deployed connectors. You'll learn how to inspect traces, return custom error messages, and use supported error classes
to improve PromptQL's understanding and self-corrective capabilities.

For a full list of supported status codes, refer to the
[Native Data Connector error specification](https://hasura.github.io/ndc-spec/specification/error-handling.html).

## Debugging

### Local development

As your connector is running inside a Docker container, any logs (i.e., `console.log()`, `print()`, or `fmt.Println()`)
from your custom business logic will be visible in the container's logs.

These logs are printed to your terminal when running the default `ddn run docker-start` command, can be viewed by
running `docker logs <lambda_container_name>`, or via Docker Desktop.

:::info Enable watch mode

We recommend enabling Compose Watch on your lambda connectors to create a shorter feedback loop during development. See
the guide [here](/business-logic/dev-mode.mdx).

:::

### Deployed connectors

For deployed connectors, you can use the DDN CLI to locate the connector's build ID and then output the logs to your
terminal.

#### Step 1. List all builds for a connector

Start by entering a project directory.

```ddn title="For example, to get the list of builds for a connector named my_ts:"
ddn connector build get --connector-name my_ts
```

```plaintext title="Which will return a list of all builds for my_ts:"
+---------------------+----------+--------------------------------------+---------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------+----------------+-----------------------+
|    CREATION TIME    | SUBGRAPH |          CONNECTORBUILD ID           |   CONNECTOR   |                                   READ URL                                   |                                  WRITE URL                                   |     STATUS     |     HUBCONNECTOR      |
+---------------------+----------+--------------------------------------+---------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------+----------------+-----------------------+
| 13 Apr 25 19:07 PDT | app      | b336c2f5-de3a-4d11-9f88-52578f3d8d92 | my_ts         | https://service-b336c2f5-de3a-4d11-9f88-52578f3d8d92-<project-id>.a.run.app  | https://service-b336c2f5-de3a-4d11-9f88-52578f3d8d92-<project-id>.a.run.app  | deploy_success | hasura/nodejs:v1.13.0 |
+---------------------+----------+--------------------------------------+---------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------+----------------+-----------------------+
```

#### Step 2. Fetch the logs for a build

```ddn title="Then, use the CONNECTORBUILD ID to fetch the logs:"
ddn connector build logs b336c2f5-de3a-4d11-9f88-52578f3d8d92
```

The [`ddn connector build logs` command](/reference/cli/commands/ddn_connector_build_logs.mdx) supports tailing logs
along with other customizations.

## Returning custom error messages

Lambda connectors allow you to throw classes of errors with your own custom message and metadata to indicate specific
error conditions. These classes are designed to provide clarity in error handling when PromptQL interacts with your data
sources. To explore the available error classes, use your editor's autocomplete or documentation features to view all
supported classes and their usage details.

<Tabs groupId="source-preference" className="api-tabs">

<TabItem value="TypeScript" label="TypeScript">

```typescript title="TypeScript examples:" {1,6,14,22}

/** @readonly */
export function updateResource(userRole: string): void {
  if (userRole !== "admin") {
    throw new sdk.Forbidden("User does not have permission to update this resource", { role: userRole });
  }
  console.log("Resource updated successfully.");
}

/** @readonly */
export function createResource(id: string, existingIds: string[]): void {
  if (existingIds.includes(id)) {
    throw new sdk.Conflict("Resource with this ID already exists", { existingId: id });
  }
  console.log("Resource created successfully.");
}

/** @readonly */
export function divide(x: number, y: number): number {
  if (y === 0) {
    throw new sdk.UnprocessableContent("Cannot divide by zero", { myErrorMetadata: "stuff", x, y });
  }
  return x / y;
}
```

</TabItem>

<TabItem value="Python" label="Python">

```python title="Python examples:" {4}
# There are different error types including: BadRequest, Forbidden, Conflict, UnprocessableContent, InternalServerError, NotSupported, and BadGateway
@connector.register_query
def error():
    raise UnprocessableContent(message="This is an error", details={"Error": "This is an error!"})
```

</TabItem>

<TabItem value="Go" label="Go">

```go title="Go examples:" {7,29-31}
package functions

	"context"
	"fmt"

	"github.com/hasura/ndc-sdk-go/schema"
	"hasura-ndc.dev/ndc-go/types"
)

// A hello argument
type HelloArguments struct {
	Greeting string `json:"greeting"`
	Count    *int   `json:"count"`
}

// A hello result
type HelloResult struct {
	Reply string `json:"reply"`
	Count int    `json:"count"`
}


func FunctionHello(ctx context.Context, state *types.State, arguments *HelloArguments) (*HelloResult, error) {
	count := 1
	authorized := false // This is just an example

	if !authorized {
		return nil, schema.UnauthorizeError("User is not authorized to perform this operation", map[string]any{
			"function": "hello",
		})
	}

	if arguments.Count != nil {
		count = *arguments.Count + 1
	}
	return &HelloResult{
		Reply: fmt.Sprintf("Hi! %s", arguments.Greeting),
		Count: count,
	}, nil
}
```

</TabItem>

</Tabs>

:::info How detailed should error messages be?

Exposing stack traces to end users is generally discouraged. Instead, administrators can review traces logged in the
OpenTelemetry traces to access detailed stack trace information.

That said, the more clarity provided in an error message, the better PromptQL can self-correct and improve its
understanding of the function. Clear, descriptive error messages allow PromptQL to learn from errors and provide more
accurate interactions with your data over time.

:::

### Access OpenTelemetry traces {#access-opentelemetry-traces}

Traces — complete with your custom error messages — are available for each request. You can find these in the `Insights`
tab of your project's console. These traces help you understand how PromptQL is interacting with your data and where
improvements can be made to enhance accuracy.

### Response-size limitations

Lambda connectors have a response-size limit, which may vary depending on the SDK and should be considered when
developing your application's business logic functions.

| Connector       | Limit   | Configurable |
| --------------- | ------- | ------------ |
| `hasura/nodejs` | `30` MB | false        |
| `hasura/python` | `30` MB | false        |
| `hasura/go`     | `30` MB | false        |



==============================



# overview.mdx

URL: https://hasura.io/docs/promptql/data-modeling/overview


# Data Modeling

## Introduction

To power PromptQL, Hasura DDN defines your API schema declaratively. An approach that centralizes all your data
collections, operations, relationships, and permissions in one place. This makes it easy to organize, modify, secure,
reason about, and grow the schema which represents your API which is is accessed via PromptQL.

## Lifecycle

PromptQL uses Hasura DDN to define your API schema.

Hasura DDN uses data connectors to connect to your data sources.

Data connectors introspect the source to understand the source schema.

The DDN CLI uses the introspection results to generate the API schema metadata objects. This can also be done manually.

The metadata is then "built" by the DDN CLI into a format which the Hasura engine can use to provide information to
PromptQL.

<Thumbnail src="/img/data-modeling/ddn-cli-process.png" alt="Data modeling lifecycle" width="1000px" />

## Metadata Objects

There are [many types of metadata objects](/reference/metadata-reference/index.mdx) which define your API, but the most

- [Models](/data-modeling/model.mdx) which read data
- [Commands](/data-modeling/command.mdx) which modify data
- [Relationships](/data-modeling/relationship.mdx) which connect data
- [Permissions](/data-modeling/permissions.mdx) which protect data

We will cover each of these in more detail in the following sections.



==============================



# model.mdx

URL: https://hasura.io/docs/promptql/data-modeling/model




# Models Empower PromptQL to Understand Your Data

## Introduction

Models are foundational components that enable PromptQL to accurately access and interact with your data. They represent
entities or collections in your data sources, including tables, views, collections, and native queries, providing the
semantic understanding PromptQL needs to generate precise query plans and produce meaningful insights from your data.

## Lifecycle

Creating models for your PromptQL supergraph involves the following steps:

1. Identify data sources that you want PromptQL to intelligently interact with.
2. Introspect your data source using the DDN CLI with the relevant data connector to fetch the entity resources.
3. Add the model to your metadata with the DDN CLI.
4. Create a build of your supergraph with the DDN CLI.
5. Serve your build with the Hasura engine either locally or in the cloud.
6. Interact with your data through PromptQL in a natural, conversational way.

<Thumbnail src="/img/data-modeling/ddn-cli-process.png" alt="Data modeling lifecycle" />

## Create a model

To add a model you will need to have a data connector already set up and connected to the data source. Follow the
relevant tutorial for your data source in [How to Build with PromptQL](/how-to-build-with-promptql/overview.mdx) to get
to that point.

### From a source entity

```ddn title="Introspect your data source:"
ddn connector introspect <connector_name>
```

Whenever you update your data source, you can run the above command to fetch the latest resources.

```ddn title="Show the resources discovered from your data source:"
ddn connector show-resources <connector_name>
```

This is an optional step and will output a list of resources that DDN discovered in your data source in the previous
step.

```ddn
ddn model add <connector_link_name> <collection_name>
```

Or you can optionally add all the models by specifying `"*"`.

```ddn
ddn model add <connector_link_name> "*"
```

This will add models with their accompanying metadata definitions to your metadata.

You can now build your supergraph, serve it, and begin having meaningful conversations with your data through PromptQL.
With robust models in place, PromptQL can create dynamic query plans that accurately address your requests and provide
deep insights into your data.

:::info Context for CLI commands

Note that the above CLI commands work without also adding the relevant subgraph to the command with the `--subgraph`
flag because this has been set in the CLI context. You can learn more about creating and switching contexts in the
[CLI context](/reference/cli/commands/ddn_context.mdx) section.

:::

## Update a model

If you want to update your model to reflect a change that happened in the underlying data source you should first
introspect to get the latest resources and then update the relevant model.

```ddn title="Introspect your data source:"
ddn connector introspect <connector_name>
```

```ddn title="Then, update your existing model:"
ddn model update <connector_link_name> <model_name>
```

You will see an output which explains how new resources were added or updated in the model.

You can now build your supergraph, serve it, and continue interacting with your data using PromptQL, which will
automatically adjust its query plans based on the updated model definitions.

You can also update the model by editing the metadata manually.

## Extend a model

A model can be extended to enable PromptQL to understand and navigate relationships between different data entities.

For example, you can extend a model like `Customers` to also include related `Orders` data. This allows PromptQL to
answer questions like "Show me customer Jane Doe's recent orders" by understanding the relationship between customers
and orders.

Or you can add custom business logic to a model like `Orders` to compute and return the current currency conversion of
the total price, enabling PromptQL to provide real-time financial insights across different currencies.

The way this is done is via a `Relationship`. Read more about
[creating relationships here](/data-modeling/relationship.mdx).

## Delete a model

```ddn title="If you no longer need a model, you can delete it:"
ddn model remove users
```

In addition to removing the `Model` object itself, the DDN CLI will also remove the associated metadata definitions.

## Reference

You can learn more about models in the metadata reference [docs](/reference/metadata-reference/models.mdx).



==============================



# relationship.mdx

URL: https://hasura.io/docs/promptql/data-modeling/relationship


# Relationships Connect Data

## Introduction

Relationships allow you to connect your data, enabling PromptQL to understand the semantic connections between your data
entities and accurately talk to all your data. By defining these relationships, PromptQL can generate intelligent query
plans that navigate complex data structures to provide meaningful insights.

Examples of how PromptQL uses relationships to deliver powerful insights:

- When asking PromptQL about a customer's purchasing patterns, it can understand the relationship between `Customer`,
  their `Orders`, and each `Product` item in those orders, delivering comprehensive purchasing analysis. (Model to
  Model)
- When requesting customer behavior analytics, PromptQL can correlate a `Customer` with their app usage metrics from
  another data source, providing cross-data-source insights. (Model to Model in another subgraph or data connector)
- When analyzing international sales performance, PromptQL can combine `Order` data with live currency conversions from
  an external API through a lambda data connector, delivering real-time financial analysis. (Model to Command)

Relationships can be added between _any_ kind of semantically related models and/or commands. They do not need to be
related in the data source by, for example, a foreign key. They also do not need to be backed by the same data source or
be in the same subgraph.

## Lifecycle

Many relationships can be created automatically by the DDN CLI from detected underlying connections such as foreign
keys. In such cases the lifecycle in creating a relationship in your metadata is as follows:

1. Introspect your data source using the DDN CLI with the relevant data connector to fetch the entity resources.
2. Add the detected relationships to your metadata with the DDN CLI.
3. Create a build of your supergraph API with the DDN CLI.
4. Serve your build to enable PromptQL to access your data with the Hasura engine either locally or in the cloud.
5. Iterate on your PromptQL experience by repeating this process or by editing your metadata manually as needed.

<Thumbnail src="/img/data-modeling/ddn-cli-process.png" alt="Data modeling lifecycle" width="1000px" />

If the relationship cannot be detected automatically, you can easily manually create a relationship in your metadata and
then perform lifecycle steps 3-5 from above as needed.

## Create a relationship

Relationships are defined in metadata from an
[object type](/reference/metadata-reference/types.mdx#objecttype-objecttype), to a
[model](/reference/metadata-reference/models.mdx) or [command](/reference/metadata-reference/commands.mdx). But since
models and commands are also defined with object types, you can think of relationships as being between models and/or
commands.

The target command can be enabled with a custom piece of business logic on a lambda data connector, or a native mutation
operation.

### Using the DDN CLI

The DDN CLI and your data connectors will detect many relationships in your data sources automatically, for instance
from foreign keys in a relational database, and once introspected, you can add them to your metadata.

```ddn title="Introspect your data source:"
ddn connector introspect <connector_name>
```

```ddn title="Show the found relationships:"
ddn connector show-resources <connector_name>
```

```ddn title="Add a relationship to your metadata:"
ddn relationship add <connector_link_name> <collection_name>
```

Or optionally add all relationships found for a connector at once:

```ddn
ddn relationship add <connector_link_name> "*"
```

:::info Context for CLI commands

Note that the above CLI commands work without also adding the relevant subgraph to the command with the `--subgraph`
flag because this has been set in the CLI context. You can learn more about creating and switching contexts in the
[CLI context](/) section. {/* TODO: Add link */}

:::

### Manually creating a relationship

Relationships can also be manually added to your metadata.

The [VS Code extension](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura) can help you to author
relationships.

For example, you can configure a relationship so that when a user asks PromptQL about a Customer's purchasing history,
it can also access their Orders data.

```yaml title="Create a relationship in your metadata:"
---
kind: Relationship
version: v1
definition:
  sourceType: Customers # The existing source object type which also defines the model
  name: orders # A name we want to use when we query the Orders from the Customer
  description: |
    Links customers to their purchase orders.
    One customer can have multiple orders.
    This is a critical business relationship that supports order history lookups, 
    customer purchase analysis, and revenue attribution.
    Historical orders are retained even if customer becomes inactive.
  target:
    model: # The target can be a model or a command
      name: Orders # The existing model that we want to access when we query the Orders from the Customer
      relationshipType: Array # The relationship type which can be Object or Array. Since a customer can have many orders, we use an Array.
  mapping: # The mapping defines which field on the source object type maps to which field on the target model
    - source:
        fieldPath:
          - fieldName: customerId # The existing field on the source object type that we want to map to the target model
      target:
        modelField:
          - fieldName: customerId # The existing field on the target model that we want to map to the source object type
```

By defining this `Relationship` object, PromptQL will understand that customers and orders are connected, allowing it to
generate accurate query plans that can navigate from customer data to their associated orders when responding to user
queries about customer purchasing behavior.

Learn more about the [Relationship](/reference/metadata-reference/relationships.mdx) object.

## Update a relationship

Your underlying data source may change over time. You can update your relationship to reflect these changes.

If you have an automatically detected relationship and a property on the source object type has changed, you can update
the relationship to reflect this change.

First, update your connector configuration and models.

```ddn title="Update your source introspection:"
ddn connector introspect <connector_name>
```

```ddn title="Then, update your model:"
ddn model update <connector_name> <model_name>
```

Now, you can either delete the existing `Relationship` object and use the DDN CLI to add it again:

```ddn title="Delete your existing relationship manually and add it again:"
ddn relationship add <connector_link_name> <collection_name>
```

Or you can update the `Relationship` object manually. Learn more about the
[Relationship](/reference/metadata-reference/relationships.mdx) object.

## Delete a relationship

If you no longer need a relationship, simply delete the `Relationship` metadata object manually. It is fully
self-contained.

## Reference

You can learn more about relationships in the metadata reference
[docs](/reference/metadata-reference/relationships.mdx).



==============================



# command.mdx

URL: https://hasura.io/docs/promptql/data-modeling/command



# Commands Enable PromptQL to Modify Data

## Introduction

Commands are a crucial component that enable PromptQL to take action on your data. While PromptQL can intelligently
query your data through natural language conversations, commands allow it to go further by modifying data (inserts,
updates, deletes), executing complex operations, or implementing custom business logic across your connected data
sources.

When a user requests changes to their data through PromptQL, the system dynamically builds a query plan that
incorporates these commands as part of its execution strategy, providing high accuracy and detailed explanations of each
action taken.

## Lifecycle

When setting up commands for use with PromptQL, follow this lifecycle:

1. Have some operation in your data source that you want to make executable via PromptQL.
2. Introspect your data source using the DDN CLI with the relevant data connector to fetch the operation resources.
3. Add the command to your metadata with the DDN CLI.
4. Create a build of your supergraph API with the DDN CLI.
5. Serve your build as your API with the Hasura engine either locally or in the cloud.
6. Interact with your data through PromptQL, which can now execute these commands as part of its dynamic query planning.

<Thumbnail src="/img/data-modeling/ddn-cli-process.png" alt="Data modeling lifecycle" />

## Create a command

To add a command you will need to have a data connector already set up and connected to the data source. Follow the
[Quickstart](/quickstart.mdx) or the tutorial in [How to Build with DDN](/how-to-build-with-promptql/overview.mdx) to
get to that point.

### From a source operation

Some data connectors support the ability to introspect your data source to discover the commands that can be added to
your supergraph automatically.

```ddn title="Introspect your data source:"
ddn connector introspect <connector_name>
```

```ddn title="Show the resources discovered from your data source:"
ddn connector show-resources <connector_name>
```

```ddn title="Add the command from the discovered resources to your metadata:"
ddn command add <connector_link_name> <operation_name>
```

Or you can optionally add all the commands by specifying `"*"`.

```ddn title="Add all commands from your data source:"
ddn command add <connector_link_name> "*"
```

This will add commands with their accompanying metadata definitions to your metadata.

### Via native operations {#via-native-operations-how-to}

Some data connectors support the ability to add commands via native operations so that you can add any operation that is
not supported by the automatic introspection process.

For classic database connectors, this will be native query code for that source. This can be, for example, a more
complex read operation or a way to run custom business logic, which PromptQL can incorporate into its query plan.

For Lambda connectors, eg: (TypeScript, Go, Python, etc) this will be a function (read-only) or procedure (mutation or
other side-effects) that PromptQL can use to execute specialized operations on your data.

<Tabs groupId="source-preference" className="api-tabs">
  <TabItem value="TypeScript" label="Node.js TypeScript">
    You can create custom TypeScript functions in your connector to expand PromptQL's capabilities, enabling it to interact
with your data in more sophisticated ways through natural language conversations.

```ddn title="Initalize a new connector and select hasura/nodejs from the list:"
ddn connector init my_ts -i
```

```ts title="Replace the functions.ts contents with your own custom code:"
/**
 * @readonly
 */
export function myCustomCode(myInput: string): string {
  // Do something with the input
  return "My output";
}
```

The `@readonly` tag indicates this function is a read-only operation. PromptQL will recognize this as a data retrieval
function when building query plans. Without this tag, PromptQL will treat it as an action that can modify data.

```ddn title="Introspect the connector:"
ddn connector introspect my_ts
```

```ddn title="Track the function:"
ddn command add my_ts myCustomCode
```

```ddn title="Create a new build:"
ddn supergraph build local
```

```ddn title="Start your services:"
ddn run docker-start
```

```ddn title="Open your development console:"
ddn console --local
```

Once set up, you can interact with your custom function through PromptQL conversations.

By adding custom functions to your supergraph, you extend PromptQL's capabilities to talk to your data in ways specific
to your business needs.

  </TabItem>
  <TabItem value="Python" label="Python">
    You can create Python connectors with custom code to enhance PromptQL's ability to talk to all your data. These
connectors become available in your supergraph, allowing PromptQL to include them in its dynamic query plans.

```ddn title="Initialize a new connector and select hasura/python from the list:"
ddn connector init my_py -i
```

```py title="Replace the functions.py contents with your own custom code:"
from hasura_ndc import start
from hasura_ndc.function_connector import FunctionConnector

connector = FunctionConnector()

@connector.register_query
def my_custom_code(my_input: str) -> str:
  # Do something with the input
  return "My output"

if __name__ == "__main__":
  start(connector)
```

When you add the `@connector.register_query` decorator, you're creating a function that PromptQL can access as part of
its query planning. If you use `@connector.register_mutation` instead, the function will be available for data
modification operations in PromptQL's query plans.

```ddn title="Introspect the connector:"
ddn connector introspect my_py
```

```ddn title="Track the function:"
ddn command add my_py my_custom_code
```

```ddn title="Create a new build:"
ddn supergraph build local
```

```ddn title="Start your services:"
ddn run docker-start
```

```ddn title="Open your development console:"
ddn console --local
```

```graphql title="You can now query your native operation:"
query MyCustomCode {
  myCustomCode(myInput: "My input")
}
```

By extending your supergraph with custom Python connectors, you give PromptQL the ability to perform specialized
operations that go beyond standard data access, enabling more powerful and flexible interactions with your data.

  </TabItem>
  <TabItem value="Go" label="Go">
    You can run whatever arbitrary code you want in your Go connector and expose it for PromptQL to access, enabling your AI
agents to accurately interact with your custom business logic.

```ddn title="Initialize a new connector and select hasura/go from the list:"
ddn connector init my_go -i
```

```go title="Replace the functions.go contents with your own custom code:"
package functions

	"context"

	"hasura-ndc.dev/ndc-go/types"
)

// InputArguments represents the input of the native operation.
type InputArguments struct {
	MyInput string `json:"myInput"`
}

// OutputResult represents the output of the native operation.
type OutputResult struct {
	MyOutput string `json:"myOutput"`
}

// ProcedureCustomCode is a native operation that can be called via PromptQL.
func ProcedureCustomCode(ctx context.Context, state *types.State, arguments *InputArguments) (*OutputResult, error) {
	// Do something with the input
	return &OutputResult{
		MyOutput: "My output",
	}, nil
}
```

Using the prefix `Procedure` ensures ProcedureCustomCode() is exposed as a mutation in your supergraph. Prefixing with
`Function` identifies it as a function to be exposed as a query. Both will be accessible through PromptQL.

Both have typed input arguments and return strings, which the connector will use to generate the corresponding schema
for your supergraph.

```ddn title="Introspect the connector:"
ddn connector introspect my_go
```

```ddn title="Track the function:"
ddn command add my_go customCode
```

```ddn title="Create a new build:"
ddn supergraph build local
```

```ddn title="Start your services:"
ddn run docker-start
```

```ddn title="Open your development console:"
ddn console --local
```

Once your connector and custom function are set up, you can use PromptQL to talk to your data using natural language.

  </TabItem>
</Tabs>

Once set up, PromptQL will automatically consider these commands when generating query plans based on user requests,
enabling accurate data modifications and complex operations through natural language interactions.

## Update a command

When your underlying data source changes, you'll need to update the commands available to PromptQL to ensure continued
accuracy in its operations:

```ddn title="Introspect your data source:"
ddn connector introspect <connector_name>
```

```ddn title="Then, update your existing command:"
ddn command update <connector_link_name> <command_name>
```

You will see an output which explains how new resources were added or updated in the command.

After updating, PromptQL will automatically incorporate these changes into its query planning, ensuring that user
interactions with their data remain accurate and up-to-date.

You can also update the command by editing the command's metadata manually.

## Delete a command

```ddn title="If you no longer need a command, you can delete it:"
ddn command remove <command_name>
```

Along with the command itself, the associated metadata is also removed, and PromptQL will no longer include this command
in its query planning.

## Reference

You can learn more about commands in the metadata reference [docs](/reference/metadata-reference/commands.mdx).



==============================



# permissions.mdx

URL: https://hasura.io/docs/promptql/data-modeling/permissions

# Permissions Protect Data

## Introduction

Permissions keep data secure by allowing you to control what data can be accessed through PromptQL conversations by
which user roles. This ensures that when users talk to their data using PromptQL, they only see the information they're
authorized to access.

When an authentication mode is enabled, the Hasura engine will look for session variables on every PromptQL request, it
can then use permissions defined in metadata and the session variables to determine if the request is allowed to
proceed.

## Lifecycle

Hasura DDN uses Role Based Access Control (RBAC) to determine which user roles can access which data in your supergraph
when interacting with PromptQL.

The DDN CLI will automatically create permissions for your models and commands when they are added to your metadata for
only the `admin` role by default.

All other permissions for all other user roles must be added manually.

## Create permissions

### Row access

You can create a `ModelPermission` object to implement row-level security and restrict which rows a user can access
through PromptQL.

For example, to only allow users to access their own records in the `Users` table when talking to their data through
PromptQL:

```yaml title=""
---
# e.g., Users.hml
kind: ModelPermissions
version: v1
definition:
  modelName: Users
  permissions:
    # admin is present by default
    - role: admin
      select:
        filter: null
    #highlight-start
    - role: user
      select:
        filter:
          fieldComparison:
            field: id
            operator: _eq
            value:
              sessionVariable: x-hasura-user-id
    #highlight-end
```

The highlighted role above will filter data accessed through PromptQL to only show `Users` records whose `id` matches
the `x-hasura-user-id` passed in the header of the request.

### Field access

To restrict which fields can be accessed through PromptQL conversations, you can create a `TypePermission` object.

Below, the user role can only access the `name` field, not the `id` field which the admin role can.

```yaml title="The user role can only access their name field:"
# e.g., Users.hml
---
kind: TypePermissions
version: v1
definition:
  typeName: Users
  permissions:
    # admin is present by default
    - role: admin
      output:
        allowedFields:
          - id
          - name
    #highlight-start
    - role: user
      output:
        allowedFields:
          - name
    #highlight-end
```

### Command (mutation) access

To determine which commands can be executed by which roles when interacting with PromptQL, you can create a
`CommandPermission` object.

```yaml title="In this example, we'll make it so a user can update their own record:"
# e.g., UpdateUsersById.hml
---
kind: CommandPermissions
version: v1
definition:
  commandName: UpdateUsersById # Specify the existing command
  permissions:
    - role: admin
      allowExecution: true
      #highlight-start
    - role: user
      allowExecution: true
      argumentPresets: # Specify the arguments and their values which need to be passed to the command
        - argument: keyId
          value:
            sessionVariable: "x-hasura-user-id" # The value of the argument must equal the session variable
      #highlight-end
```

When a user asks PromptQL to update their information, these permissions ensure that they can only modify their own
records.

## Update permissions

Since all permissions are stored in metadata, you can use your text editor to find and update them easily.

For example, to check everything which the `user` role can access when talking to data through PromptQL, search for
`- role: user` and analyze the results.

## Deleting permissions

If you no longer need a role, find all mentions of it in your metadata and remove them all.

If you no longer need a particular permission, simply remove it from the relevant `ModelPermissions`, `TypePermissions`,
or `CommandPermissions` object.

## Reference

You can learn more about permissions in the metadata reference [docs](/reference/metadata-reference/permissions.mdx).



==============================



# data-modeling-workflows.mdx

URL: https://hasura.io/docs/promptql/data-modeling/data-modeling-workflows

# Data Modeling Workflows

Learn how to iterate on your data model, add semantic information, and more.

## Iterating on your data model

When your underlying data model changes, you need to iterate on your data model to reflect the changes.

[Learn more about iterating on your data model](/data-modeling/iterate.mdx)

## Adding semantic information

When you add semantic information to your data model, you can help PromptQL understand your data and create appropriate
query plans.

[Learn more about semantic information](/data-modeling/semantic-information.mdx)



==============================



# iterate.mdx

URL: https://hasura.io/docs/promptql/data-modeling/iterate

# Iterate on your Data Model

## Introduction

After you've created a DDN supergraph to use with PromptQL, you'll often need to iterate on your data model — whether
you're adding a new table, refining a command, or evolving your business logic.

This guide explains when and why you need to run `ddn supergraph build local` to ensure your changes are reflected in
your API.

As a general rule:

- **If your metadata changes** (like models, commands, or relationships): ✅ rebuild.
- **If your connector signature changes** (like a function’s arguments or return type): ✅ rebuild.
- **If you're only changing implementation logic** inside a lambda connector (like editing function body): ❌ no rebuild
  required.

## When to rebuild your application

### You add a new source

Each time you [add a new data source](/data-sources/connect-to-a-source.mdx), you're generating new metadata. This is a
good rule of thumb: **when metadata changes, a rebuild is required.**

After following the steps in the doc referenced above, introspect your source and add your metadata objects before
rebuilding.

### You edit metadata

Whether your metadata changes are generated via the CLI or you've hand-authored a change (using the
[VS Code extension](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura)), you'll need to create a new
build of your application. This will ensure the JSON configuration files consumed by your Hasura Engine are updated with
the latest changes.

### You make changes to your underlying data model

Depending on your data source, you may be adding a column to a table in PostgreSQL, or a creating a new collection in
MongoDB. You may have written new custom logic in TypeScript or want to import a new command from the Stripe connector.
Whatever it is, you will follow the same steps each time anything changes in your data source schema to iterate on your
data model.

#### Re-introspecting your data model

Re-introspecting your data model will update the connector configuration to reflect the changes in your data sources.

```ddn
ddn connector introspect my_connector
```

#### Viewing resources

```ddn title="You can then view the resources that have been discovered by running:"
ddn connector show-resources my_connector
```

#### Adding resources

```ddn title="And add precisely the resources you need running any of the following commands with real values:"
ddn model add my_connector my_model
ddn command add my_connector my_command
ddn relationship add my_connector my_relationship
```

```ddn title="Or if you want to add all resources:"
ddn model add my_connector "*"
ddn command add my_connector "*"
ddn relationship add my_connector "*"
```

#### Adding semantic Information

It's highly recommended to provide extra natural language descriptions of the resources in your project so that the
PromptQL can better understand your data and create appropriate query plans.

The description field can be added to `Model`, `Command` and `Relationship` metadata elements to provide semantic
context. See more about [semantic information here](/data-modeling/semantic-information.mdx).

#### Building a new supergraph

```ddn title="You can then build a new local supergraph by running:"
ddn supergraph build local
```

:::info Build for Hasura Cloud

You can also build a new supergraph for a project on Hasura Cloud:

If you don't have a project on Hasura Cloud, you can create one by running:

```ddn
ddn project init
```

Then:

```ddn
ddn supergraph build create
```

:::

#### Restarting services locally

If you are iterating locally, you then need to restart the Docker services by running:

```ddn
ddn run docker-start
```

:::info When is rebuilding not necessary?

In short: any time you make modifications that don't require metadata updates. Notably, if you're quickly iterating on
the logic of lambda connectors, you can [enable Compose Watch](/business-logic/dev-mode.mdx) to reflect your changes
instantly and rebuild your lambda connector's container.

:::



==============================



# overview.mdx

URL: https://hasura.io/docs/promptql/promptql-playground/overview


# PromptQL Playground

## Introduction

The PromptQL Playground is our UI for chatting with your data. This familiar interface allows you to ask questions
directly about your connected datasources and even take action on behalf of users.

You can see an example below:

<PqlChat>
  <User>Hi, what are some questions you can answer?</User>
  <Pql>
    I can help you analyze data from what appears to be an e-commerce system. Here are some types of questions I can answer:

    <ol>
        <li>Product-related:</li>
        <ul>
            <li>Find popular products based on orders or reviews</li>
            <li>Analyze product pricing across categories</li>
            <li>Look up product details and descriptions</li>
            <li>Find products from specific manufacturers or countries</li>
        </ul>
        <li>Order and Purchase Analysis:</li>
        <ul>
            <li>Track order statuses</li>
            <li>Analyze delivery times</li>
            <li>Look at purchase patterns</li>
            <li>Find abandoned carts</li>
        </ul>
        <li>Customer Behavior:</li>
        <ul>
            <li>Analyze customer reviews and ratings</li>
            <li>Look at shopping patterns</li>
            <li>Track cart abandonment</li>
            <li>Study product category preferences</li>
        </ul>
        <li>Business Metrics:</li>
        <ul>
            <li>Calculate sales metrics</li>
            <li>Analyze review scores</li>
            <li>Track coupon usage</li>
            <li>Monitor cart completion rates</li>
        </ul>
    </ol>

    Would you like me to explore any of these areas in particular?

  </Pql>
</PqlChat>

Threads produce [artifacts](/promptql-playground/artifacts.mdx), which you can export and re-use throughout your thread.
Additionally, the Playground makes it collaborate and share your project with others, whether as collaborators or
publicly.

## Next steps

- [Learn more about threads](/promptql-playground/threads/index.mdx).
- [Learn what types of artifacts are available](/promptql-playground/artifacts.mdx).
- [Learn how to make your project public](/promptql-playground/public-projects.mdx).
- [Learn how to use saved prompts](/promptql-playground/saved-prompts.mdx).
- [Learn how to deal with the most common issues](/promptql-playground/troubleshooting.mdx).



==============================



# index.mdx

URL: https://hasura.io/docs/promptql/promptql-playground/threads/

# Threads

Threads are the conversations you have with your data. They are the backbone of the PromptQL Playground and are how you
interact with your data.

In this section, you'll find tutorials that walk you through some common use cases and best practices for using threads.
Initially, we recommend checking out common use cases:

- [Share a thread](/promptql-playground/threads/1-shared-threads.mdx)
- [Check threads history across all users (as an admin)](/promptql-playground/threads/2-thread-history.mdx)



==============================



# 2-thread-history.mdx

URL: https://hasura.io/docs/promptql/promptql-playground/threads/2-thread-history


# Thread History

## Introduction

As an admin user, you can access the complete history of all threads created within your project. This powerful feature
allows you to monitor conversations, review interactions, and gain insights into how users are engaging with your data.

- View all threads from every user in your project
- Access complete conversation history in read-only mode
- Filter and search through threads to find specific interactions
- Analyze usage patterns and identify common queries

## Accessing Thread History

Navigate to the **Insights** tab in your project dashboard, then select the **Thread History** section to view all
threads.

<Thumbnail src="/img/get-started/thread-history-1.png" alt="Thread History Location" />

## Select date range

The Thread History interface allows you to select a date range to filter the threads.

<Thumbnail src="/img/get-started/thread-history-2.png" alt="Thread History List" />

## Viewing Threads

The Thread History interface displays all conversations in a chronological list. Each entry shows:

- Thread ID
- Thread title
- User who created the thread
- Creation date and time

<Thumbnail src="/img/get-started/thread-history-3.png" alt="Thread History List" />

## Thread Details

Click on any thread ID to view its complete contents. The detailed view provides:

- Full conversation history
- All queries executed within the thread
- Any visualizations or results generated
- Timestamps for each interaction

<Thumbnail src="/img/get-started/thread-history-4.png" alt="Thread History Detail View" />

## Admin Capabilities

This feature is only available to users with the `admin` role. As an admin, you can view threads in read-only mode. This
ensures that the original conversations remain intact while still providing complete visibility into how users are
interacting with your data.

This feature is particularly valuable for:

- Troubleshooting user issues
- Identifying common usage patterns
- Improving data models based on user queries
- Monitoring for potential misuse or security concerns



==============================



# 1-shared-threads.mdx

URL: https://hasura.io/docs/promptql/promptql-playground/threads/1-shared-threads


# Shared Threads

## Introduction

You can extend the functionality of the Public PromptQL Playground and share a thread publicly.

- You can share specific chat threads with anyone — quickly and effortlessly.
- Users can view a shared thread without logging in.
- Your shared thread is accessible via a public link, with no sign-up or login required for viewers. Allowing you to
  share your insights with the world, whether it's an interesting conversation or an impressive query result with
  artifacts.

## Share

To share a thread publicly, simply click the `Share` button in the thread.

<Thumbnail src="/img/get-started/shared-threads.jpeg" alt="Shared Threads" />

Sharing a thread doesn't reveal any identifiable information about your project. Only the content of the thread itself
is visible. Plus, any messages added after sharing won't appear in the public link, keeping the snapshot of your
conversation intact.



==============================



# artifacts.mdx

URL: https://hasura.io/docs/promptql/promptql-playground/artifacts


# Artifacts

## Introduction

Artifacts are reusable outputs generated by programs in the Playground. They allow you to reference, inspect, and reuse
results such as text, tables, and visualizations across interactions.

When a program returns a supported output type, PromptQL automatically stores it as an artifact. These artifacts can
then be linked across steps in a thread, reused in new instructions, or exported for use elsewhere.

## Types of artifacts

### TextArtifact

**Type**: `"text"`  
**Data shape**: `string`

Text artifacts store plain text — like summaries, explanations, or generated responses. They're ideal for lightweight
results that don’t require structure or formatting. In the Playground, a short preview is shown to help you quickly
identify the content.

For example, if a program returns a single paragraph explaining a dataset, PromptQL will save that result as a
`TextArtifact`.

<Thumbnail alt="Text artifact in the Playground" src="/img/promptql-playground/textartifact.png" />

### TableArtifact

**Type**: `"table"`  
**Data shape**: `list of objects`, where each object represents a row and keys are column names

Table artifacts are used for structured data. Whether you're returning a list of users, products, or any tabular output,
the system recognizes it and saves it as a `TableArtifact`.

In the Playground, you'll see the number of rows and a small sample preview. This is useful for quickly scanning the
data before diving deeper or using it as input to a follow-up question.

<Thumbnail alt="Table artifact in the Playground" src="/img/promptql-playground/tableartifact.png" />

### VisualizationArtifact

**Type**: `"visualization"`  
**Data shape**: an object containing:

- `html`: string of rendered visualization markup
- `visualization_data`: the structured data used to create the visualization

Visualization artifacts are built for more expressive output — like charts, graphs, or custom visuals. When a program
returns a visualization with HTML and data, it’s stored as a `VisualizationArtifact`, enabling rich visual exploration
directly in the Playground.

In the interface, the visualization is rendered from the HTML output, giving you a visual snapshot of your program’s
result.

<Thumbnail alt="Visualization artifact in the Playground" src="/img/promptql-playground/visualizationartifact.png" />

## Exporting artifacts

Each artifact contains set of three options for exporting data.

### Copy data

The first allows you to copy the raw data. For text artifacts, this is simply a string of text; for table artifacts,
it's an array of JSON objects; and for visualization artifacts, this is the HTML of the visualization in addition to the
JSON of the underlying and visualized data.

### Download data

The download option is identical to the copy option, but provides a context-appropriate file type (e.g., tables are
represented as CSV files).

### Download artifact definition

The artifact definition option provides a JSON file for each artifact that provides additional information, such as the
identifier, along with the artifact's raw data.

:::info reuse artifacts

You can reuse artifacts via the PromptQL Playground; look for the share icon in the top-right corner of an artifact to
reuse it in a new thread.

:::



==============================



# public-projects.mdx

URL: https://hasura.io/docs/promptql/promptql-playground/public-projects


# Public Projects

## Introduction

PromptQL allows you to make your project accessible to anyone with a unique URL. This feature enables easy sharing and
collaboration without the need for explicit access grants.

## Enabling Public PromptQL Playground

### Step 1. Access PromptQL Settings

Navigate to the PromptQL settings by clicking the gear icon in the bottom left corner of the console project page. From
there, select the PromptQL section.

### Step 2. Locate Public PromptQL Playground Section

Find the "Enable Public PromptQL Playground" option on the PromptQL settings page.

<Thumbnail alt="Enable Public PromptQL Playground" src="/img/get-started/public-promptql.png" />

### Step 3. Enable and Save

Toggle the switch to enable the Public PromptQL Playground and save your changes.

### Step 4. Get Shareable URL

After enabling, a unique URL will be generated. This URL can be shared with anyone you want to give access to your
PromptQL project.

:::info Authentication

When users access your public PromptQL project via the shared URL, they only need to log in to Hasura. No additional
access granting is required, making it truly public in nature.

:::

## Custom Project README

When a project is set to public mode, project administrators can create a custom README in Markdown format. This README
is visible to all users who access the project's public URL, providing essential context and information.

To add a custom README:

1. Navigate to the "README" section in the Project sidebar
2. Compose or paste your Markdown content
3. Click the Save button to apply changes
4. For enabled public PromptQL projects, a public URL is displayed at the top of the page

<Thumbnail alt="Custom Project README" src="/img/get-started/promptql-readme.png" />

The custom README feature allows you to:

- Provide project context and background
- Offer usage instructions or guidelines
- Share relevant documentation or resources
- Highlight key features or functionalities
- Set expectations for your users

By utilizing this feature effectively, you can enhance the user experience and facilitate better understanding of your
public PromptQL project.

:::info Token Usage and Billing

Token usage in public projects is charged to your PromptQL credits by default. However, project owners can set up a
custom LLM to manage token usage according to their preferences. During your 30-day trial period, you'll receive
complimentary promotional credits to get started.

When your trial period expires, [reach out to our sales team](https://hasura.io/contact-us) to discuss available pricing
plans and options.

:::



==============================



# saved-prompts.mdx

URL: https://hasura.io/docs/promptql/promptql-playground/saved-prompts

# Saved Prompts

Saved and Shareable Prompts allow you to streamline collaboration and user onboarding in PromptQL projects. By appending
a predefined prompt to a URL, you can guide users directly into a conversation with the prompt pre-filled in the input
chat box, ready for execution.

## How It Works

Saved and Shareable Prompts utilize URL parameters to embed predefined prompts within a link. When a user clicks the
link, the specified prompt is automatically loaded into the chat box, ensuring a seamless experience.

## URL Format

To create a shareable prompt, append the prompt query parameter to a PromptQL playground URL:

```code
https://promptql.console.hasura.io/public/<your_project>/playground?prompt=<encoded_prompt>
```

For example:

```code
https://promptql.console.hasura.io/public/duckduckgo/playground?prompt=find%20the%20top%2010%20results%20for%20hasura
```

In this case, the prompt `find the top 10 results for hasura` is URL-encoded and pre-filled in the chat input when the
link is opened.

## URL Encoding

Special characters in the prompt are automatically escaped to ensure proper functionality. This ensures that even
complex prompts with spaces or special symbols can be shared without issues.



==============================



# troubleshooting.mdx

URL: https://hasura.io/docs/promptql/promptql-playground/troubleshooting

# Troubleshooting

## Introduction

Below, you'll find common errors and how to troubleshoot them.

## Error streaming from LLM: Illegal header value b'Bearer

This error happens when the PromptQL playground has not been started with a PromptQL Secret key.

This typically indicates that a DDN project was not initialized with the local project or the project does not have an
associated PromptQL Secret key.

```ddn title="To check if the project was initialized, run this in the project directory:"
ddn context get project
```

### Case 1: Project has not been initialized

If you get the error `Key "project" is not set.`, you need to initialize a DDN project.

```ddn title="Run the following command in the project directory:"
ddn project init
```

Restarting the local services post this should fix the issue.

### Case 2: Project has been initialized but does not have a PromptQL Secret key

If you already have a project initialized, you might need to generate the PromptQL Secret key for the project.

```ddn title="Run the following command in the project directory to generate a PromptQL Secret key:"
ddn auth generate-promptql-secret-key
```

:::info Use the latest version of the CLI

Ensure you're using the latest version of the CLI:

```ddn
ddn update-cli
```

:::

Restarting the local services post this should fix the issue.

## Error streaming from LLM: Client error '403 Forbidden' for url `https://llm.promptql.pro.hasura.io/stream` For more information check: `https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403`

This error happens when the PromptQL playground has been started with an invalid PromptQL Secret key.

This typically happens if an existing PromptQL Secret key is deleted or becomes invalid.

```ddn title="Run the following command in the project directory to generate a new PromptQL Secret key:"
ddn auth generate-promptql-secret-key
```

:::info Use the latest version of the CLI

Ensure you're using the latest version of the CLI:

```ddn
ddn update-cli
```

:::

Restarting the local services post this should fix the issue.

## Local PromptQL threads disappear on service restarts

A local file to store the PromptQL Playground state is required to persist threads data. We can create this file and
mount it to the `promptql-playground` service to ensure the threads data stays persisted.

**Step 1. Create an empty file to store the Playground state**

In your project root directory, run:

```bash copy
touch .promptql_playground.db
```

**Step 2. Mount the file to the Playground service**

Update the project `compose.yaml` file as follows:

```yaml
...
promptql-playground:
   ...
   volumes:
     ...
     - ./.promptql_playground.db:/app/promptql_playground/promptql_playground.db
   ...
```

**Step 3. Restart the local services**

Restart the local services, e.g. `docker compose down && ddn run docker-start`.



==============================



# overview.mdx

URL: https://hasura.io/docs/promptql/promptql-apis/overview


# PromptQL APIs

## Introduction

End users can interact with a PromptQL application via the PromptQL Playground—our built-in GUI—but, PromptQL APIs let
you embed AI-native features—like natural language querying and intelligent program execution—directly into your app,
powered by Hasura DDN.

## Available APIs

- **[Natural Language API](/promptql-apis/natural-language-api.mdx)** - Enable conversational access to your data. This
  API lets you send user messages, stream assistant responses, and manage artifacts like text and tables as part of the
  interaction.

- **[Execute Program API](/promptql-apis/execute-program-api.mdx)** - Run exported PromptQL programs with your data
  using a single endpoint. This API is ideal for more structured interactions or automations.

Each API is backed by Hasura DDN, meaning you can connect your own project, pass context via artifacts, and control
behavior using system instructions and customizations.



==============================



# natural-language-api.mdx

URL: https://hasura.io/docs/promptql/promptql-apis/natural-language-api

# Natural Language API

## Introduction

The Natural Language Query API allows you to interact with PromptQL directly, sending messages and receiving responses
with support for streaming. This API is particularly useful for building interactive applications that need to
communicate with PromptQL in real-time.

```json title="Example request using our quickstart:"
{
  "version": "v1",
  "promptql_api_key": "<YOUR_API_KEY_HERE>",
  "llm": {
    "provider": "hasura"
  },
  "ddn": {
    "url": "https://<PROJECT_NAME>.ddn.hasura.app/v1/sql",
    "headers": {}
  },
  "artifacts": [],
  "system_instructions": "Responde solo en espanol, por favor",
  "timezone": "America/Los_Angeles",
  "interactions": [
    {
      "user_message": {
        "text": "What can you do?"
      },
      "assistant_actions": []
    }
  ],
  "stream": false
}
```

<details>
  <summary>
    Click here to see a sample response ☝️
  </summary>

```json title="Example response using our quickstart:"
{
  "assistant_actions": [
    {
      "message": "¡Hola! Soy un asistente que puede ayudarte con varias tareas relacionadas con películas. Específicamente, puedo:\n\n1. Buscar y analizar películas de la base de datos basado en diferentes criterios como:\n   - Título\n   - Director\n   - Año de lanzamiento\n   - Género\n   - Calificación IMDB\n   - Actores\n   - Y más\n\n2. Ayudarte a rentar películas a través del servicio Promptflix\n\n3. Crear análisis y resúmenes personalizados de películas\n\n4. Responder preguntas sobre películas específicas o tendencias generales\n\n¿Hay algo específico sobre películas que te gustaría explorar?",
      "plan": null,
      "code": null,
      "code_output": null,
      "code_error": null
    }
  ],
  "modified_artifacts": []
}
```

</details>

## Query Endpoint

Send messages to PromptQL and receive responses, optionally in a streaming format.

### Request

`POST https://api.promptql.pro.hasura.io/query`

**Headers**

```
Content-Type: application/json
```

**Request Body**

```json
{
  "version": "v1",
  "promptql_api_key": "<YOUR_API_KEY_HERE>",
  "llm": {
    "provider": "hasura"
  },
  "ddn": {
    "url": "https://<PROJECT_NAME>.ddn.hasura.app/v1/sql",
    "headers": {}
  },
  "artifacts": [],
  "system_instructions": "Optional system instructions for the LLM",
  "timezone": "America/Los_Angeles",
  "interactions": [
    {
      "user_message": {
        "text": "Your message here"
      },
      "assistant_actions": [
        {
          "message": "Previous assistant message",
          "plan": "Previous execution plan",
          "code": "Previously executed code",
          "code_output": "Previous code output",
          "code_error": "Previous error message if any"
        }
      ]
    }
  ],
  "stream": false
}
```

**Request Body Fields**

| Field                 | Type    | Required | Description                                                                                                                                                                                                                                                                                       |
| --------------------- | ------- | -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `version`             | string  | Yes      | Must be set to "v1"                                                                                                                                                                                                                                                                               |
| `promptql_api_key`    | string  | Yes      | PromptQL API key created from project settings                                                                                                                                                                                                                                                    |
| `llm`                 | object  | No       | Configuration for the main LLM provider                                                                                                                                                                                                                                                           |
| `ai_primitives_llm`   | object  | No       | Optional Configuration for the AI primitives LLM provider. If this is missing, the main LLM provider is used                                                                                                                                                                                      |
| `ddn`                 | object  | Yes      | DDN configuration including URL and headers                                                                                                                                                                                                                                                       |
| `artifacts`           | array   | No       | List of existing artifacts to provide context                                                                                                                                                                                                                                                     |
| `system_instructions` | string  | No       | Custom system instructions for the LLM                                                                                                                                                                                                                                                            |
| `timezone`            | string  | Yes      | IANA timezone for interpreting time-based queries                                                                                                                                                                                                                                                 |
| `interactions`        | array   | Yes      | List of message interactions, including user messages and assistant responses. Each interaction contains a `user_message` object with the user's text and an optional `assistant_actions` array containing previous assistant responses with their messages, plans, code executions, and outputs. |
| `stream`              | boolean | Yes      | Whether to return a streaming response                                                                                                                                                                                                                                                            |

**LLM Provider Options**

The `llm` and `ai_primitives_llm` fields support the following providers:

1. Hasura:

```json
{
  "provider": "hasura"
}
```

:::info Free credits

You get $20 worth of free credits with Hasura's built-in provider during the trial.
[Reach out to us](https://hasura.io/contact-us) to add more credits.

:::

2. Anthropic:

```json
{
  "provider": "anthropic",
  "api_key": "<your anthropic api key>"
}
```

3. OpenAI:

```json
{
  "provider": "openai",
  "api_key": "<your openai api key>"
}
```

#### Request DDN Auth

The `ddn.headers` field can be used to pass any auth header information through to DDN. Read more about
[auth with these APIs](/promptql-apis/auth.mdx).

### Response

The response format depends on the `stream` parameter:

**Non-streaming Response** (application/json)

```json
{
  "assistant_actions": [
    {
      "message": "Response message",
      "plan": "Execution plan",
      "code": "Executed code",
      "code_output": "Code output",
      "code_error": "Error message if any"
    }
  ],
  "modified_artifacts": [
    {
      "identifier": "artifact_id",
      "title": "Artifact Title",
      "artifact_type": "text|table",
      "data": "artifact_data"
    }
  ]
}
```

**Streaming Response** (text/event-stream)

The streaming response sends chunks of data in Server-Sent Events (SSE) format:

```JSON
data: {
  "message": "Chunk of response message",
  "plan": null,
  "code": null,
  "code_output": null,
  "code_error": null,
  "type": "assistant_action_chunk",
  "index": 0
}

data: {
  "type": "artifact_update_chunk",
  "artifact": {
    "identifier": "artifact_id",
    "title": "Artifact Title",
    "artifact_type": "text|table",
    "data": "artifact_data"
  }
}
```

### Response Fields

| Field                | Type  | Description                                                  |
| -------------------- | ----- | ------------------------------------------------------------ |
| `assistant_actions`  | array | List of actions taken by the assistant                       |
| `modified_artifacts` | array | List of artifacts created or modified during the interaction |

**Assistant Action Fields**

| Field         | Type   | Description                            |
| ------------- | ------ | -------------------------------------- |
| `message`     | string | Text response from the assistant       |
| `plan`        | string | Execution plan if any                  |
| `code`        | string | Code that was executed                 |
| `code_output` | string | Output from code execution             |
| `code_error`  | string | Error message if code execution failed |

### Error Response

```json
{
  "type": "error_chunk",
  "error": "Error message describing what went wrong"
}
```

## Notes for using the Natural Language API

1. **Streaming vs Non-streaming**

   - Use streaming (`stream: true`) for real-time interactive applications
   - Use non-streaming (`stream: false`) for batch processing or when you need the complete response at once

2. **Artifacts**

   - You can pass existing artifacts to provide context for the conversation
   - Both text and table artifacts are supported
   - Artifacts can be modified or created during the interaction

3. **Timezone Handling**

   - Always provide a timezone to ensure consistent handling of time-based queries
   - Use standard [IANA timezone formats](https://www.iana.org/time-zones) (e.g., "America/Los_Angeles",
     "Europe/London")

4. **Error Handling**
   - Always check for error responses, especially in streaming mode
   - Implement appropriate retry logic for transient failures



==============================



# execute-program-api.mdx

URL: https://hasura.io/docs/promptql/promptql-apis/execute-program-api


# Execute Program API

## Introduction

Under the hood, PromptQL utilizes a set of built-in primitives, in conjunction with any
[custom business logic](/business-logic/overview.mdx) you've provided, to create small programs on top of your data.

These primitives pair relevant data or artifacts—represented as `inputs` or `data`—from your application with on-the-fly
and context-aware `instructions` based on the thread between the user and PromptQL.

### Summarize

```python title="Example:"
summaries = executor.summarize(
    instructions="""
    Create a concise summary of each reservation focusing on:
    - The restaurant name
    - Any special notes or important details
    Keep each summary to 1-2 sentences.
    """,
    inputs=reservation_texts
)
```

### Extract

```python title="Example:"
extracted_info = executor.extract(
        json_schema=json_schema,
        instructions="""
        Extract any mentioned dietary requirements, special occasions, or seating preferences from the reservation notes.
        For dietary requirements, include any allergies, restrictions, or preferences mentioned.
        For special occasions, look for mentions of birthdays, anniversaries, celebrations etc.
        For seating preferences, identify if they specifically requested indoor, outdoor, bar seating etc.
        If any field is not mentioned, omit it from the output.
        """,
        inputs=notes_to_extract
    )
```

### Classify

```python title="Example:"
classifications = executor.classify(
    instructions="""
    Categorize each restaurant based on its name and any available description.
    A restaurant can belong to multiple categories.
    Consider both the dining style (Fine Dining, Casual) and cuisine type.
    Examples:
    - A high-end Italian restaurant would be both "Fine Dining" and "Italian"
    - A casual American pub would be both "Casual Dining" and "American"
    """,
    categories=categories,
    allow_multiple=True,
    inputs=unique_restaurants
)
```

### Visualize

```python title="Example:"
viz_result = executor.visualize(
    instructions="""
    Create an interactive bar chart showing the distribution of restaurant categories.
    Requirements:
    - Use a horizontal bar chart for better readability of category names
    - Sort bars by count in descending order
    - Use a pleasant color scheme (preferably blues or teals)
    - Include hover tooltips showing exact count
    - Add a clear title 'Restaurant Categories Distribution'
    - Make the chart responsive to container width
    - Include total number of restaurants in each category
    """,
    data=viz_data
)
```

:::info Think of this as a toolbox

Each primitive is a specialized tool designed to perform a specific task on your data. They're flexible, composable, and
context-aware, letting you build intelligent workflows by combining natural language instructions with structured input.

As you add your own custom business logic, you're providing more specific tools to PromptQL.

:::

## Execute Program Endpoint

Execute a PromptQL program with your data.

### Request

`POST https://api.promptql.pro.hasura.io/execute_program`

**Headers**

```
Content-Type: application/json
```

**Request Body**

```json
{
  "code": "<YOUR_PROMPTQL_GENERATED_CODE_HERE>",
  "promptql_api_key": "<YOUR_API_KEY_HERE>",
  "ai_primitives_llm": {
    "provider": "hasura"
  },
  "ddn": {
    "url": "https://<PROJECT_NAME>.ddn.hasura.app/v1/sql",
    "headers": {}
  },
  "artifacts": []
}
```

**Request Body Fields**

| Field               | Type   | Required | Description                                              |
| ------------------- | ------ | -------- | -------------------------------------------------------- |
| `code`              | string | Yes      | The PromptQL program code to execute                     |
| `promptql_api_key`  | string | Yes      | PromptQL API key created from project settings           |
| `ai_primitives_llm` | object | Yes      | Configuration for the AI primitives LLM provider         |
| `ddn`               | object | Yes      | DDN configuration including URL and headers              |
| `artifacts`         | array  | Yes      | List of artifacts to provide as context or initial state |

**LLM Provider Options**

The `ai_primitives_llm` field supports the following providers:

1. Hasura:

```json
{
  "provider": "hasura"
}
```

2. Anthropic:

```json
{
  "provider": "anthropic",
  "api_key": "<your anthropic api key>"
}
```

3. OpenAI:

```json
{
  "provider": "openai",
  "api_key": "<your openai api key>"
}
```

**Artifacts**

The `artifacts` array can contain both text and table artifacts:

1. Text Artifact:

```json
{
  "identifier": "my_text",
  "title": "My Text Document",
  "artifact_type": "text",
  "data": "Text content here"
}
```

2. Table Artifact:

```json
{
  "identifier": "my_table",
  "title": "My Data Table",
  "artifact_type": "table",
  "data": [
    {
      "column1": "value1",
      "column2": "value2"
    }
  ]
}
```

#### Request DDN Auth

The `ddn.headers` field can be used to pass any auth header information through to DDN. Read more about
[auth with these APIs](/promptql-apis/auth.mdx).

### Response

```json
{
  "output": "<program output>",
  "error": null,
  "accessed_artifact_ids": ["artifact1", "artifact2"],
  "modified_artifacts": [
    {
      "identifier": "new_artifact",
      "title": "New Artifact",
      "artifact_type": "table",
      "data": [
        {
          "column1": "value1",
          "column2": "value2"
        }
      ]
    }
  ],
  "llm_usages": [
    {
      "provider": "anthropic",
      "model": "claude-3-5-sonnet-20241022",
      "input_tokens": 691,
      "output_tokens": 33
    }
  ]
}
```

### Response Fields

| Field                   | Type         | Description                                                      |
| ----------------------- | ------------ | ---------------------------------------------------------------- |
| `output`                | string       | The program's output, similar to what you see in the playground  |
| `error`                 | string\|null | Error message if execution failed, null otherwise                |
| `accessed_artifact_ids` | array        | List of artifact identifiers that were accessed during execution |
| `modified_artifacts`    | array        | List of artifacts that were created or modified during execution |
| `llm_usages`            | array        | Details about LLM usage during execution                         |

**LLM Usage Fields**

| Field           | Type    | Description                                                   |
| --------------- | ------- | ------------------------------------------------------------- |
| `provider`      | string  | The LLM provider used (e.g., "hasura", "anthropic", "openai") |
| `model`         | string  | The specific model used                                       |
| `input_tokens`  | integer | Number of input tokens consumed                               |
| `output_tokens` | integer | Number of output tokens generated                             |

### Error Response

When the API encounters an error, it will return a 422 status code with a validation error response:

```json
{
  "detail": [
    {
      "loc": ["field_name"],
      "msg": "error message",
      "type": "error_type"
    }
  ]
}
```

## Notes for using the Execute Program API

1. **Program Code**

   - Ensure your PromptQL program code is properly formatted and tested
   - You can export working programs from the PromptQL playground using the "Export as API" button

2. **Artifacts**

   - Provide all necessary artifacts that your program needs to run
   - Make sure artifact identifiers match those referenced in your program code
   - Both text and table artifacts are supported

3. **Error Handling**
   - Always check the error field in the response
   - Implement appropriate retry logic for transient failures
   - Validate your inputs against the API schema to catch issues early

## Get PromptQL programs from the playground

You don't have to write these programs yourself! You can get PromptQL programs from Playground interactions by clicking
on the "Export as API" button next to the Query Plan. Then, you can use the exported program in the `code` key-value
pair in your request to the `/execute_program` endpoint.

<Thumbnail src="/img/get-started/export-as-api.png" alt="Export as API" />



==============================



# auth.mdx

URL: https://hasura.io/docs/promptql/promptql-apis/auth

# Authentication in APIs

## Introduction

PromptQL endpoints accept a `ddn` object which contains details like your project's URL and what headers to pass along.
These endpoints accept all DDN-compatible authentication strategies; learn more about these in our
[auth docs](/auth/overview.mdx).

## Modes

### Default

```json {10} title="By default, you can use the short-lived x-hasura-ddn-token to make requests against the Execute Program API:"
{
  "code": "# Get Saving Private Ryan's details\nsql = \"\"\"\nSELECT series_title, overview, genre, director, imdb_rating, released_year\nFROM app.movies\nWHERE LOWER(series_title) = 'saving private ryan'\n\"\"\"\nmovie = executor.run_sql(sql)\n\nif len(movie) == 0:\n    executor.print(\"Movie not found\")\nelse:\n    movie = movie[0]\n    # Prepare text for classification\n    classification_text = f\"\"\"\nMovie: {movie['series_title']}\nOverview: {movie['overview']}\n\"\"\"",
  "promptql_api_key": "<YOUR_API_KEY>",
  "ai_primitives_llm": {
    "provider": "hasura"
  },
  "ddn": {
    "url": "https://<PROJECT_NAME>.ddn.hasura.app/v1/sql",
    "headers": {
      "x-hasura-ddn-token": "<YOUR_DDN_AUTH_TOKEN_FROM_THE_PLAYGROUND>"
    }
  },
  "artifacts": []
}
```

### JWT & Webhook Mode

```json {10} title="If you're using JWT or Webhook Mode, you'll pass your authentication header:"
{
  "code": "# Get Saving Private Ryan's details\nsql = \"\"\"\nSELECT series_title, overview, genre, director, imdb_rating, released_year\nFROM app.movies\nWHERE LOWER(series_title) = 'saving private ryan'\n\"\"\"\nmovie = executor.run_sql(sql)\n\nif len(movie) == 0:\n    executor.print(\"Movie not found\")\nelse:\n    movie = movie[0]\n    # Prepare text for classification\n    classification_text = f\"\"\"\nMovie: {movie['series_title']}\nOverview: {movie['overview']}\n\"\"\"",
  "promptql_api_key": "<YOUR_API_KEY>",
  "ai_primitives_llm": {
    "provider": "hasura"
  },
  "ddn": {
    "url": "https://<PROJECT_NAME>.ddn.hasura.app/v1/sql",
    "headers": {
      "authorization": "Bearer <YOUR_TOKEN>"
    }
  },
  "artifacts": []
}
```

:::info Authorization Strategies

The example above uses the Bearer strategy for the `tokenLocation`. Your setup may be different; consult our
[auth docs](/auth/overview.mdx) for more information about setting up an authentication mode.

:::



==============================



# index.mdx

URL: https://hasura.io/docs/promptql/promptql-apis/sdk/

# Available SDKs

## Introduction

Hasura provides a set of officially-supported SDKs to make development with PromptQL easier. The following libraries
enable you to connect to a PromptQL instance and interact with both the
[Natural Language](/promptql-apis/natural-language-api.mdx) and
[Execute Program](/promptql-apis/execute-program-api.mdx) APIs.

## Learn more

- [Node.js](/promptql-apis/sdk/nodejs.mdx)
- [Python](/promptql-apis/sdk/python.mdx)
- [Go](/promptql-apis/sdk/go.mdx)



==============================



# nodejs.mdx

URL: https://hasura.io/docs/promptql/promptql-apis/sdk/nodejs

# Node.js SDK

## Introduction

The Node.js SDK is available [here](https://www.npmjs.com/package/@hasura/promptql).

## Install

```sh title="The SDK can be installed using npm:"
npm i @hasura/promptql
```

## Connect

```js title="You'll connect to an instnace by creating a client:"

const client = createPromptQLClient({
  apiKey: "<your-promptql-api-key>",
  ddn: {
    url: "<your-project-endpoint>",
    headers: {
      Authorization: "<credential>",
    },
  },
});
```

## Query the Natural Language API

```js title="Then, use your client to query the Natural Language API:"
client
    .queryStream({
        artifacts: [],
        interactions: [
            user_message: {
                text: 'what can you do?',
            }
        ],
    },
    async (chunk) => {
        console.log(chunk);
    },
);
```

## Query the Execute Program API

```ts title="Or, the Exceute Program API:"
function executeProgram(body: PromptQLExecuteRequest, executeOptions?: FetchOptions) Promise<PromptQlExecutionResult>;
```



==============================



# python.mdx

URL: https://hasura.io/docs/promptql/promptql-apis/sdk/python

# Python SDK

## Introduction

The Python SDK is available [here](https://pypi.org/project/promptql-api-sdk/).

:::info The Python SDK currently supports the Natural Language API

Support for the Execute Program API will be released soon.

:::

## Install

```sh title="The SDK can be installed using pip:"
pip install promptql-api-sdk
```

```sh title="Or Poetry:"
poetry add promptql-api-sdk
```

## Connect

```python title="You'll connect to an instnace by creating a client:"
from promptql_api_sdk import PromptQLClient
from promptql_api_sdk.types.models import HasuraLLMProvider

# Initialize the client
client = PromptQLClient(
    api_key="your-promptql-api-key",
    ddn_url="your-ddn-url",
    llm_provider=HasuraLLMProvider(),
    timezone="America/Los_Angeles",
)
```

## Query the Natural Language API

```python title="Then, use your client to query the Natural Language API:"
# Send a simple query
response = client.query("What is the average temperature in San Francisco?")
print(response.assistant_actions[0].message)

# Use streaming for real-time responses
for chunk in client.query("Tell me about the weather in New York", stream=True):
    if hasattr(chunk, "message") and chunk.message:
        print(chunk.message, end="", flush=True)
```



==============================



# go.mdx

URL: https://hasura.io/docs/promptql/promptql-apis/sdk/go

# Go SDK

## Introduction

The Go SDK is available [here](https://github.com/hasura/promptql-go-sdk).

## Install

```sh title="The SDK can be installed using the Go CLI:"
go get github.com/hasura/promptql-go-sdk
```

## Connect

```go title="You'll connect to an instnace by creating a client:"
    "github.com/hasura/promptql-go-sdk/promptql"
)

client, err := promptql.NewClient("<promptql-api-key>", &promptql.ClientConfig{
    DdnBaseURL: "https://your-ddn-project",
    DdnHeaders: map[string]string{
        // Optional: add authorization headers if required by your DDN project
        // "Authorization": "Bearer <token>",
    },
})

if err != nil {
    log.Fatalf("failed to create client: %s", err)
}
```

## Query the Natural Language API

```js title="Then, use your client to query the Natural Language API:"
result, err := client.Query(
    context.Background(),
    promptql.NewQueryRequestMessage("what can you do?"),
)

if err != nil {
    log.Fatalf("query failed: %s", err)
}


// Get the response
if len(result.AssistantActions) > 0 {
    if msg := result.AssistantActions[0].Message.Get(); msg != nil {
        log.Println(msg)
    }
}

```

## Query the Execute Program API

```go title="Or, the Exceute Program API:"
func (c *Client) ExecuteProgram(ctx context.Context, body api.ExecuteRequest) (*api.PromptQlExecutionResult, error)
```



==============================



# Recipes

URL: https://hasura.io/docs/promptql/recipes/overview


# Recipes

## Introduction

There's loads of ways you can build an application with PromptQL. Whether you're connecting to a single data source,
want to connect to a variety of existing API endpoints, or want to bring API data over en masse to avoid issues like
rate-limiting and throttling, you can do it with PromptQL.

## Find out more

For now, check out our [tutorials section](/recipes/tutorials/index.mdx) with cloneable repos to get you started!



==============================



# playground-auth.mdx

URL: https://hasura.io/docs/promptql/auth/playground-auth


# PromptQL Playground Auth

You can check the auth which the PromptQL Playground client is using by clicking the Auth button on the left-hand side
of the chat dialogue.

<Thumbnail src="/img/auth/piql-auth-button.png" alt="Authentication using JWT" />

## Configure headers

<Thumbnail src="/img/auth/piql-configure-headers.png" alt="Authentication using JWT" />

In the headers tab you will be able to see the key value pairs of the headers which are being sent to the Hasura DDN
engine for each request.

You can also add custom headers to the request by clicking the `+` button.

:::info Reserved headers

The `x-hasura-ddn-token` header is automatically added by the PromptQL Playground client and should not be modified. If
passing a custom JWT in [JWT mode](/auth/jwt/jwt-mode.mdx) it should be set with another `x-hasura-*` header key.

:::

## Authorization

The `Authorization` tab shows the current authorization mode, current user role, and will show any values of session
variables if there are any set.

<Thumbnail src="/img/auth/piql-configure-headers-authorization.png" alt="Authentication using JWT" />



==============================



# Authentication and Authorization Overview

URL: https://hasura.io/docs/promptql/auth/overview

# Auth

## Introduction

By default, PromptQL accesses your data through Hasura DDN in [NoAuth](/auth/noauth-mode.mdx) mode as an `admin` user.
This means that by default on a new installation, it has full access to your data.

However, you can integrate many popular auth services or use your own custom solution to authenticate users.

After the authentication step, session variables, including a user role, are passed via either a valid JWT or webhook
response, to the Hasura Engine to be checked against the access control rules or "permissions" to determine what data
the user can access. This ensures any PromptQL conversations are scoped to only return information that a particular
user is permitted to access.

As you will most often be using PromptQL in the console interface, you can check the auth which the client is using by
clicing the "Auth" button on the left of the chat textbox.

## PromptQL Playground Auth

Learn how to check the auth which the PromptQL Playground client is using [here](/auth/playground-auth.mdx).

## AuthConfig options

Authentication can be set up in one of three modes. These modes and their configuration options are specified in the
`AuthConfig` object within your metadata.

### JWT mode

Your authentication service must issue JWTs which contain session variables that are passed to the Hasura Engine by the
PromptQL client on each request.

[Read more](/auth/jwt/index.mdx).

### Webhook mode

PromptQL will call a webhook on each request with the client headers forwarded. On successful authentication, the
webhook must return a valid `http` response with session variables in the body.

[Read more](/auth/webhook/index.mdx).

### NoAuth mode

No authentication is required for a specific role to access the data.

[Read more](/auth/noauth-mode.mdx).



==============================



# Tutorials

URL: https://hasura.io/docs/promptql/recipes/tutorials/

# Tutorials

This section of documentation contains ready-made repositories that you can clone and get started with PromptQL
immediately.

- [GitHub Assistant](/recipes/tutorials/github-assistant.mdx)
- [Apple Health Assistant](/recipes/tutorials/apple-health-assistant.mdx)
- [Hugginface Datsets](/recipes/tutorials/huggingface-csv-parquet-sqlite.mdx)
- [Realtime chat with the BART API](/recipes/tutorials/bart-api.mdx)
- [CSV Files](/recipes/tutorials/csv-files.mdx)
- [Kaggle Datasets](/recipes/tutorials/kaggle-csv-sqlite.mdx)
- [DuckDuckGo Web Search](/recipes/tutorials/duckduckgo-web-search.mdx)
- [Add Vector Search to PostgreSQL](/recipes/tutorials/add-vector-search-to-postgresql.mdx)



==============================



# GitHub Assistant

URL: https://hasura.io/docs/promptql/recipes/tutorials/github-assistant


# GitHub Assistant

In this guide, you'll get hands on experience with PromptQL and the GitHub connector. We will be building a "Github
Issues" assistant.

You will learn how to:

- Set up PromptQL to work with a GitHub issues and pull requests.
  - You will be able to do natural language query to triage feature requests, bugs and other issues with your connected
    Github repository.
- Create and configure the necessary API keys.
- Run your PromptQL project in a local environment.
- Ask your questions with PromptQL playground.


## Prerequisites

**Install the DDN CLI**

:::info Minimum version requirements

To use this guide, ensure you've installed/updated your CLI to at least `v2.28.0`.

:::


<Tabs groupId="os-preference" className="api-tabs">

<TabItem value="macOS-or-linux" label="macOS and Linux">

Simply run the installer script in your terminal:

<CodeBlock language="bash">
  {`curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/get.sh | bash`}
</CodeBlock>

<Admonition type="info" title="ARM-based Linux Machines">
  Currently, the CLI does not support installation on ARM-based Linux systems.
</Admonition>

</TabItem>

<TabItem value="windows" label="Windows">
- Download the latest <a href={`https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/latest/DDN_CLI_Setup.exe`}>DDN CLI installer for Windows.</a> 
- Run the `DDN_CLI_Setup.exe` installer file and follow the instructions. This will only take a minute.
- By default, the DDN CLI is installed under `C:\Users\{Username}\AppData\Local\Programs\DDN_CLI`
- The DDN CLI is added to your `%PATH%` environment variable so that you can use the `ddn` command from your terminal.
<br />

</TabItem>
</Tabs>


**Install [Docker](https://docs.docker.com/engine/install/)**

The Docker-based workflow helps you iterate and develop locally without deploying any changes to Hasura DDN, making the
development experience faster and your feedback loops shorter. **You'll need Docker Compose `v2.20` or later.**

**Validate the installation**

You can verify that the DDN CLI is installed correctly by running:

```ddn
ddn doctor
```


## Build your PromptQL app

##ge# Clone the example project

The [example project](https://github.com/hasura/example-promptql-github) is already set up to connect PromptQL to any
GitHub repo.

```bash copy
git clone git@github.com:hasura/example-promptql-github.git
cd example-promptql-github
```

### Set up your .env file

Set up your .env file with your GitHub API token.

```bash copy
cp .env.example .env
```

**Add GitHub API token to .env**

For the Github integration. Create an API token from https://github.com/settings/tokens

```bash
# .env

...
GITHUB_API_TOKEN=<GITHUB_API_TOKEN>
```

This token only needs read access to the repo you are interested in.

<Thumbnail src="/img/get-started/github-permissions-dark.png" alt="Example: PromptQL on GitHub" />

### Set up your GitHub repo

Head to `app/connector/github/index.ts` and change the org name and the repo name to something you'd like:

```typescript
// index.ts

...
  const manager = new GitHubIssueSyncManager('<org-name>', '<repo-name>');
  if (!process.env.GITHUB_API_TOKEN) {
...
```

### Setup your DDN project

This will create a Hasura DDN cloud project and set up PromptQL keys to connect to the PromptQL runtime.

```ddn
ddn project init
```

### Fire up your PromptQL project

Build your supergraph.

```ddn
ddn supergraph build local
```

Then bring up the PromptQL API server, the engine and the connector

```ddn
ddn run docker-start
```

You'll notice in amongst your Docker logs that your github synchronization has started.

<details>
  <summary>Docker logs</summary>
  [2024-01-01 12:00:01] Starting GitHub sync...
  <br />
  [2024-01-01 12:00:02] Fetching repository metadata...
  <br />
  [2024-01-01 12:00:03] Syncing issues and pull requests...
  <br />
  [2024-01-01 12:00:04] Loading repository contents...
  <br />
  [2024-01-01 12:00:05] GitHub sync complete
</details>

If the specified repo has many issues or comments it may take some time to get them all and you may be rate limited.
That's ok, you can go ahead and try PromptQL without the process having fully finished yet.

If you notice some logs regarding GitHub permissions, check that your GitHub API token has the correct permissions for
the repo you're trying to access.

### Restarting the app

If you made a mis-step or want to start from scratch, simply restarting the docker containers will reset all state
including any loaded data.

```ddn
ddn run docker-start
```

## Act on your data

### Open the PromptQL playground

In another terminal, run

```ddn
ddn console --local
```

### Ask questions about your GitHub repo

The console is a web app hosted at console.hasura.io that connects to your local PromptQL API and data sources. Your
data is processed in the DDN PromptQL runtime but isn't persisted externally.

Head over to the console and ask a few questions about your GitHub repo.

```text copy
> What are the open pull requests?
> What kind of questions can I ask?
```



==============================



# Apple Health Assistant

URL: https://hasura.io/docs/promptql/recipes/tutorials/apple-health-assistant


# Apple Health Assistant

In this guide, you will learn how to set up an Apple Health Assistant using PromptQL and Hasura. This assistant empowers
you to analyze your own Apple Health data by interacting in natural language. You’ll import your health metrics from an
iPhone into a PostgreSQL database, connect it with PromptQL, and interact with the data.

Here’s a sample conversation:

<Thumbnail src="/img/get-started/fitness-activity.jpeg" alt="Apple Health Fitness Activity" />

Check out the [GitHub repo](https://github.com/praveenweb/apple-health-ai-assistant).


## Prerequisites

**Install the DDN CLI**

:::info Minimum version requirements

To use this guide, ensure you've installed/updated your CLI to at least `v2.28.0`.

:::


<Tabs groupId="os-preference" className="api-tabs">

<TabItem value="macOS-or-linux" label="macOS and Linux">

Simply run the installer script in your terminal:

<CodeBlock language="bash">
  {`curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/get.sh | bash`}
</CodeBlock>

<Admonition type="info" title="ARM-based Linux Machines">
  Currently, the CLI does not support installation on ARM-based Linux systems.
</Admonition>

</TabItem>

<TabItem value="windows" label="Windows">
- Download the latest <a href={`https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/latest/DDN_CLI_Setup.exe`}>DDN CLI installer for Windows.</a> 
- Run the `DDN_CLI_Setup.exe` installer file and follow the instructions. This will only take a minute.
- By default, the DDN CLI is installed under `C:\Users\{Username}\AppData\Local\Programs\DDN_CLI`
- The DDN CLI is added to your `%PATH%` environment variable so that you can use the `ddn` command from your terminal.
<br />

</TabItem>
</Tabs>


**Install [Docker](https://docs.docker.com/engine/install/)**

The Docker-based workflow helps you iterate and develop locally without deploying any changes to Hasura DDN, making the
development experience faster and your feedback loops shorter. **You'll need Docker Compose `v2.20` or later.**

**Validate the installation**

You can verify that the DDN CLI is installed correctly by running:

```ddn
ddn doctor
```


## Export your Apple Health Data

This is a private health assistant and it relies on the Apple Health data of the user. In order to export the Health
data from your iPhone, follow the instructions below:

- Go to the Health app on your iPhone.
- Tap your picture or initials at the top right.
- If you don’t see your picture or initials, tap Summary or Browse at the bottom of the screen, then scroll to the top
  of the screen.
- Tap Export All Health Data, then choose a method for sharing your data. If you have a Macbook, share it via AirDrop to
  get started quickly. Otherwise, use Google Drive to save and download.

Here are the docs on
[Apple Support](https://support.apple.com/guide/iphone/share-your-health-data-iph5ede58c3d/ios#iphe962dcbd2) around
this. Do note that the export can take some time depending on how much data you have. Ensure your phone doesn't lock
during the export to prevent interruption.

## Import your Apple Health data into PostgreSQL

Step 1: Clone the project

```bash copy
git clone git@github.com:praveenweb/apple-health-ai-assistant.git
cd apple-health-ai-assistant
```

Step 2: Copy Apple Health export.zip

Copy the `export.zip` file obtained from the Apple Health export above into the root directory of the git repo you just
cloned.

Step 3: Configure environment variables

```bash copy
cp .env.sample .env
```

The default Postgres credentials should work. Feel free to change them as necessary.

Step 4: Start the data import using docker compose

```bash copy
docker compose up -d –build
```

This should run the Python program, which takes in the `export.zip` that was placed in the root directory, unpacks and
parses the XML data, and inserts it into the PostgreSQL database.

**Note**: Depending on how big your health export is, it should take anywhere between 30 seconds to 2 minutes to fully
proceed to follow the steps below.

## Build your PromptQL app

Now, let's set up the Hasura DDN project with PromptQL to start exploring the data in natural language!

- Set up the Hasura DDN project already scaffolded in the repo:

```bash copy
cd ddn-project
ddn supergraph build local
ddn project init
```

- Add Anthropic API key to the`.env`.

Get an api key from https://console.anthropic.com/settings/keys

```.env file
...
ANTHROPIC_API_KEY=<your-anthropic-api-key>
```

To use an OpenAI API key instead, you’ll have to set the `OPENAI_API_KEY` key-value in your `.env` file and change the
environment variable LLM to `openai` in the compose.yaml file.

- Start the DDN project

Let's start the DDN project by executing the following command:

```ddn
ddn run docker-start
```

- Open the local DDN Console to start exploring:

```ddn
ddn console --local
```

This should open up your browser (or print a browser URL) for displaying the Hasura Console. It’ll typically be
something like:
[https://promptql.console.hasura.io/local?engine=localhost:3280&promptql=localhost:3282](https://promptql.console.hasura.io/local?engine=localhost:3280&promptql=localhost:3282)

## Ask questions about personal health

The app has metadata about various health metrics and it can answer a range of questions; as simple as "What’s my step
count per day?" to as complicated as, "Give me a nutrition plan based on my weight / BMR, activity and heart health to
reduce 1 pound every 2 weeks."

Here's a sample of what you can ask:

- What's my average walking per day?
- How much time am I spending outdoors?
- Based on my weight and BMR, tell me if I'm on track to lose weight and what should my calorie intake be for losing 1
  pound per week. Analyze data over the last 3 months.
- Am I being exposed to a lot of noise in the environment? Check the impact on my hearing health.
- How is my heart health?
- How active have I been over the last few months? Are there any patterns and anomalies in my activity?
- How much exercise activity did I do in the last year? What's the trend looking like?

## Clean up and restart your app

If you want to reset the Apple Health data and start from scratch, you can execute the following in the root directory
of the repo:

```bash copy
docker compose down -v && docker compose up -d --build
```

## Interactive demo

Here is an interactive demo of the AI Assistant. We'll start by looking at the exported data and then talk to it with
PromptQL! You can click the `Replay` button at any point to restart the demo.

<div style={{ position: "relative", paddingBottom: "calc(49.14021164021164% + 41px)", height: 0, width: "100%" }}>
  <iframe
    src="https://demo.arcade.software/nsce2VetitXx7SAwQ3BB?embed&embed_mobile=tab&embed_desktop=inline&show_copy_link=true"
    title="PromptQL: Apple Health AI Assistant"
    frameBorder="0"
    loading="lazy"
    allowFullScreen
    allow="clipboard-write"
    style={{ position: "absolute", top: 0, left: 0, width: "100%", height: "100%", colorScheme: "light" }}
  />
</div>



==============================



# Huggingface Datasets

URL: https://hasura.io/docs/promptql/recipes/tutorials/huggingface-csv-parquet-sqlite


# Huggingface Datasets

In this guide, you will learn how to import any dataset from Huggingface (like CSV, Parquet, and SQLite files) and
connect it to PromptQL to be able to query using natural language.

Check out the [GitHub repo](https://github.com/hasura/huggingface-dataset-promptql).


## Prerequisites

**Install the DDN CLI**

:::info Minimum version requirements

To use this guide, ensure you've installed/updated your CLI to at least `v2.28.0`.

:::


<Tabs groupId="os-preference" className="api-tabs">

<TabItem value="macOS-or-linux" label="macOS and Linux">

Simply run the installer script in your terminal:

<CodeBlock language="bash">
  {`curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/get.sh | bash`}
</CodeBlock>

<Admonition type="info" title="ARM-based Linux Machines">
  Currently, the CLI does not support installation on ARM-based Linux systems.
</Admonition>

</TabItem>

<TabItem value="windows" label="Windows">
- Download the latest <a href={`https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/latest/DDN_CLI_Setup.exe`}>DDN CLI installer for Windows.</a> 
- Run the `DDN_CLI_Setup.exe` installer file and follow the instructions. This will only take a minute.
- By default, the DDN CLI is installed under `C:\Users\{Username}\AppData\Local\Programs\DDN_CLI`
- The DDN CLI is added to your `%PATH%` environment variable so that you can use the `ddn` command from your terminal.
<br />

</TabItem>
</Tabs>


**Install [Docker](https://docs.docker.com/engine/install/)**

The Docker-based workflow helps you iterate and develop locally without deploying any changes to Hasura DDN, making the
development experience faster and your feedback loops shorter. **You'll need Docker Compose `v2.20` or later.**

**Validate the installation**

You can verify that the DDN CLI is installed correctly by running:

```ddn
ddn doctor
```


## Import your Huggingface Dataset

**Step 1: Clone the project**

```bash copy
git clone git@github.com:hasura/huggingface-dataset-promptql.git
cd huggingface-dataset-promptql
```

**Step 2: Get the Huggingface Dataset ID handy**

In order to import the dataset, we need to configure the Dataset ID with the path to the file(s).

For example, here's a top 1000 IMDB dataset -
[https://huggingface.co/datasets/drossi/EDA_on_IMDB_Movies_Dataset](https://huggingface.co/datasets/drossi/EDA_on_IMDB_Movies_Dataset).
Now the dataset ID for this would be something like:

```bash copy
drossi/EDA_on_IMDB_Movies_Dataset/*.csv
```

Notice the usage of glob patterns for selecting all the CSV files. Replace this with the dataset of choice along with
the path to the files. This should work for any ".csv", ".parquet" or ".sqlite" files in Huggingface.

Refer to this [DuckDB blog](https://duckdb.org/2024/05/29/access-150k-plus-datasets-from-hugging-face-with-duckdb.html)
for more examples of this format.

**Step 3: Add Anthropic API key to .env**

Set up your .env file with your Anthropic (or OpenAI) API key and GitHub API token.

```bash copy
cp .env.example .env
```

Get an api key from https://console.anthropic.com/settings/keys

```bash
# .env
...
...
ANTHROPIC_API_KEY=<your-anthropic-api-key>
```

To use an OpenAI key instead, you'll have to set `OPENAI_API_KEY` in your .env file and change the environment variable
`LLM` to `openai` in the compose.yaml file.

**Step 4: Configure .env for huggingface**

Head to the `app/connector/huggingface` directory to now configure the Dataset.

```bash copy
cd app/connector/huggingface
cp .env.sample .env
```

Modify the values for the `HUGGINGFACE_DATASET` ENV. It is of the format: "user/dataset/file-path".

For example:

```bash copy
HUGGINGFACE_DATASET="drossi/EDA_on_IMDB_Movies_Dataset/*.csv"
```

The IMDB example mentioned in the sample env is available as a sample dataset to choose. Feel free to configure the
dataset that you would like to.

**Step 5: Introspect the Huggingface Connector**

```ddn
ddn connector introspect huggingface --log-level=DEBUG
```

**Note**: Depending on how big the dataset is, it should take sometime to fully import the data. The schema will be
initialized quickly and the data import happens in the background, so you can proceed to follow the steps below.

The above command runs in DEBUG mode to make it easy to catch errors for invalid files.

**Step 6: Add Models**

Based on the dataset imported, a SQL schema would be generated. Let's track all the models to get started quickly.

```ddn
ddn model add huggingface "*"
```

## Build your PromptQL app

Now, let's set up the Hasura DDN project with PromptQL to start exploring the data in natural language!

- Set up the Hasura DDN project already scaffolded in the repo:

In the root directory of the repo, run the following commands:

```ddn
ddn supergraph build local
ddn project init
```

- Start the DDN project

Let's start the DDN project by executing the following command:

```ddn
ddn run docker-start
```

- Open the local DDN Console to start exploring:

```ddn
ddn console --local
```

This should open up your browser (or print a browser URL) for displaying the Hasura Console. It’ll typically be
something like:
[https://promptql.console.hasura.io/local?engine=localhost:3280&promptql=localhost:3282](https://promptql.console.hasura.io/local?engine=localhost:3280&promptql=localhost:3282)

## Ask questions about your dataset

The app will have metadata about the dataset that you just imported above. You should be able to ask domain specific
questions and play around with the data.

Here's a sample of what you can ask to get started.

- Hi, what can you do?

Depending on the dataset schema, PromptQL will tell you what it can answer and you can go from there.

## Clean up and restart your app

If you want to reset the data and start from scratch:

You can stop the `ddn run docker-start` command, whereever it is running and you can execute the following in the root
directory of the repo:

```bash copy
docker compose down -v && ddn run docker-start
```



==============================



# Realtime Chat with the BART API

URL: https://hasura.io/docs/promptql/recipes/tutorials/bart-api


# AI Assistant for BART (Bay Area Rapid Transit)

This is a simple example of exposing PromptQL to real-time public data using the BART API. The BART API is a public API
that provides real-time data on the Bay Area Rapid Transit system. This example demonstrates how to use PromptQL to
create a real-time chat application that allows a user to talk to the BART API and get real-time information on
departures.

Check out the [GitHub repo](https://github.com/robertjdominguez/example-promptql-bart).

Here's a sample of what you can ask:

- What time is the next train arriving at Embarcadero?
- If I leave 576 Folsom St now, can I make the next train at Montgomery?

<Thumbnail src="/img/get-started/bart-assistant.jpeg" alt="Bart AI Assistant" />


## Prerequisites

**Install the DDN CLI**

:::info Minimum version requirements

To use this guide, ensure you've installed/updated your CLI to at least `v2.28.0`.

:::


<Tabs groupId="os-preference" className="api-tabs">

<TabItem value="macOS-or-linux" label="macOS and Linux">

Simply run the installer script in your terminal:

<CodeBlock language="bash">
  {`curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/get.sh | bash`}
</CodeBlock>

<Admonition type="info" title="ARM-based Linux Machines">
  Currently, the CLI does not support installation on ARM-based Linux systems.
</Admonition>

</TabItem>

<TabItem value="windows" label="Windows">
- Download the latest <a href={`https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/latest/DDN_CLI_Setup.exe`}>DDN CLI installer for Windows.</a> 
- Run the `DDN_CLI_Setup.exe` installer file and follow the instructions. This will only take a minute.
- By default, the DDN CLI is installed under `C:\Users\{Username}\AppData\Local\Programs\DDN_CLI`
- The DDN CLI is added to your `%PATH%` environment variable so that you can use the `ddn` command from your terminal.
<br />

</TabItem>
</Tabs>


**Install [Docker](https://docs.docker.com/engine/install/)**

The Docker-based workflow helps you iterate and develop locally without deploying any changes to Hasura DDN, making the
development experience faster and your feedback loops shorter. **You'll need Docker Compose `v2.20` or later.**

**Validate the installation**

You can verify that the DDN CLI is installed correctly by running:

```ddn
ddn doctor
```


## Setup the Bart API

**Step 1: Clone the project**

```bash copy
git clone git@github.com:robertjdominguez/example-promptql-bart.git
cd example-promptql-bart
```

**Step 2: Add Anthropic API key to .env**

Set up your .env file with your Anthropic (or OpenAI) API key and GitHub API token.

```bash copy
cp .env.example .env
```

Get an api key from https://console.anthropic.com/settings/keys

```bash
# .env
...
...
ANTHROPIC_API_KEY=<your-anthropic-api-key>
```

To use an OpenAI key instead, you'll have to set `OPENAI_API_KEY` in your .env file and change the environment variable
`LLM` to `openai` in the compose.yaml file.

## Build your PromptQL app

Now, let's set up the Hasura DDN project with PromptQL to start exploring the data in natural language!

- Set up the Hasura DDN project already scaffolded in the repo:

In the root directory of the repo, run the following commands:

```ddn
ddn project init
```

Create a build:

```ddn
ddn supergraph build local
```

- Start the DDN project

Let's start the DDN project by executing the following command:

```ddn
ddn run docker-start
```

- Open the local DDN Console to start exploring:

```ddn
ddn console --local
```

This should open up your browser (or print a browser URL) for displaying the Hasura Console. It’ll typically be
something like:
[https://promptql.console.hasura.io/local?engine=localhost:3280&promptql=localhost:3282](https://promptql.console.hasura.io/local?engine=localhost:3280&promptql=localhost:3282)



==============================



# CSV Files

URL: https://hasura.io/docs/promptql/recipes/tutorials/csv-files


# AI Assistant on CSV Files

Supports importing multiple CSV files, each with their own schema.


## Prerequisites

**Install the DDN CLI**

:::info Minimum version requirements

To use this guide, ensure you've installed/updated your CLI to at least `v2.28.0`.

:::


<Tabs groupId="os-preference" className="api-tabs">

<TabItem value="macOS-or-linux" label="macOS and Linux">

Simply run the installer script in your terminal:

<CodeBlock language="bash">
  {`curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/get.sh | bash`}
</CodeBlock>

<Admonition type="info" title="ARM-based Linux Machines">
  Currently, the CLI does not support installation on ARM-based Linux systems.
</Admonition>

</TabItem>

<TabItem value="windows" label="Windows">
- Download the latest <a href={`https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/latest/DDN_CLI_Setup.exe`}>DDN CLI installer for Windows.</a> 
- Run the `DDN_CLI_Setup.exe` installer file and follow the instructions. This will only take a minute.
- By default, the DDN CLI is installed under `C:\Users\{Username}\AppData\Local\Programs\DDN_CLI`
- The DDN CLI is added to your `%PATH%` environment variable so that you can use the `ddn` command from your terminal.
<br />

</TabItem>
</Tabs>


**Install [Docker](https://docs.docker.com/engine/install/)**

The Docker-based workflow helps you iterate and develop locally without deploying any changes to Hasura DDN, making the
development experience faster and your feedback loops shorter. **You'll need Docker Compose `v2.20` or later.**

**Validate the installation**

You can verify that the DDN CLI is installed correctly by running:

```ddn
ddn doctor
```


## Import your CSV Files

**Step 1: Clone the project**

```bash copy
git clone git@github.com:hasura/csv-promptql.git
cd csv-promptql
```

**Step 2: Configure CSV file(s)**

Place your CSV file(s) inside `app/connector/csv/csv_files` directory.

```bash copy
cd app/connector/csv/csv_files
```

Add your CSV files here.

**Step 3: Introspect the Connector**

```ddn
ddn connector introspect csv
```

**Note**: Depending on how big the dataset is, it should take sometime to fully import the data. The schema will be
initialized quickly and the data import happens in the background, so you can proceed to follow the steps below.

**Step 4: Add Models**

Based on the dataset imported, a SQL schema would be generated. Let's track all the models to get started quickly.

```ddn
ddn model add csv "*"
```

## Build your PromptQL app

Now, let's set up the Hasura DDN project with PromptQL to start exploring the data in natural language!

- Set up the Hasura DDN project already scaffolded in the repo:

In the root directory of the repo, run the following commands:

```ddn
ddn supergraph build local
ddn project init
```

- Start the DDN project

Let's start the DDN project by executing the following command:

```ddn
ddn run docker-start
```

- Open the local DDN Console to start exploring:

```ddn
ddn console --local
```

This should open up your browser (or print a browser URL) for displaying the Hasura Console. It’ll typically be
something like:
[https://promptql.console.hasura.io/local?engine=localhost:3280&promptql=localhost:3282](https://promptql.console.hasura.io/local?engine=localhost:3280&promptql=localhost:3282)

## Ask questions about your dataset

The app will have metadata about the dataset that you just imported above. You should be able to ask domain specific
questions and play around with the data.

Here's a sample of what you can ask to get started.

- Hi, what can you do?

Depending on the dataset schema, PromptQL will tell you what it can answer and you can go from there.

## Clean up and restart your app

If you want to reset the data and start from scratch:

Modify or delete the csv files in the `app/connector/csv/csv_files` directory

You can stop the `ddn run docker-start` command, whereever it is running and you can execute the following in the root
directory of the repo:

```bash copy
docker compose down -v && ddn run docker-start
```



==============================



# Kaggle Datasets

URL: https://hasura.io/docs/promptql/recipes/tutorials/kaggle-csv-sqlite


# Import Kaggle Datasets | CSV, SQLite

In this guide, you will learn how to import any dataset from Kaggle (like CSV, and SQLite files) and connect it to
PromptQL to be able to query using natural language.

Check out the [GitHub repo](https://github.com/hasura/kaggle-dataset-promptql).


## Prerequisites

**Install the DDN CLI**

:::info Minimum version requirements

To use this guide, ensure you've installed/updated your CLI to at least `v2.28.0`.

:::


<Tabs groupId="os-preference" className="api-tabs">

<TabItem value="macOS-or-linux" label="macOS and Linux">

Simply run the installer script in your terminal:

<CodeBlock language="bash">
  {`curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/get.sh | bash`}
</CodeBlock>

<Admonition type="info" title="ARM-based Linux Machines">
  Currently, the CLI does not support installation on ARM-based Linux systems.
</Admonition>

</TabItem>

<TabItem value="windows" label="Windows">
- Download the latest <a href={`https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/latest/DDN_CLI_Setup.exe`}>DDN CLI installer for Windows.</a> 
- Run the `DDN_CLI_Setup.exe` installer file and follow the instructions. This will only take a minute.
- By default, the DDN CLI is installed under `C:\Users\{Username}\AppData\Local\Programs\DDN_CLI`
- The DDN CLI is added to your `%PATH%` environment variable so that you can use the `ddn` command from your terminal.
<br />

</TabItem>
</Tabs>


**Install [Docker](https://docs.docker.com/engine/install/)**

The Docker-based workflow helps you iterate and develop locally without deploying any changes to Hasura DDN, making the
development experience faster and your feedback loops shorter. **You'll need Docker Compose `v2.20` or later.**

**Validate the installation**

You can verify that the DDN CLI is installed correctly by running:

```ddn
ddn doctor
```


## Import your Kaggle Dataset

**Step 1: Clone the project**

```bash copy
git clone git@github.com:hasura/kaggle-dataset-promptql.git
cd kaggle-dataset-promptql
```

**Step 2: Get the Kaggle Credentials and Dataset Identifier**

To get the Username and Key, go to the 'Account' tab of your Kaggle user profile and select 'Create New Token'. This
will trigger the download of kaggle.json, a file containing your API credentials.

In order to import the dataset, we need to configure the Identifier of the dataset.

For example, here's an IMDB dataset -
[https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset](https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset).
Now the Kaggle Identifier for this would be something like:

```bash copy
rounakbanik/the-movies-dataset
```

Replace this with the identifier of choice. This should work for any ".csv", or ".sqlite" files in Kaggle.

**Step 3: Configure .env for kaggle**

Head to the `app/connector/kaggle` directory to now configure the Dataset.

```bash copy
cd app/connector/kaggle
cp .env.sample .env
```

Modify the values for the `KAGGLE_IDENTIFIER` ENV and add the values for `KAGGLE_USERNAME` and `KAGGLE_KEY` environment
variables obtained from the previous step from the token download.

For example:

```bash copy
KAGGLE_USERNAME="<your_username>"
KAGGLE_KEY="xxxxxxxxxxxxxxxx"
KAGGLE_IDENTIFIER="rounakbanik/the-movies-dataset"
```

The IMDB example mentioned in the sample env is available as a sample dataset to choose. Feel free to configure the
dataset that you would like to.

**Step 4: Introspect the Kaggle Connector**

```ddn
ddn connector introspect kaggle --log-level=DEBUG
```

**Note**: Depending on how big the dataset is, it should take sometime to fully import the data. The schema will be
initialized quickly and the data import happens in the background, so you can proceed to follow the steps below in a
different terminal window.

The above command runs in DEBUG mode to make it easy to catch errors for invalid files.

**Step 5: Add Models**

Based on the dataset imported, a SQL schema would be generated. Let's track all the models to get started quickly.

```ddn
ddn model add kaggle "*"
```

## Build your PromptQL app

Now, let's set up the Hasura DDN project with PromptQL to start exploring the data in natural language!

- Set up the Hasura DDN project already scaffolded in the repo:

In the root directory of the repo, run the following commands:

```ddn
ddn supergraph build local
ddn project init --with-promptql
```

- Start the DDN project

Let's start the DDN project by executing the following command:

```ddn
ddn run docker-start
```

- Open the local DDN Console to start exploring:

```ddn
ddn console --local
```

This should open up your browser (or print a browser URL) for displaying the Hasura Console. It’ll typically be
something like:
[https://promptql.console.hasura.io/local?engine=localhost:3280&promptql=localhost:3282](https://promptql.console.hasura.io/local?engine=localhost:3280&promptql=localhost:3282)

## Ask questions about your dataset

The app will have metadata about the dataset that you just imported above. You should be able to ask domain specific
questions and play around with the data.

Here's a sample of what you can ask to get started.

- Hi, what can you do?

Depending on the dataset schema, PromptQL will tell you what it can answer and you can go from there.

## Clean up and restart your app

If you want to reset the data and start from scratch:

You can stop the `ddn run docker-start` command, whereever it is running and you can execute the following in the root
directory of the repo:

```bash copy
docker compose down -v && ddn run docker-start
```



==============================



# DuckDuckGo Web Search

URL: https://hasura.io/docs/promptql/recipes/tutorials/duckduckgo-web-search


# AI Assistant for DuckDuckGo Web Search with Agentic Query Planning

Supports web search in realtime to query DuckDuckGo and return a list of search results that PromptQL can operate on.

- Query for keywords (including multiple at the same time)
- Can return a maximum of 20 results for each keyword.
- Uses the "us-en" region as default, but can be changed.

<Thumbnail src="/img/get-started/duckduckgo-search.jpeg" alt="AI Assistant for DuckDuckGo Search" />

In this guide, you will learn how to setup DuckDuckGo web search functionality and connect it to PromptQL to be able to
query using natural language and perform programmatic analysis or computation on search results.

Check out the [GitHub repo](https://github.com/hasura/duckduckgo-search).


## Prerequisites

**Install the DDN CLI**

:::info Minimum version requirements

To use this guide, ensure you've installed/updated your CLI to at least `v2.28.0`.

:::


<Tabs groupId="os-preference" className="api-tabs">

<TabItem value="macOS-or-linux" label="macOS and Linux">

Simply run the installer script in your terminal:

<CodeBlock language="bash">
  {`curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/get.sh | bash`}
</CodeBlock>

<Admonition type="info" title="ARM-based Linux Machines">
  Currently, the CLI does not support installation on ARM-based Linux systems.
</Admonition>

</TabItem>

<TabItem value="windows" label="Windows">
- Download the latest <a href={`https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/latest/DDN_CLI_Setup.exe`}>DDN CLI installer for Windows.</a> 
- Run the `DDN_CLI_Setup.exe` installer file and follow the instructions. This will only take a minute.
- By default, the DDN CLI is installed under `C:\Users\{Username}\AppData\Local\Programs\DDN_CLI`
- The DDN CLI is added to your `%PATH%` environment variable so that you can use the `ddn` command from your terminal.
<br />

</TabItem>
</Tabs>


**Install [Docker](https://docs.docker.com/engine/install/)**

The Docker-based workflow helps you iterate and develop locally without deploying any changes to Hasura DDN, making the
development experience faster and your feedback loops shorter. **You'll need Docker Compose `v2.20` or later.**

**Validate the installation**

You can verify that the DDN CLI is installed correctly by running:

```ddn
ddn doctor
```


## Setup DuckDuckGo Search Connector

**Step 1: Clone the project**

```bash copy
git clone git@github.com:hasura/duckduckgo-search.git
cd duckduckgo-search
```

**Step 2: Introspect the DuckDuckGo Connector**

```ddn
ddn connector introspect duckduckgo --log-level=DEBUG
```

The above command runs in DEBUG mode to make it easy to catch errors in case there are any issues.

**Step 3: Add Commands**

Let's track all the commands that enable web search on DuckDuckGo to get started quickly.

```ddn
ddn model add duckduckgo "*"
```

## Build your PromptQL app

Now, let's set up the Hasura DDN project with PromptQL to start exploring the data in natural language!

- Set up the Hasura DDN project already scaffolded in the repo:

In the root directory of the repo, run the following commands:

```ddn
ddn supergraph build local
ddn project init
```

- Start the DDN project

Let's start the DDN project by executing the following command:

```ddn
ddn run docker-start
```

- Open the local DDN Console to start exploring:

```ddn
ddn console --local
```

This should open up your browser (or print a browser URL) for displaying the Hasura Console. It’ll typically be
something like:
[https://promptql.console.hasura.io/local?engine=localhost:3280&promptql=localhost:3282](https://promptql.console.hasura.io/local?engine=localhost:3280&promptql=localhost:3282)

## Ask questions with access to DuckDuckGo search

The app will have metadata about the web search functionality of DuckDuckGo.

Here's a sample of what you can ask to get started.

- Hi, what can you do? (PromptQL will tell you what it can answer and you can go from there)
- give me just the URLs and title tags for the top 20 results ranking for the keyword "hasura".
- summarize the top 3 results for "promptql"
- for "promptql", classify the results as informational or product intent.



==============================



# Add Vector Search to PostgreSQL

URL: https://hasura.io/docs/promptql/recipes/tutorials/add-vector-search-to-postgresql

# Add Vector Search to PostgreSQL

## Introduction

This tutorial shows you how to connect your own vector data and retrieval functions to PromptQL.

We will use OpenAI models to create embeddings. To continue this tutorial, you'll also need an OpenAI API key.

For Vector Search functionality, we need two things

- Logic to Vectorize unstructured data.
  - Fetch data from the database, create embeddings using OpenAI's embedding model.
  - Insert the embedding into the database as a vector column, managed by `pg_vector` extension.
- Function to perform Semantic Search retrieval.
  - Create an embedding for the input search query.
  - Perform a similarity search query, backed by `pg_vector` with the embedding and return the matched results.

:::info You can use any embedding model

Depending on the use case, feel free to use any text embedding model from OpenAI or from other providers like Cohere for
example. In this example, we are going ahead with OpenAI's embedding model `text-embedding-3-large`.

:::

Alternatively, if you already have a retrieval function for your vector database set up, follow the steps below and
replace the env vars and code snippets with your own as appropriate.

We will add a Python connector to add the Vector Search functionality.

## Tutorial

### Step 1. Initialize a new connector

```ddn
ddn connector init mypython -i
```

- Select **hasura/python**
- Skip port, etc. Press “return”.

### Step 2. Add your OpenAI API key

We'll be using the OpenAI API key to create embeddings for our unstructured data.

In your project root directory, add the key to your `.env` file.

```sh
echo APP_MYPYTHON_OPENAI_API_KEY='sk-proj...' >> .env
```

### Step 3. Add your PostgreSQL connection string

We'll be using the database connection string in the Python code to connect.

In your project directory, add the PG connection string to your `.env` file.

```sh
# Replace postgres:/.... your database connection string
echo APP_DATABASE_URL=postgres://username:password@host:port/database >> .env
```

### Step 4. Make these environment variables available to your connector

- In `mysupergraph/app/connector/mypython/connector.yaml` under `definition: envMapping:`, add:

```yaml copy
DATABASE_URL:
  fromEnv: APP_DATABASE_URL
OPENAI_API_KEY:
  fromEnv: APP_MYPYTHON_OPENAI_API_KEY
```

### Step 5. Write custom functions

In `mysupergraph/app/connector/mypython/functions.py`, replace the boilerplate code with your custom functions to
vectorize product reviews and perform semantic search.

<details>
<summary>Click to show Python code</summary>

```python copy
from hasura_ndc import start
from hasura_ndc.function_connector import FunctionConnector
from pydantic import BaseModel
from typing import List, Optional

connector = FunctionConnector()

class ReviewRow(BaseModel):
    reviewId: int

@connector.register_query
async def semanticSearchReviews(text: str, limit: Optional[int] = None, offset: Optional[int] = None) -> List[ReviewRow]:
    openai_api_key = os.environ.get("OPENAI_API_KEY")
    pg_connection_uri = os.environ.get("DATABASE_URL")

    try:
        # Generate embedding for the search text
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {openai_api_key}",
        }
        payload = {
            "input": text,
            "model": "text-embedding-3-large",
        }

        async with aiohttp.ClientSession() as session:
            async with session.post("https://api.openai.com/v1/embeddings", headers=headers, json=payload) as response:
                embeddingData = await response.json()

        embedding = embeddingData['data'][0]['embedding']
        formattedEmbedding = '[' + ','.join(map(str, embedding)) + ']'

        # Connect to the database
        conn = await asyncpg.connect(pg_connection_uri)

        # Base query to find reviews with similar embeddings
        searchQuery = """
            SELECT
                review_id,
                1 - (embedding <=> $1::vector) as similarity
            FROM Reviews
            WHERE embedding IS NOT NULL
            ORDER BY embedding <=> $1::vector
        """

        if limit is not None:
            searchQuery += f" LIMIT {limit}"
            if offset is not None:
                searchQuery += f" OFFSET {offset}"
        else:
            searchQuery += " LIMIT 20"

        queryParams = [formattedEmbedding]

        results = await conn.fetch(searchQuery, *queryParams)

        # Map the results to the expected ReviewRow interface
        reviewRows = [ReviewRow(reviewId=row['review_id']) for row in results]

        await conn.close()

        return reviewRows
    except Exception as e:
        print(f"Error performing semantic search: {e}")
        return []

@connector.register_mutation
async def vectorize() -> str:
    openai_api_key = os.environ.get("OPENAI_API_KEY")
    pg_connection_uri = os.environ.get("DATABASE_URL")

    try:
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {openai_api_key}",
        }

        conn = await asyncpg.connect(pg_connection_uri)

        # Get all reviews that don't have embeddings yet
        getReviewsQuery = """
            SELECT review_id, comment
            FROM Reviews
            WHERE embedding IS NULL AND comment IS NOT NULL
        """
        reviews = await conn.fetch(getReviewsQuery)

        # Process reviews in batches to avoid rate limits
        batchSize = 100
        for i in range(0, len(reviews), batchSize):
            batch = reviews[i:i+batchSize]

            async def get_embedding_for_review(review):
                payload = {
                    "input": review['comment'],
                    "model": "text-embedding-3-large",
                }
                async with aiohttp.ClientSession() as session:
                    async with session.post("https://api.openai.com/v1/embeddings", headers=headers, json=payload) as response:
                        embeddingData = await response.json()
                embedding = embeddingData['data'][0]['embedding']
                return {
                    'review_id': review['review_id'],
                    'embedding': embedding
                }

            tasks = [get_embedding_for_review(review) for review in batch]
            embeddings = await asyncio.gather(*tasks)

            # Update reviews with their embeddings
            updateQuery = """
                UPDATE Reviews
                SET embedding = $1::vector
                WHERE review_id = $2
            """
            for item in embeddings:
                formattedEmbedding = '[' + ','.join(map(str, item['embedding'])) + ']'
                await conn.execute(updateQuery, formattedEmbedding, item['review_id'])

            # Log progress
            print(f"Processed {min(i + batchSize, len(reviews))} out of {len(reviews)} reviews")

        await conn.close()

        return "SUCCESS"
    except Exception as e:
        print(f"Error vectorizing reviews: {e}")
        raise e

if __name__ == "__main__":
    start(connector)
```

</details>

### Step 6. Add required libraries

In `mysupergraph/app/connector/mypython/requirements.txt`, add:

```plaintext
aiohttp==3.10.10
asyncpg==0.30.0
```

### Step 7. Introspect your connector

```ddn title="Make sure Docker is running, then execute:"
ddn connector introspect mypython
```

### Step 8. Add your resources

```ddn title="Create metadata for the commands in your supergraph:"
ddn command add mypython '*'
```

### Step 9. Build your supergraph

```ddn title="Create your supergraph build locally:"
ddn supergraph build local
```

### Step 10. Start your supergraph locally

```ddn
ddn run docker-start
```

### Step 11. Head to your local DDN console

```ddn title="Run the following from your project's directory:"
ddn console --local
```

### Step 12. Vectorize your data

A GraphQL API is exposed by your project as well. You can run the following command to invoke a mutation which will
vectorize your data. **Or, if your data is already vectorized, skip to the next step!**

```sh
curl -X POST http://localhost:3280/graphql \
  -H "Content-Type: application/json" \
  -d '{"query": "mutation VectorizeReviews { vectorize }"}'

```

```json title="You should see a response like this:"
{
  "data": {
    "vectorize": "SUCCESS"
  }
}
```

### Step 13. Talk to your data

Now, you can ask semantic questions on your data:

```text
Which review would be best for a customer testimonial?
Which customer feedback best highlights our product’s unique value?
Which client comment would be most persuasive for a case study?
```



==============================



# Add JSONB Support for Snowflake

URL: https://hasura.io/docs/promptql/recipes/tutorials/add-jsonb-support-snowflake

# Add JSONB Support for Snowflake

## Introduction

This guide shows you how to enable JSONB support in PromptQL for Snowflake data sources using native queries in Hasura
DDN. It should be used as a template and modified to fit your own schema.

For JSONB support, we need two things:

- A native query — which is your own custom SQL — to retrieve the relevant information.
- Descriptions added to the metadata to augment PromptQL's knowledge of the data source.

## Tutorial

### Step 1. Create a new project with the Snowflake connector enabled

If you haven't done so already, complete the steps in our
[guide for connecting Snowflake to PromptQL](/how-to-build-with-promptql/with-snowflake.mdx).

### Step 2. Create a native query

Native queries enable you to write your own SQL and expose it as a root-level field in your GraphQL API. This field will
then also be available to PromptQL for retrieving data from your Snowflake instance. We can use the existing JSON
operator built into Snowflake to transform a column's value into structured JSON.

As an example, imagine we have a table called `USERS` in an database called `ECOMM`; if the `user_details` are stored as
a string — but are in fact JSON — we can use a native query to return the structured output to our API:

```sql
--- This will return the user_id and email in their existing format, while transforming the user_details to JSON:
select user_id, email, TO_JSON(user_details) as user_details, created_at from PUBLIC.ECOMM.USERS
```

We can then include this SQL in a native query by adding it to the `nativeQueries` object of our `configuration.json`
found in our connector's directory:

```sql copy
  "nativeQueries": {
  "ECOMM_USERS": {
    "sql": {
      "parts": [
        {
          "type": "text",
          "value": "select user_id, email, TO_JSON(user_details) as user_details, created_at from PUBLIC.ECOMM.USERS"
        }
      ]
    },
    "columns": {
      "USER_DETAILS": {
        "type": "named",
        "name": "STRING"
      },
      "USER_ID": {
        "type": "named",
        "name": "BIGINTEGER"
      },
      "CREATED_AT": {
        "type": "named",
        "name": "TIMESTAMP"
      },
      "EMAIL": {
        "type": "named",
        "name": "STRING"
      }
    },
    "arguments": {},
    "description": null,
    "isProcedure": false
  }
}
```

The **key** of the new object is the name which will be exposed as a top-level field in your API. After adding your
custom SQL, include the returned columns along with their types.

<details>
<summary>Click to see an explanation of the fields</summary>
<br />

`nativeQueries`

**Description:** A top-level object containing all native query definitions.  
**Structure:** Each key represents the name of a native query, and the value defines its configuration.

**Query Name (e.g., ECOMM_USERS)**  
**Description:** A unique identifier for the native query. This name is used to reference the query in the application.

`sql`

**Description:** Specifies the SQL query for the native query.  
**Structure:** Contains a `parts` array where each part has:

- **type:** Indicates the type of part. Currently, only `text` is supported.
- **value:** The actual SQL query as a string.

`columns`

**Description:** Maps the output columns of the SQL query to their types.  
**Structure:** An object where each key represents a column name and the value specifies its type.

- **type:** Defines how the column type is specified. Typically `named`.
- **name:** The name of the data type. The data type must match the types supplied by the connector in the data
  connector link configuration.

`arguments`

**Description:** Defines any arguments required by the query. Used for parameterized queries.  
**Structure:** An object where each key is the argument name and the value specifies its type and properties. If no
arguments are needed, this is an empty object.

`description`

**Description:** An optional field for adding a human-readable description of the query.  
**Structure:** A string value or `null` if no description is provided.

`isProcedure`

**Description:** Indicates whether the native query represents a mutation or a query.  
**Structure:** A boolean value (`true` for mutations and `false` for queries).

</details>

### Step 3. Re-introspect your connector

As you've made a new field available in your connector's configuration, you'll need to re-introspect your connector:

```ddn
ddn connector introspect <connector-name>
```

### Step 4. Add the model

Then, add the new model **using the name you provided in your native query's key**:

```ddn
# E.g., ddn model add my_snowflake "ECOMM_USERS"
ddn model add <connector-name> "<model-name>"
```

This will generate an HML file, which you'll use in the next step, representing your model.

### Step 5. Augment the metadata description

The more context afforded to PromptQL, the better it understands your data and the ways it can interact with it. If we
add a description to the metadata object, PromptQL will infer that structured JSON will be returned.

For example:

```yaml {11-15}
kind: ObjectType
version: v1
definition:
  name: EcommUsers
  fields:
    - name: createdAt
      type: Timestamp!
    - name: email
      type: String_1!
    - name: userDetails
      type: String_1!
      description:
        "This is stringified JSON it has the following fields: address, shipping_state, phone, preferences is an object
        which has newsletter and theme {address: '123 Main St', shipping_state: 'CA', phone: '555-555-5555',
        preferences: {newsletter: true, theme: 'dark'}}"
    - name: userId
      type: Biginteger!
  graphql:
    typeName: EcommUsers
    inputTypeName: EcommUsersInput
  dataConnectorTypeMapping:
    - dataConnectorName: snow
      dataConnectorObjectType: ECOMM_USERS
      fieldMapping:
        createdAt:
          column:
            name: CREATED_AT
        email:
          column:
            name: EMAIL
        userDetails:
          column:
            name: USER_DETAILS
        userId:
          column:
            name: USER_ID
```

Here, we're not only explaining that this is stringified JSON, but also an example of what can be expected to be
returned.

### Step 6. Create a new build and start your services

```ddn title="First, create a new build:"
ddn supergraph build local
```

```ddn title="Then, start your services:"
ddn run docker-start
```

```ddn title="And open the development console:"
ddn console --local
```

### Step 7. Talk to your data

Ask any questions you wish about your Snowflake data...including those that utilize your structured JSON directly from
Snowflake!



==============================



# Add Support for Weaviate

URL: https://hasura.io/docs/promptql/recipes/tutorials/weaviate

# Add Support for Weaviate

## Introduction

This tutorial shows you how to connect your Weaviate cluster and perform semantic search with PromptQL.

## Prerequisites

In this example, we'll use a Weaviate Cluster on Cloud.

1. Create a cluster on Weaviate
2. You will need the Weaviate REST API URL and API Key, both available on Weaviate Cloud.
3. You'll also need a Cohere API key for the embeddings.

## Tutorial

### Step 1. Clone the project

```bash
git clone git@github.com:hasura/weaviate-promptql-quickstart.git
cd promptql-weaviate-quickstart
```

### Step 2. Add your API keys for Weaviate and Cohere

We'll be using the Cohere API key to create embeddings for our unstructured data via Weaviate Client. We also need the
Weaviate Cloud URL and Weaviate API Key. Both can be obtained from Weaviate Cloud.

In your project directory, add the key to your `.env` file.

First, copy the `env.sample` file to `.env`

```sh
cp env.sample .env
```

```sh title="Now, update the API keys:"
APP_PYTHON_COHERE_API_KEY='....'
APP_PYTHON_WCD_URL='....'
APP_PYTHON_WCD_API_KEY='....'
```

:::info Embeddings are up to you

Feel free to modify the embedding configuration in Weaviate Client to use something other than Cohere and configure the
API key accordingly.

:::

### Step 3. Create a collection and load data

After configuring the `.env` values with the above credentials, continue with the data loading into Weaviate.

```sh title="Head to app/connector/python and execute:"
python3 load-data.py
```

This will create a movie collection and load sample movie data with vector embedding using Cohere.

### Step 4. Write custom functions

In `app/connector/python/functions.py`, we already have two functions:

- `semantic_search`
- `keyword_search`

They are exposed as functions to PromptQL. Add more per your requirements.

<details>
<summary>Click to show Python code</summary>

```python copy
"""
functions.py

This is an example of how you can use the Python SDK's built-in Function connector to easily write Python code.
When you add a Python Lambda connector to your Hasura project, this file is generated for you!

In this file you'll find code examples that will help you get up to speed with the usage of the Hasura lambda connector.
If you are an old pro and already know what is going on you can get rid of these example functions and start writing your own code.
"""
from weaviate.classes.init import Auth

from hasura_ndc import start
from hasura_ndc.function_connector import FunctionConnector
from pydantic import BaseModel, Field # You only need this import if you plan to have complex inputs/outputs, which function similar to how frameworks like FastAPI do
from hasura_ndc.errors import UnprocessableContent
from typing import Annotated

# Weaviate Environment Variables
wcd_url = os.environ["WCD_URL"]
wcd_api_key = os.environ["WCD_API_KEY"]
cohere_api_key = os.environ["COHERE_API_KEY"]

client = weaviate.connect_to_weaviate_cloud(
    cluster_url=wcd_url,                                    # Replace with your Weaviate Cloud URL
    auth_credentials=Auth.api_key(wcd_api_key),             # Replace with your Weaviate Cloud key
    headers={"X-Cohere-Api-Key": cohere_api_key}
)

connector = FunctionConnector()

class Movies(BaseModel):
  title: str

# Semantic search to find similar entities in weaviate
@connector.register_query # This is how you register a query
def semantic_search(query: str) -> list[Movies]:
    # do near by search with weaviate python client
    # Get the collection
    movies = client.collections.get("Movie")

    # Perform query
    response = movies.query.near_text(
        query=query, limit=5, return_metadata=wq.MetadataQuery(distance=True)
    )

    movies_list = []
    # Inspect the response
    for o in response.objects:
        print("XXXXXXX")
        print(o.properties)
        print(o.properties["release_date"])
        print(
            o.properties["title"]
        )  # Print the title and release year (note the release date is a datetime object)
        movies_list.append(Movies(title=o.properties["title"]))
        print(
            f"Distance to query: {o.metadata.distance:.3f}\n"
        )  # Print the distance of the object from the query

    return movies_list

# This is an example of a keyword search function which uses the BM25 algorithm to find the most relevant Movie entities in Weaviate
@connector.register_query # This is how you register a query
def keyword_search(query: str) -> list[Movies]:
    # Get the collection
    movies = client.collections.get("Movie")

    # Perform query
    response = movies.query.bm25(
        query="history", limit=5, return_metadata=wq.MetadataQuery(score=True)
    )

    movies_list = []
    # Inspect the response
    for o in response.objects:
        print(
            o.properties["title"]
        )  # Print the title and release year (note the release date is a datetime object)
        movies_list.append(Movies(title=o.properties["title"]))
        print(
            f"BM25 score: {o.metadata.score:.3f}\n"
        )  # Print the BM25 score of the object from the query
    return movies_list

if __name__ == "__main__":
    start(connector)

```

</details>

### Step 5. Build your supergraph

```ddn title="Create your supergraph build locally:"
ddn supergraph build local
```

```ddn title="Run the services:"
ddn run docker-start
```

### Step 6. Head to your local DDN console

```ddn title="Run the following from your project's directory:"
ddn console --local
```

### Step 7. Talk to your data

```text title="Now, you can ask semantic questions on your data:"
Movies with dystopian future
Historical movies
```

## Iterating on the Python Code and Logic

If you need to iterate on your code or schema, check out the [iteration guide](/data-modeling/iterate.mdx).



==============================



# index.mdx

URL: https://hasura.io/docs/promptql/auth/jwt/

# JWT Mode

In JWT mode, session variables are passed to the Hasura Engine on each request in JSON Web Tokens (JWTs).

## JWT mode setup

- [How to set up JWT mode](/auth/jwt/jwt-mode.mdx)
- [JWT configuration information](/auth/jwt/jwt-configuration.mdx)

## Integrations with third-party services

- [Auth0 JWT integration](/auth/jwt/tutorials/integrations/1-auth0.mdx)
- [AWS Cognito JWT integration](/auth/jwt/tutorials/integrations/2-aws-cognito.mdx)
- [Firebase JWT integration](/auth/jwt/tutorials/integrations/3-firebase.mdx)
- [Clerk JWT integration](/auth/jwt/tutorials/integrations/4-clerk.mdx)



==============================



# jwt-mode.mdx

URL: https://hasura.io/docs/promptql/auth/jwt/jwt-mode


# JWT Mode

## Introduction

JWT mode requires that the client making the request sends a valid JSON Web Token to application. This JWT is provided
by an auth service such as Auth0, AWS Cognito, Firebase, Clerk, or your own custom solution.

Hasura then verifies and decodes the JWT to extract `x-hasura-*` session variable claim values from a defined namespace
in the token.

The `x-hasura-default-role` and `x-hasura-allowed-roles` session variables are required, and you will also most likely
utilize the user id and any other information which you need to determine access to your data.

The token can be passed in the header of the request in a dedicated key, or using the `Authorization` header with the
`Bearer` prefix, or as a cookie. All these options are defined in the `AuthConfig` object in your metadata.

<Thumbnail src="/img/auth/auth-jwt-overview-diagram.png" alt="Authentication using JWT" />

## Session variable requirements

Session variables passed via JWT or webhook can contain any information you want, but must at least contain an
`x-hasura-default-role` property and `x-hasura-allowed-roles` array.

An `x-hasura-role` value can optionally be sent as a plain header in the request to indicate the role which should be
used. If this is not provided, the engine will use the `x-hasura-default-role` value in the JWT.

To clarify, the `x-hasura-role` header is optional and can be used to override the default role in the JWT allowing the
same verified JWT to be used for different roles.

Only keys prefixed with `x-hasura-` will be accessible by the application.

Session variable keys are case-insensitive. Values are case-sensitive.

## Enabling JWT authentication

You can enable your application to use JWTs in just a few steps.

### Step 1. Update your AuthConfig

Hasura utilizes an [AuthConfig](/reference/metadata-reference/auth-config.mdx) object that allows you to define the
configuration for your authentication service. In a standard setup the `auth-config.hml` file can be found in your
`globals` directory.

:::tip Hasura VS Code extension

You can use [Hasura's VS Code extension](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura) to
scaffold out your `AuthConfig` object by typing `AuthConfig` and selecting this object from the list of available
options. As you navigate through the skeleton, you can type `CTRL+SPACEBAR` at any point to reveal options for the
different key-value pairs.

:::

Below, we're showing using the `BearerAuthorization` header location format using a fixed secret key from an environment
variable. However, we support other methods for
[where the application can locate the JWT](reference/metadata-reference/auth-config.mdx#authconfig-jwttokenlocation) and
[how it is verified](reference/metadata-reference/auth-config.mdx#authconfig-jwtalgorithm).

```yaml title="globals/metadata/auth-config.hml"
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        fixed:
          algorithm: HS256
          key:
            valueFromEnv: AUTH_SECRET
```

Read more about other setup options [here](/reference/metadata-reference/auth-config.mdx#authconfig-jwtconfig).

### Step 2. Define the JWT with custom claims

Your auth service should include an object with a key of `claims.jwt.hasura.io` in the JWT. Within this, each claim
should be prefixed with `x-hasura-*` and include the relevant information. Note that an extra optional `x-hasura-role`
**header** can be passed to override the default role found in the JWT's custom claims.

| Key                      | Required | Value                                                                                                          |
| ------------------------ | -------- | -------------------------------------------------------------------------------------------------------------- |
| `x-hasura-default-role`  | Yes      | The role that will be used when the optional x-hasura-role header is not passed                                |
| `x-hasura-allowed-roles` | Yes      | A list of allowed roles for the user making the request.                                                       |
| `x-hasura-[custom]`      | No       | Where `[custom]` is any string you wish (e.g., `org`, `user-id`, `customer`). The value can be any JSON value. |

In the simple example below, we're including the required claims by stating the default role is `admin` and the list of
available roles is limited to `user` and `admin`. Additionally, we're passing a custom key of `x-hasura-user-id` which
can be used with [permissions](/reference/metadata-reference/permissions.mdx) when executing queries.
[Read more about the default claims here](/auth/jwt/jwt-configuration.mdx#hasura-jwt-format).

```json title="Example JWT payload"
{
  "iat": 1735916718,
  "exp": 1796916677,
  "claims.jwt.hasura.io": {
    "x-hasura-default-role": "admin",
    "x-hasura-allowed-roles": ["user", "admin"],
    "x-hasura-user-id": 1234
  }
}
```

Your auth service will encode this object using a secret and create a token which can then be passed to the application.
You can see an example of the above token encoded
[here](https://jwt.io/#debugger-io?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpYXQiOjE3MzU5MTY3MTgsImV4cCI6MTc5NjkxNjY3NywiY2xhaW1zLmp3dC5oYXN1cmEuaW8iOnsieC1oYXN1cmEtZGVmYXVsdC1yb2xlIjoiYWRtaW4iLCJ4LWhhc3VyYS1hbGxvd2VkLXJvbGVzIjpbInVzZXIiLCJhZG1pbiJdLCJ4LWhhc3VyYS11c2VyLWlkIjoxMjM0fX0.1Dv6B077loe9wGcUFwK_JAHRUtLOZHOEp9EE-OehUzY).
The signature secret to verify this token with the HS256 algorithm is `ultra-secret-very-secret-super-secret-key`.

:::warning Setting audience check

Certain JWT providers (like Firebase) share JWKs between multiple tenants. They use the `aud` claim of JWT to specify
the intended tenant for the JWT. Setting the `audience` field in the Hasura JWT configuration will make sure that the
`aud` claim from the JWT is also checked during verification. Not doing this check will allow JWTs issued for other
tenants to be valid as well.

In these cases, you **MUST** set the `audience` field to appropriate value. **Failing to do so is a major security
vulnerability**. Learn how to set this [here](reference/metadata-reference/auth-config.mdx#authconfig-jwtconfig).

If your [Compatibility Config date](/reference/metadata-reference/compatibility-config.mdx) is set to `2025-03-11` or
newer, you _must_ set the `audience` field if your JWTs will contain an `aud` claim. If you do not, the JWT will be
rejected as invalid and authentication will fail.

:::

### Step 3. Add permissions to an object in your supergraph

Let's add some example `TypePermissions` so that an admin role can access all fields in the Orders type, but we restrict
a user role from accessing the `deliveryDate` field.

```bash title="Example TypePermissions for Orders type"
---
kind: TypePermissions
version: v1
definition:
  typeName: Orders
  permissions:
    - role: admin
      output:
        allowedFields:
          - createdAt
          - deliveryDate
          - id
          - isReviewed
          - productId
          - status
          - updatedAt
          - userId
  # highlight-start
    - role: user
      output:
        allowedFields:
          - createdAt
          - id
          - isReviewed
          - productId
          - status
          - updatedAt
          - userId
  # highlight-end
```

Let's also add some example `ModelPermissions` so that an admin role can access all rows in the Orders model, but a user
role can only access rows where the userId field matches the user id session variable in the JWT.

```bash title="Example ModelPermissions for Orders model"
---
kind: ModelPermissions
version: v1
definition:
  modelName: Orders
  permissions:
    - role: admin
      select:
        filter: null
        allowSubscriptions: true
# highlight-start
    - role: user
      select:
        filter:
          fieldComparison:
            field: userId
            operator: _eq
            value:
              sessionVariable: x-hasura-user-id
# highlight-end
```

:::info Example JWT payload

For these examples we'll set the payload of the JWT to specify a `user` role and a UUID for the user id.

```json title="Example JWT payload which we will send to the application"
{
  "iat": 1735916718,
  "exp": 1796916677,
  "claims.jwt.hasura.io": {
    "x-hasura-default-role": "user",
    "x-hasura-allowed-roles": ["user"],
    "x-hasura-user-id": "7cf0a66c-65b7-11ed-b904-fb49f034fbbb"
  }
}
```

:::

### Step 4. Rebuild your application

Once you've updated your `AuthConfig` object in `auth-config.hml` and updated your claims, you can rebuild your
application and test it locally.

```ddn title="For example, from the root of your project, run:"
ddn supergraph build local
```

### Step 5. Make an authenticated request

In the example above, we're using the `BearerAuthorization` method. As such, as we can make a request to our Hasura DDN
instance by including a header with the key-value of `Authorization: Bearer <our-encoded-token>`. For testing, you can
pass this value in via the [PromptQL Playground client](/auth/playground-auth.mdx).

## Next steps

If you're looking for step-by-step help to get started with common authentication providers, check
[this section](/auth/jwt/tutorials/index.mdx) of tutorials.



==============================



# jwt-configuration.mdx

URL: https://hasura.io/docs/promptql/auth/jwt/jwt-configuration

# JWT Configuration

This section describes the JSON Web Token (JWT) configuration options available.

## Payload Definition

Example JSON Web Token (JWT) payload configuration definition:

```yaml title="globals/metadata/auth-config.hml"
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        fixed:
          algorithm: HS256
          key:
            value: ultra-secret-very-secret-super-secret-key
      audience: ["myapp-1234", "myapp-6789"]
      allowedSkew: 60
      issuer: https://my-auth-server.com
```

As a minimum, either the `claimsConfig`, `tokenLocation`, **and** `key` values **have to be present**.

## Example Decoded Payload

```json
{
  "iat": 1735916718,
  "exp": 1796916677,
  "claims.jwt.hasura.io": {
    "x-hasura-default-role": "user",
    "x-hasura-allowed-roles": ["user", "admin"],
    "x-hasura-user-id": "123",
    "x-hasura-org-id": "456",
    "x-hasura-custom": "custom-value"
  }
}
```

## Example Encoded JWT

```text
eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpYXQiOjE3MzU5MTY3MTgsImV4cCI6MTc5NjkxNjY3NywiY2xhaW1zLmp3dC5oYXN1cmEuaW8iOnsieC1oYXN1cmEtZGVmYXVsdC1yb2xlIjoidXNlciIsIngtaGFzdXJhLWFsbG93ZWQtcm9sZXMiOlsidXNlciIsImFkbWluIl0sIngtaGFzdXJhLXVzZXItaWQiOiIxMjMiLCJ4LWhhc3VyYS1vcmctaWQiOiI0NTYiLCJ4LWhhc3VyYS1jdXN0b20iOiJjdXN0b20tdmFsdWUifX0.5bwSMgxsyULY1uhCJxYd-sO35rCdznRCZ4YMLwDD5u8
```

**Note:** `x-hasura-default-role` and `x-hasura-allowed-roles` are mandatory, while the rest of the claims are optional.

[See here for the JWT debugger](https://jwt.io/#debugger-io?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpYXQiOjE3MzU5MTY3MTgsImV4cCI6MTc5NjkxNjY3NywiY2xhaW1zLmp3dC5oYXN1cmEuaW8iOnsieC1oYXN1cmEtZGVmYXVsdC1yb2xlIjoidXNlciIsIngtaGFzdXJhLWFsbG93ZWQtcm9sZXMiOlsidXNlciIsImFkbWluIl0sIngtaGFzdXJhLXVzZXItaWQiOiIxMjMiLCJ4LWhhc3VyYS1vcmctaWQiOiI0NTYiLCJ4LWhhc3VyYS1jdXN0b20iOiJjdXN0b20tdmFsdWUifX0.5bwSMgxsyULY1uhCJxYd-sO35rCdznRCZ4YMLwDD5u8)
of this example JWT token. The signature secret to verify this token with the HS256 algorithm is
`ultra-secret-very-secret-super-secret-key`.

## Hasura JWT format

The `x-hasura-role` value can be sent as a plain **header** in the request to indicate the role which should be used.

When your auth server generates the JWT, the custom claims in the JWT **must contain the following** in a custom
namespace. This namespace can be any string you choose, as long as it matches the `namespace.location` defined in your
AuthConfig. Using `claims.jwt.hasura.io` will match our examples.

1.  A `x-hasura-default-role` field. The role that will be used when the optional `x-hasura-role` _header_ is **not
    passed**.
2.  A `x-hasura-allowed-roles` field. A list of allowed roles for the user i.e. acceptable values of the optional
    `x-hasura-role` _header_.
3.  Add any other optional `x-hasura-*` claim fields (required as per your defined permissions) to the custom namespace.

To summarize, `x-hasura-allowed-roles` session variable contains a list of all the roles that the user can assume and
the `x-hasura-role` header tells the Hasura Engine which role to use for the request, and if that is missing then the
`x-hasura-default-role` session variable will be used.

This setup makes it more convenient for a JWT to only need to be issued once with a list of allowed roles for the user,
and then allow the client to decide which of those roles to actually use for a request. This prevents the user needing
to log in again or unnecessary JWT re-issuance.

If, for example, your app will not need to switch user roles and the user only needs one role, for instance: `user`, you
can just issue a JWT with `x-hasura-default-role` set to `user` and `x-hasura-allowed-roles` set to `["user"]` and not
send the `x-hasura-role` header in the request.

This setup is designed so that there is one authoritative way to construct your JWT token for the Hasura Engine which
can cover a wide range of use cases.

## Hasura JWT Claim Description

### x-hasura-default-role

The `x-hasura-default-role` will be the role that the user falls back to when no `x-hasura-role` value is specified in
the header of the request. Usually, this will be the role with the least privileges and can be overridden by the
`x-hasura-role` header when making a request.

### x-hasura-allowed-roles

The `x-hasura-allowed-roles` list can contain all the roles which a particular user can assume, eg:
`[ "user", "manager", "owner" ]`. Usually, these will have varying degrees of access to your data as specified in
Permissions and by specifying this list it lets the Hasura Engine know that this user can assume any of them.

### x-hasura-\*

The JWT can have other user-defined `x-hasura-*` fields and their values can only be strings (they will be converted to
the right type automatically). You can use these `x-hasura-*` values in your permission rules.

The JWT will normally also contain standard (`sub`, `iat` etc.) and custom (`name`, `admin` etc.) claims depending on
your auth provider.

## JWT Notes

- JWT claim fields eg: `x-hasura-default-role` are case-insensitive.
- Hasura Engine only has access to headers and JWT claims which are prefixed with `x-hasura-`.
- Hasura Engine only has access to JWT claims in namespace defined in the `AuthConfig` object in metadata.
- All `x-hasura-*` values should be of type `String`, they will be converted to the right type automatically.

## Hasura JWT configuration options

### claimsConfig

You can specify where the engine should look for the claims within the decoded token either with one of `namespace` and
`locations` options.

#### namespace {#jwt-claims-config-namespace}

The `namespace` option is used when all of the Hasura claims are present in a single object within the decoded JWT. Our
example uses `claims.jwt.hasura.io` in the [Example Decoded Payload](#example-decoded-payload).

```yaml
claimsConfig:
  namespace:
    claimsFormat: Json
    location: /claims.jwt.hasura.io
```

The `location` field indicates the location of the namespace object that uses
[RFC 6901 JSON Pointer](https://datatracker.ietf.org/doc/html/rfc6901) string syntax.

The `claimsFormat` field indicates whether the Hasura-specific claims are a regular JSON object or a stringified JSON.
The following possible values are allowed: `Json`, `StringifiedJson`.

This is required because providers like AWS Cognito only allow strings in the JWT claims.
[See #1176](https://github.com/hasura/graphql-engine/issues/1176).

**Example**:

If `claimsFormat` is `Json` then the JWT claims should look like:

```json
{
  "sub": "1234567890",
  "name": "John Doe",
  "admin": true,
  "iat": 1516239022,
  "claims.jwt.hasura.io": {
    "x-hasura-allowed-roles": ["editor", "user", "mod"],
    "x-hasura-default-role": "user",
    "x-hasura-user-id": "1234567890",
    "x-hasura-org-id": "123",
    "x-hasura-custom": "custom-value"
  }
}
```

If `claimsFormat` is `StringifiedJson` then the JWT claims should look like:

```json
{
  "sub": "1234567890",
  "name": "John Doe",
  "admin": true,
  "iat": 1516239022,
  "claims.jwt.hasura.io": "{\"x-hasura-allowed-roles\":[\"editor\",\"user\",\"mod\"],\"x-hasura-default-role\":\"user\",\"x-hasura-user-id\":\"1234567890\",\"x-hasura-org-id\":\"123\",\"x-hasura-custom\":\"custom-value\"}"
}
```

#### locations {#jwt-claims-config-locations}

This `locations` option can be used when Hasura claims are not all present in the single object, but individual claims
are provided a JSON pointer within the decoded JWT. In this option, you can indicate:

- a literal value.
- or a JSON pointer path for individual claims and an optional default value if the claim doesn't exist.

`x-hasura-default-role` and `x-hasura-allowed-roles` claims are required. Other custom claims are optionally configured.

The literal values should be of type `String`, except for the `x-hasura-allowed-roles` claim which expects a string
array.

**Example: JWT config with JSON path values**

```json
{
  "sub": "1234567890",
  "name": "John Doe",
  "admin": true,
  "iat": 1516239022,
  "user": {
    "id": "ujdh739kd"
  },
  "hasura": {
    "all_roles": ["user", "editor"]
  }
}
```

The mapping for `x-hasura-allowed-roles`, `x-hasura-default-role` and `x-hasura-user-id` session variables can be
specified in the `locations` configuration as follows:

```yaml
claimsConfig:
  locations:
    x-hasura-default-role:
      path:
        path: /hasura/all_roles/0
    x-hasura-allowed-roles:
      path:
        path: /hasura/all_roles
    x-hasura-user-id:
      path:
        path: /user/id
```

**Example: JWT config with JSON path values and default values**

```json
{
  "sub": "1234567890",
  "name": "John Doe",
  "admin": true,
  "iat": 1516239022,
  "hasura": {
    "all_roles": ["user", "editor"]
  }
}
```

```yaml
claimsConfig:
  locations:
    x-hasura-default-role:
      path:
        path: /hasura/all_roles/0
    x-hasura-allowed-roles:
      path:
        path: /hasura/all_roles
    x-hasura-user-id:
      path:
        path: /user/id
        default: ujdh739kd
```

In the above case, since the `/user/id` doesn't exist in the JWT token, the default value of the `x-hasura-user-id` i.e
`ujdh739kd` will be used

**Example: JWT config containing literal values**

```json
{
  "sub": "1234567890",
  "name": "John Doe",
  "admin": true,
  "iat": 1516239022,
  "user": {
    "id": "ujdh739kd"
  }
}
```

The corresponding JWT config should be:

```yaml
claimsConfig:
  locations:
    x-hasura-default-role:
      literal: user
    x-hasura-allowed-roles:
      literal: ["user", "editor"]
    x-hasura-user-id:
      path:
        path: /user/id
```

In the above example, the `x-hasura-allowed-roles` and `x-hasura-default-role` values are set in the JWT config and the
value of the `x-hasura-user-id` is a JSON path to the value in the JWT token.

### tokenLocation

Indicates the token location where request header to read the JWT from.

The following are the possible values:

#### BearerAuthorization

In this mode, Hasura expects an `Authorization` header with a `Bearer` token.

```yaml
tokenLocation:
  type: BearerAuthorization
```

The JWT header should look like:

```none
Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWI...
```

#### Cookie

In the cookie mode, Hasura will try to parse the cookie header with the given cookie name. The value of the cookie
should be the exact JWT.

```yaml
tokenLocation:
  type: Cookie
  name: cookie_name
```

The JWT header should look like:

```none
Cookie: cookie_name=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWI...
```

#### Header

In the custom header mode, Hasura expects a `header_name` header with the exact JWT token value.

```yaml
tokenLocation:
  type: Header
  name: header_name
```

The JWT header should look like:

```none
header_name: eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWI...
```

### key {#jwt-json-key}

This field specifies the JWT key configuration according to which the incoming JWT will be decoded. You can configure
either a `fixed` algorithm key or a remote JWK URL.

#### fixed {#jwt-json-key-fixed}

In this option, you must indicate a JWT key and its algorithm so the engine can decode and verify the JWT token.

```yaml
key:
  fixed:
    algorithm: HS256
    key:
      value: ultra-secret-very-secret-super-secret-key
      # valueFromEnv: AUTH_JWT_KEY
```

The `algorithm` field specifies the cryptographic signing algorithm which is used to sign the JWTs. Valid values are:
`HS256`, `HS384`, `HS512`, `RS256`, `RS384`, `RS512`, `ES256`, `ES384`, `PS256`, `PS384`, `PS512`, `EdDSA`.

The `key` field can be a literal value or an environment variable name.

- In the case of a symmetric key (i.e. a HMAC-based key), just the key as is. (e.g. -"abcdef..."). The key must be long
  enough for the chosen algorithm, (e.g. for HS256 it must be at least 32 characters long).
- In the case of an asymmetric key (RSA, EdDSA, ECDSA etc.), only the **public** key, in a PEM-encoded string or as an
  X509 certificate.

#### jwkFromUrl {#jwt-json-jwk_url}

An URL where a provider publishes their JWKs (JSON Web Keys - which are used for signing the JWTs). The URL **must**
publish the JWKs in the standard format as described [here](https://tools.ietf.org/html/rfc7517).

For example:

- Auth0 publishes their JWK url at: `https://<YOUR_AUTH0_DOMAIN>.auth0.com`.
- Firebase publishes their JWK url at:
  `https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com`.

```yaml
key:
  jwkFromUrl: https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com
```

The JWTs must be signed by the JWK published at the given URL. They can be signed by any algorithm that is compatible
with the key (eg. `RS256`, `RS384`, `RS512` algorithms require a JWK with an RSA key).

Hasura DDN does not currently support rotating JWKs.

### audience

This is an optional field. Certain providers might set a claim which indicates the intended audience for the JWT. This
must be checked by setting this field.

When this field is set, during the verification process of the JWT, the `aud` claim in the JWT will be checked to see
whether it is equal to the `audience` field given in the configuration. If they are not equal then the JWT will be
rejected.

See the [RFC](https://tools.ietf.org/html/rfc7519#section-4.1.3) for more details.

:::warning

If your [Compatibility Config date](/reference/metadata-reference/compatibility-config.mdx) is set to `2025-03-11` or
newer, you _must_ set the `audience` field if your JWTs will contain an `aud` claim. If you do not, the JWT will be
rejected as invalid and authentication will fail.

:::

This field must be a list of strings.

Examples:

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      # ...
      audience: ["myapp-1234", "myapp-6789"]
```

:::danger Audience Security Vulnerability

Certain JWT providers share JWKs between multiple tenants. They use the `aud` claim of the JWT to specify the intended
audience. Setting the `audience` field in the Hasura JWT configuration will make sure that the `aud` claim from the JWT
is also checked during verification. Not doing this check will allow JWTs issued for other tenants to be valid as well.

In these cases, you **MUST** set the `audience` field to the appropriate value. Failing to do so is a **major security
vulnerability**.

:::

### issuer

This is an optional field. It takes a string value.

When this field is set, during the verification process of the JWT, the `iss` claim in the JWT will be checked to see
whether it is equal to the `issuer` field given in the configuration. If they are not equal then the JWT will be
rejected.

See [RFC](https://tools.ietf.org/html/rfc7519#section-4.1.1) for more details.

Examples:

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      # ...
      issuer: https://my-auth-server.com
```

#### Issuer Notes

- Certain providers require you to verify the `iss` claim on the JWT. To do that you can set this field to the
  appropriate value.
- A JWT configuration without an issuer will match any issuer field present in an incoming JWT.
- An incoming JWT without an issuer specified will match a configuration even if it specifies an issuer.

### allowed_skew

`allowedSkew` is an optional field to provide some leeway (to account for clock skews) while comparing the JWT expiry
time. This field expects an integer value which will be the number of seconds of the skew value.

### Hasura JWT Config Examples

#### HMAC-SHA based

Your auth server is using HMAC-SHA algorithms to sign JWTs, and is using a 256-bit key. In this case, the JWT config
will look like:

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        fixed:
          algorithm: HS256
          key:
            value: 3EK6FD+o0+c7tzBNVfjpMkNDi2yARAAKzQlk8O2IKoxQu4nF7EdAh8s3TwpHwrdWT6R
```

The `key` is the actual shared secret, which is used by Hasura and the external auth server.

#### RSA based

If your auth server is using the RSA algorithm to sign JWTs, and is using a 512-bit key, the JWT config only needs to
have the public key.

**Example 1**: public key in PEM format (not OpenSSH format):

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        fixed:
          algorithm: RS512
          key:
            value:
              '-----BEGIN PUBLIC
              KEY-----\nMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDdlatRjRjogo3WojgGHFHYLugd\nUWAY9iR3fy4arWNA1KoS8kVw33cJibXr8bvwUAUparCwlvdbH6dvEOfou0/gCFQs\nHUfQrSDv+MuSUMAe8jzKE4qW+jK+xQU9a03GUnKHkkle+Q0pX/g6jXZ7r1/xAK5D\no2kQ+X5xK9cipRgEKwIDAQAB\n-----END
              PUBLIC KEY-----\n'
```

**Example 2**: public key as X509 certificate:

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        fixed:
          algorithm: RS512
          key:
            value:
              '-----BEGIN
              CERTIFICATE-----\nMIIDHDCCAgSgAwIBAgIINw9gva8BPPIwDQYJKoZIhvcNAQEFBQAwMTEvMC0GA1UE\nAxMmc2VjdXJldG9rZW4uc3lzdGVtLmdzZXJ2aWNlYWNjb3VudC5jb20wHhcNMTgQt7dIsMTIU9k1SUrFviZOGnmHWtIAw\nmtYBcM9I0f9/ka45JIRp5Y1NKpAMFSShs7Wv0m1JS1kXQHdJsPSmjmDKcwnBe3R/\nTU3foRRywR/3AJRM15FNjTqvUm7TeaW16LkkRoECAwEAAaM4MDYwDAYDVR0TAQH/\nBAIwADAOBgNVHQ8BAf8EBAMCB4AwFgYDVR0lAQH/BAwwCgYIKwYBBQUHAwIwDQYJ\nKoZIhvcNAQEFBQADggEBADfY2DEmc2gb8/pqMNWHYq/nTYfJPpK4VA9A0lFTNeoq\nzmnbGwhKj24X+Nw8trsvkrKxHvCI1alDgBaCyzjGGvgOrh8X0wLtymp1yj6PWwee\nR2ZPdUaB62TCzO0iRv7W6o39ey+mU/FyYRtxF0ecxG2a0KNsIyFkciXUAeC5UVDo\nBNp678/SDDx9Ltuxc6h56a/hpBGf9Yzhr0RvYy3DmjBs6eopiGFmjnOKNxQrZ5t2\n339JWR+yiGEAtoHqk/fINMf1An6Rung1xYowrm4guhCIVi5unAvQ89fq0I6mzPg6\nLhTpeP0o+mVYrBmtYVpDpv0e71cfYowSJCCkod/9YbY=\n-----END
              CERTIFICATE-----'
```

**Example 3**: public key published as JWKs:

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        jwkFromUrl: https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com
```

#### EdDSA based

If your auth server is using EdDSA to sign JWTs, and is using the Ed25519 variant key, the JWT config only needs to have
the public key.

**Example 1**: public key in PEM format (not OpenSSH format):

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        fixed:
          algorithm: Ed25519
          key:
            value:
              '-----BEGIN PUBLIC KEY-----\nMCowBQYDK2VwAyEAG9I+toAAJicilbPt36tiC4wi7E1Dp9rMmfnwdKyVXi0=\n-----END PUBLIC
              KEY-----'
```

**Example 2**: public key as X509 certificate:

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        fixed:
          algorithm: Ed25519
          key:
            value:
              '-----BEGIN CERTIFICATE
              REQUEST-----\nMIIBAzCBtgIBADAnMQswCQYDVQQGEwJERTEYMBYGA1UEAwwPd3d3LmV4YW1wbGUu\nY29tMCowBQYDK2VwAyEA/9DV/InajW02Q0tC/tyr9mCSbSnNP1txICXVJrTGKDSg\nXDBaBgkqhkiG9w0BCQ4xTTBLMAsGA1UdDwQEAwIEMDATBgNVHSUEDDAKBggrBgEF\nBQcDATAnBgNVHREEIDAegg93d3cuZXhhbXBsZS5jb22CC2V4YW1wbGUuY29tMAUG\nAytlcANBAKbTqnTyPcf4ZkVuq2tC108pBGY19VgyoI+PP2wD2KaRz4QAO7Bjd+7S\nljyJoN83UDdtdtgb7aFgb611gx9W4go=\n-----END
              CERTIFICATE REQUEST-----'
```

#### EC based

If your auth server is using ECDSA to sign JWTs, and is using the ES variant with a 256-bit key, the JWT config only
needs to have the public key.

**Example 1**: public key in PEM format (not OpenSSH format):

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        fixed:
          algorithm: ES256
          key:
            value:
              '-----BEGIN PUBLIC
              KEY-----\nMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEEVs/o5+uQbTjL3chynL4wXgUg2R9\nq9UU8I5mEovUf86QZ7kOBIjJwqnzD1omageEHWwHdBO6B+dFabmdT9POxg==\n-----END
              PUBLIC KEY-----'
```

**Example 2**: public key as X509 certificate:

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        fixed:
          algorithm: ES256
          key:
            value:
              '"-----BEGIN
              CERTIFICATE-----\nMIIBbjCCARWgAwIBAgIUGn02F6Y6s88dDGmIfwiNxWxDjhswCgYIKoZIzj0EAwIw\nDTELMAkGA1UEBhMCSU4wHhcNMjMwNTI0MTAzNTI4WhcNMjgwNTIyMTAzNTI4WjAN\nMQswCQYDVQQGEwJJTjBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABBFbP6OfrkG0\n4y93Icpy+MF4FINkfavVFPCOZhKL1H/OkGe5DgSIycKp8w9aJmoHhB1sB3QTugfn\nRWm5nU/TzsajUzBRMB0GA1UdDgQWBBSaqFjzps1qG+x2DPISjaXTWsTOdDAfBgNV\nHSMEGDAWgBSaqFjzps1qG+x2DPISjaXTWsTOdDAPBgNVHRMBAf8EBTADAQH/MAoG\nCCqGSM49BAMCA0cAMEQCIBDHHWa/uLAVdGFEk82auTmw995+MsRwv52VXLw2Z+ji\nAiAXzOWIcGN8p25uhUN/7v9gEcADGIS4yUiv8gsn/Jk2ow==\n-----END
              CERTIFICATE-----'
```

**Example 3**: public key published as JWKs:

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        jwkFromUrl: https://www.gstatic.com/iap/verify/public_key-jwk
```

## Security considerations

### Setting audience check

Certain JWT providers share JWKs between multiple tenants (like Firebase). They use the `aud` claim of JWT to specify
the intended tenant for the JWT. Setting the `audience` field in the Hasura JWT configuration will make sure that the
`aud` claim from the JWT is also checked during verification. Not doing this check will allow JWTs issued for other
tenants to be valid as well.

In these cases, you **MUST** set the `audience` field to appropriate value. **Failing to do so is a major security
vulnerability**.

## JWT with the WebSocket protocol

When executing a subscription (or query or mutation) over the WebSocket protocol, the authentication step is executed on
`connection_init` when the websocket is connected to Hasura Engine and is valid until the expiry of the JWT when in JWT
mode.

Once authenticated, all operations are allowed without further check, until the authentication expires.

## Popular providers and known issues

### AWS Cognito

AWS Cognito and ELB (Elastic Load Balancer) has a known issue where it adds additional padding (using = characters) to
the JWT token that is generated from Cognito.

This is a known issue and is documented by AWS in
[their docs](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html#user-claims-encoding):

> Standard libraries are not compatible with the padding that is included in the Application Load Balancer
> authentication token in JWT format.

Currently, there is no workaround possible in Hasura. Even if Hasura strips the additional padding the signature
verification of the token would fail (as Hasura had to tamper the token).

### Firebase

Firebase publishes the JWKs at:

[https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com](https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com)

If you are using Firebase and Hasura, use this config:

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: Header
        name: Authorization
      key:
        jwkFromUrl: https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com
      issuer: https://securetoken.google.com/<firebase-project-id>
      audience: <firebase-project-id>
```

### Auth0 {#auth0-issues}

Refer to the [Auth0 JWT tutorial](/auth/jwt/tutorials/integrations/1-auth0.mdx) for a detailed guide on integrating
Auth0 with Hasura.

### Clerk

Clerk integrates with Hasura DDN using JWTs.

Clerk publishes their JWK under: `https://<YOUR_CLERK_FRONTEND_API>/.well-known/jwks.json`

Refer to the [Clerk JWT tutorial](/auth/jwt/tutorials/integrations/4-clerk.mdx) to set up authenticated requests to
Hasura with Clerk.

### Keycloak

By default, Keycloak uses the `RSA-OAEP` algorithm, which Hasura doesn't support. Remove the algorithm in the
`Realm Settings -> Keys -> Add Providers` tab.



==============================



# index.mdx

URL: https://hasura.io/docs/promptql/auth/jwt/tutorials/

# Authentication

## Introduction

In this section of tutorials, we'll provide you with concise up-to-date descriptions of how to connect your preferred
authentication provider to Hasura DDN.

## Tutorials

- [Auth0](/auth/jwt/tutorials/integrations/1-auth0.mdx)
- [AWS Cognito](/auth/jwt/tutorials/integrations/2-aws-cognito.mdx)
- [Firebase](/auth/jwt/tutorials/integrations/3-firebase.mdx)
- [Clerk](/auth/jwt/tutorials/integrations/4-clerk.mdx)



==============================



# index.mdx

URL: https://hasura.io/docs/promptql/auth/jwt/tutorials/integrations/

# Authentication

## Introduction

In this section of tutorials, we'll provide you with concise up-to-date descriptions of how to connect your preferred
authentication provider to Hasura DDN.

## Tutorials

- [Auth0](/auth/jwt/tutorials/integrations/1-auth0.mdx)
- [AWS Cognito](/auth/jwt/tutorials/integrations/2-aws-cognito.mdx)
- [Firebase](/auth/jwt/tutorials/integrations/3-firebase.mdx)
- [Clerk](/auth/jwt/tutorials/integrations/4-clerk.mdx)



==============================



# 1-auth0.mdx

URL: https://hasura.io/docs/promptql/auth/jwt/tutorials/integrations/1-auth0

# Auth0

## Introduction

In this tutorial, you'll learn how to configure an existing [Auth0 application](https://auth0.com) and generate a JWT
which you can pass in the header of your requests. After setting up your
[AuthConfig](/reference/metadata-reference/auth-config.mdx) object to use JWT mode, this will allow you to validate
users' identities and create [permission rules](/reference/metadata-reference/permissions.mdx) which can limit access to
underlying data served by Hasura DDN to PromptQL.

:::info Prerequisites

Before continuing, ensure you have:

- An Auth0 [application](https://manage.auth0.com/dashboard).
- A local application that you're actively developing, built with any language or framework supported by
  [Auth0's SDKs](https://auth0.com/docs/libraries).
- A local PromptQL project.

:::

## Tutorial

### Step 1. Create a new Auth0 application

From your Auth0 dashboard, click `Applications` in the sidebar and then click on `Create Application`. Enter a name for
your application, then choose the application type that best suits your needs.

After creating the application, go to `APIs` in the sidebar and create a new API with your GraphQL endpoint as the
identifier.

### Step 2. Create a new Auth0 Action

From your Auth0 dashboard, click `Actions` in the sidebar and choose `Triggers`.

Under `Sign Up & Login`, select the `post-login` trigger, click on the `+` icon, and then choose `Built from Scratch` to
create a new Action.

Enter a name for your Action such as `Hasura JWT Claims` and paste the following code:

```javascript
exports.onExecutePostLogin = async (event, api) => {
  const namespace = "claims.jwt.hasura.io";
  // Here, you'll need to fetch the user's role from Hasura DDN using an admin-level authenticated request
  // Learn more here: https://hasura.io/docs/3.0/auth/authentication/jwt/special-roles
  // Below, we're hard-coding the value for now
  const user_role = "user"; // the role returned from your request ☝️
  api.idToken.setCustomClaim(namespace, {
    "x-hasura-default-role": user_role,
    "x-hasura-allowed-roles": [user_role],
    "x-hasura-user-id": event.user.user_id,
    // Add any other custom claims you wish to include
  });

  // Set the necessary access token claims for Hasura to authenticate the user
  api.accessToken.setCustomClaim(namespace, {
    "x-hasura-default-role": user_role,
    "x-hasura-allowed-roles": [user_role],
    "x-hasura-user-id": event.user.user_id,
  });
};
```

This will add the required Hasura namespace with the keys that Hasura DDN expects when decoding a JWT. You can modify
the keys to suit your application's [roles](/reference/metadata-reference/permissions.mdx#typepermissions).

Click `Deploy`.

:::tip Custom claims

You can create any custom keys you wish and reference them in your permissions using session variables. Above,
`x-hasura-user-id` is simply an example. Any claim prefixed with `x-hasura-` is accessible to the Hasura DDN Engine.

:::

### Step 3. Update your AuthConfig

Update your AuthConfig object to use JWT mode and your
[Auth0 JWKs](https://auth0.com/docs/secure/tokens/json-web-tokens/json-web-key-sets):

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: "/claims.jwt.hasura.io"
      issuer: "<your Auth0 tenant's URL>"
      key:
        jwkFromUrl: "https://<your Auth0 tenant's URL>/.well-known/jwks.json"
      audience: ["<your endpoint>"]
      tokenLocation:
        type: Header
        name: Auth-Token
```

Then, create a new build:

```ddn
ddn supergraph build local
```

### Step 4. Test your configuration

The easiest way to verify your setup is to generate a new JWT by logging into your client application that uses Auth0.
These values aren't typically displayed to users, so you'll need to log them while in development. You can then add that
value as a header in the [PromptQL Playground](/auth/playground-auth.mdx) and test any permissions you have in your
metadata. If you're unfamiliar with this, we have a sample repo
[here](https://github.com/hasura/ddn-auth-examples/tree/main/auth0).

:::info Auth0 API Debugger

Auth0 does provide an extension: the `Auth0 Authentication API Debugger` for testing configurations. However, custom
claims have been known to cause issues. You can find more information about using their debugger
[here](https://auth0.com/docs/customize/extensions/authentication-api-debugger-extension).

:::

### Step 5. Service account access token (optional)

In certain cases, you may need to generate a service account access token to access your application when performing
backend operations. You can do this by creating a new Auth0 application named `Service Account` with a type of
`Machine to Machine Applications`.

After creating the application, go to `Triggers`, located underneath `Actions` in the sidebar, and click on the
`credentials-exchange` trigger. Similar to the `post-login` trigger, create a new Action and paste the code below:

```javascript
exports.onExecuteCredentialsExchange = async (event, api) => {
  const namespace = "claims.jwt.hasura.io";

  const service_role = "service_account";

  api.accessToken.setCustomClaim(namespace, {
    "x-hasura-default-role": service_role,
    "x-hasura-allowed-roles": [service_role],
  });
};
```

This will generate a new JWT token with the `service_account` role which can then be used to access your application.

You can generate a new access token using the following Python code:

```python

conn = http.client.HTTPSConnection("<Auth0 domain>")

payload = "{\"client_id\":\"<client id>\",\"client_secret\":\"<Auth0 Client Secret>\",\"audience\":\"<your GraphQL endpoint>\",\"grant_type\":\"client_credentials\"}"

headers = { 'content-type': "application/json" }

conn.request("POST", "/oauth/token", payload, headers)

res = conn.getresponse()
data = res.read()

print(data.decode("utf-8"))
```

The response will look like:

```json
{
  "access_token": "<service account access token>",
  "token_type": "Bearer"
}
```

You can modify your metadata to allow the `service_account` role to access the necessary models.

:::tip Best Practices for Service Accounts

Instead of granting a service account full admin access, create a custom role with only the necessary permissions. This
approach follows the principle of least privilege, thereby limiting potential impact in case of any errors or
oversights.

:::

## Wrapping up

In this guide, you learned how to integrate Auth0 with Hasura DDN and PromptQL to create a secure and scalable identity
management solution using JWTs. By leveraging custom claims in conjunction with
[permissions](/reference/metadata-reference/permissions.mdx), you can define precise access-control rules, ensuring that
your application remains secure and meets your users' needs.

As you continue building out your application, keep in mind that authentication and authorization are crucial
components. Always validate your configuration and regularly test your setup to ensure it functions as expected across
different roles and environments.

If you encounter issues or need further customization, consider reviewing our related documentation or exploring
additional Auth0 features that can enhance your authentication flows.



==============================



# 2-aws-cognito.mdx

URL: https://hasura.io/docs/promptql/auth/jwt/tutorials/integrations/2-aws-cognito

# AWS Cognito

## Introduction

In this tutorial, you'll learn how to configure an existing
[AWS Cognito user pool](https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools.html) and generate
a JWT which you can pass in the header of your requests to Hasura. After setting up your
[AuthConfig](/reference/metadata-reference/auth-config.mdx) object to use JWT mode, this will allow you to validate
users' identities and create [permission rules](/reference/metadata-reference/permissions.mdx) which can limit access to
underlying data served by Hasura DDN.

:::info Prerequisites

Before continuing, ensure you have:

- An AWS Cognito [user pool](https://manage.auth0.com/dashboard) with a domain configured.
- A local application that can integrate with Cognito for authentication.
- A local Hasura project.

:::

## Tutorial

### Step 1. Create a Lambda trigger for modifying JWT claims

To add the custom claims that Hasura requires, you will need to create an AWS Lambda function and set it as a trigger
for your Cognito user pool.

In the [AWS Lambda console](https://console.aws.amazon.com/lambda/home), create a new Lambda function. Select the
`Author from scratch` option and provide a name, runtime, architecture, and any advanced settings you wish to configure.

After your Lambda is created, you'll be redirected to an editor where you can modify the Lambda's handler. Add the
following code to modify the Cognito JWT and inject the custom Hasura namespace claims:

```javascript
export const handler = async (event) => {
  // Here, you'll need to fetch the user's role from Hasura DDN using an admin-level authenticated request
  // Learn more here: https://hasura.io/docs/3.0/auth/authentication/jwt/special-roles
  // Below, we're hard-coding the value for now
  const user_role = "user"; // the role returned from your request ☝️
  event.response = {
    claimsOverrideDetails: {
      claimsToAddOrOverride: {
        "claims.jwt.hasura.io": JSON.stringify({
          "x-hasura-user-id": event.request.userAttributes.sub,
          "x-hasura-default-role": user_role,
          "x-hasura-allowed-roles": ["user"],
        }),
      },
    },
  };

  return event;
};
```

This will add the required Hasura namespace with the keys that Hasura expects when decoding a JWT. You can modify the
keys to suit your application's [roles](/reference/metadata-reference/permissions.mdx#typepermissions).

Click `Deploy`.

:::tip Custom claims

You can create any custom keys you wish and reference them in your permissions using session variables. Above,
`x-hasura-user-id` is simply an example. Any claim prefixed with `x-hasura-` is accessible to Hasura.

:::

### Step 2. Add the Lambda as an Authentication trigger

From your user pool's dashboard, select the `User pool properties` tab and then click `Add Lambda trigger`. Choose
`Authentication` as the trigger type and choose `Pre token generation trigger`.

Then, select the Lambda you generated in the previous step and click `Add Lambda trigger`.

### Step 3. Update your AuthConfig

Update your AuthConfig object to use JWT mode and your
[Cognito JWKs](https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-tokens-verifying-a-jwt.html),
which you can find on your User pool overview card:

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: StringifiedJson
          location: "/claims.jwt.hasura.io"
      key:
        jwkFromUrl: "https://cognito-idp.<your_region>.amazonaws.com/<your_region>_<your_user_pool_id>.well-known/jwks.json"
      tokenLocation:
        type: Header
        name: Auth-Token
```

Then, create a new build:

```ddn
ddn supergraph build local
```

### Step 4. Test your configuration

Generate a new JWT by logging into your application. These values aren't typically displayed to users, so you'll need to
log them while in development. You can then add that value as a header in the Playground and test any permissions you
have in your metadata.

## Wrapping up

In this guide, you learned how to integrate AWS Cognito with PromptQL to create a secure and scalable identity
management solution using JWTs. By leveraging custom claims in conjunction with
[permissions](/reference/metadata-reference/permissions.mdx), you can define precise access-control rules, ensuring that
your application remains secure and meets your users' needs.

As you continue building out your supergraph, keep in mind that authentication and authorization are crucial components.
Always validate your configuration and regularly test your setup to ensure it functions as expected across different
roles and environments.

If you encounter issues or need further customization, consider reviewing our related documentation or exploring
additional AWS Cognito features that can enhance your authentication flows.



==============================



# 3-firebase.mdx

URL: https://hasura.io/docs/promptql/auth/jwt/tutorials/integrations/3-firebase

# Firebase

## Introduction

In this tutorial, you'll learn how to configure an existing [Firebase project](https://console.firebase.google.com/) and
generate a JWT which you can pass in the header of your requests to Hasura. After setting up your
[AuthConfig](/reference/metadata-reference/auth-config.mdx) object to use JWT mode, this will allow you to validate
users' identities and create [permission rules](/reference/metadata-reference/permissions.mdx) which can limit access to
underlying data served by Hasura DDN.

:::info Prerequisites

Before continuing, ensure you have:

- A [Firebase project](https://console.firebase.google.com/) created, with at least one authentication method enabled.
- A local application that can integrate with Firebase for authentication. We'll provide an example Node.js application
  below.
- A private key from the project's `Service Accounts` section via the
  [Firebase console](https://console.firebase.google.com/project/_/settings/serviceaccounts/adminsdk).
- A local Hasura project.

:::

## Tutorial

### Step 1. Add Firebase to your local application

#### Step 1.1 Add the firebase-admin package

Firebase works with a number of [languages and frameworks](https://firebase.google.com/docs/admin/setup#add-sdk). In the
example(s) we show below, we'll use a Node.js application.

```sh
npm i firebase-admin
```

#### Step 1.2 Initialize firebase-admin

Then, initialize the module in your application:

```javascript
const admin = require("firebase-admin");

// service_account.json points to the private key from the prerequisites
admin.initializeApp({ credential: admin.credential.cert(require("./service_account.json")) });
```

#### Step 1.3 Add the custom claims

```javascript
// Here, you'll need to fetch the user's role from Hasura using an admin-level authenticated request
// Learn more here: https://hasura.io/docs/3.0/auth/authentication/jwt/special-roles
// Below, we're hard-coding the value for now
const user_role = "user"; // the role returned from your request ☝️
const customClaims = {
  "claims.jwt.hasura.io": {
    "x-hasura-default-role": user_role,
    "x-hasura-allowed-roles": ["user"],
    "x-hasura-user-id": decodedToken.uid,
  },
};

// Set custom claims for the user based on their uid
await admin.auth().setCustomUserClaims(decodedToken.uid, customClaims);
```

This will add the required Hasura namespace with the keys that Hasura expects when decoding a JWT. You can modify the
keys to suit your application's [roles](/reference/metadata-reference/permissions.mdx#typepermissions).

:::tip Custom claims

You can create any custom keys you wish and reference them in your permissions using session variables. Above,
`x-hasura-user-id` is simply an example. Any claim prefixed with `x-hasura-` is accessible to the Hasura DDN Engine. :::

### Step 2. Update your AuthConfig

Update your AuthConfig object to use JWT mode and your
[Firebase JWKs](https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com) and audience:

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      audience: ["your-firebase-project-name"]
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: "/claims.jwt.hasura.io"
      key:
        jwkFromUrl: "https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com"
      tokenLocation:
        type: Header
        name: Auth-Token
```

:::info Firebase JWKs

Firebase uses the same set of JWKs for all applications. It distinguishes between different apps by specifying the
audience (`aud`) claim in the JWT. Make sure to set the audience field to match your Firebase project name in the
AuthConfig object to ensure proper validation.

:::

Then, create a new build:

```ddn
ddn supergraph build local
```

### Step 3. Test your configuration

Generate a new JWT by logging into your application. These values aren't typically displayed to users, so you'll need to
log them while in development. You can then add that value as a header in the Playground and test any permissions you
have in your metadata.

<details>
  <summary>Here's the complete sample Node.js server.</summary>

```javascript
const express = require("express");
const admin = require("firebase-admin");
const bodyParser = require("body-parser");
const axios = require("axios");

// Initialize Firebase Admin SDK
admin.initializeApp({
  credential: admin.credential.cert(require("./service_account.json")),
});

const app = express();
app.use(bodyParser.json());

// Firebase API key from your Firebase project settings
const FIREBASE_API_KEY = "your API key found on the Firebase project's console";

// Route to handle user login with email and password
app.post("/login", async (req, res) => {
  const { email, password } = req.body;

  if (!email || !password) {
    return res.status(400).json({ message: "Email and password are required" });
  }

  try {
    // Call Firebase REST API to sign in the user with email and password
    const response = await axios.post(
      `https://identitytoolkit.googleapis.com/v1/accounts:signInWithPassword?key=${FIREBASE_API_KEY}`,
      {
        email,
        password,
        returnSecureToken: true,
      }
    );

    const { idToken } = response.data;

    // Verify the token using Firebase Admin SDK
    const decodedToken = await admin.auth().verifyIdToken(idToken);

    // Here, you'll need to fetch the user's role from Hasura DDN using an admin-level authenticated request
    // Learn more here: https://hasura.io/docs/3.0/auth/authentication/jwt/special-roles
    // Below, we're hard-coding the value for now
    const user_role = "user"; // the role returned from your request ☝️
    const customClaims = {
      "claims.jwt.hasura.io": {
        "x-hasura-default-role": user_role,
        "x-hasura-allowed-roles": ["user"],
        "x-hasura-user-id": decodedToken.uid,
      },
    };

    // Set custom claims for the user based on their uid
    await admin.auth().setCustomUserClaims(decodedToken.uid, customClaims);

    // Send the updated JWT back in the response
    res.status(200).json({
      idToken,
    });
  } catch (error) {
    console.error("Error logging in:", error.response?.data || error.message);
    res.status(401).json({ message: "Invalid credentials", error: error.response?.data || error.message });
  }
});

app.listen(4000, () => {
  console.log("Server running on port 4000");
});
```

</details>

## Wrapping up

In this guide, you learned how to integrate Firebase with Hasura DDN and PromptQL to create a secure and scalable
identity management solution using JWTs. By leveraging custom claims in conjunction with
[permissions](/reference/metadata-reference/permissions.mdx), you can define precise access-control rules, ensuring that
your application remains secure and meets your users' needs.

As you continue building out your supergraph, keep in mind that authentication and authorization are crucial components.
Always validate your configuration and regularly test your setup to ensure it functions as expected across different
roles and environments.

If you encounter issues or need further customization, consider reviewing our related documentation or exploring
additional Firebase features that can enhance your authentication flows.



==============================



# 4-clerk.mdx

URL: https://hasura.io/docs/promptql/auth/jwt/tutorials/integrations/4-clerk

# Clerk

## Introduction

In this tutorial, you'll learn how to configure an existing [Clerk application](https://clerk.com/) and generate a JWT
which you can pass in the header of your requests to Hasura. After setting up your
[AuthConfig](/reference/metadata-reference/auth-config.mdx) object to use JWT mode, this will allow you to validate
users' identities and create [permission rules](/reference/metadata-reference/permissions.mdx) which can limit access to
underlying data served by Hasura DDN.

:::info Prerequisites

Before continuing, ensure you have:

- A [Clerk application](https://clerk.com/docs/quickstarts/setup-clerk).
- A local application that can integrate with Clerk for authentication.
- A local Hasura project.

### Step 1. Create a JWT template

From your [Clerk application's dashboard](https://dashboard.clerk.com/), click `JWT templates` in the sidenav and create
a new blank template. You can name this whatever you wish along with configuring properties like the token's lifetime,
clock skew, etc.

In the claims editor, add the following:

```javascript
{
	"claims.jwt.hasura.io": {
		"x-hasura-user-id": "{{user.id}}",
		"x-hasura-default-role": "user",
		"x-hasura-allowed-roles": [
			"user"
		]
	}
}
```

This will add the required Hasura namespace with the keys that Hasura expects when decoding a JWT. You can modify the
keys to suit your application's [roles](/reference/metadata-reference/permissions.mdx#typepermissions).

You can also see that we're hard-coding the user's role. You can dynamically set a user's role using Clerk's metadata to
set a `role` field on the `User` object whenever a user registers. You can learn more
[here](https://clerk.com/docs/users/metadata).

This enables you to then pass the value of `{{user.publicMetadata.role}}` in the custom claims of the JWT.

:::tip Custom claims

You can create any custom keys you wish and reference them in your permissions using session variables. Above,
`x-hasura-user-id` is simply an example. Any claim prefixed with `x-hasura-` is accessible to the Hasura.

:::

### Step 2. Update your AuthConfig

Update your AuthConfig object to use JWT mode and your Clerk JWKs, which rely on your Clerk domain name found in the
sidenav under `Domains`:

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: "/claims.jwt.hasura.io"
      key:
        jwkFromUrl: "https://<your-clerk-domain>/.well-known/jwks.json"
      tokenLocation:
        type: Header
        name: Auth-Token
```

Then, create a new build:

```ddn
ddn supergraph build local
```

### Step 3. Test your configuration

Generate a new JWT by logging into your application. These values aren't typically displayed to users, so you'll need to
log them while in development. You can then add that value as a header in the console and test any permissions you have
in your metadata.

Clerk's [various SDKs](https://clerk.com/docs/references/overview) make this easy, as you can pass the name of a JWT
template and get back the encoded token.

```javascript title="As an example, using the JavaScript SDK:"
const jwt = await session.getToken({ template: "hasura" });
```

## Wrapping up

In this guide, you learned how to integrate Clerk with Hasura DDN and PromptQL to create a secure and scalable identity
management solution using JWTs. By leveraging custom claims in conjunction with
[permissions](/reference/metadata-reference/permissions.mdx), you can define precise access-control rules, ensuring that
your application remains secure and meets your users' needs.

As you continue building out your supergraph, keep in mind that authentication and authorization are crucial components.
Always validate your configuration and regularly test your setup to ensure it functions as expected across different
roles and environments.

If you encounter issues or need further customization, consider reviewing our related documentation or exploring
additional Clerk features that can enhance your authentication flows.



==============================



# setup-test-jwt.mdx

URL: https://hasura.io/docs/promptql/auth/jwt/tutorials/setup-test-jwt

# Set up a JWT for Testing

By default, your supergraph uses your Hasura Cloud authentication token, also known as a personal access token (PAT),
for authentication. This is convenient for testing from the Playground, but should not be used in an application or
shared with others.

Instead, to test authentication you need to set up an
[`AuthConfig` object](/reference/metadata-reference/auth-config.mdx) and then generate the corresponding token.

## Step 1. Install the jwt-cli

Install the [`jwt-cli`](https://github.com/mike-engel/jwt-cli), which allows you to generate tokens from the command
line. You can follow their list of installation instructions found
[here](https://github.com/mike-engel/jwt-cli?tab=readme-ov-file#installation).

## Step 2. Generate a random string

Generate a random string that we'll use as the JWT secret key:

```bash title="In your teminal, run the following command"
openssl rand -hex 16
```

Copy the value returned by the terminal.

:::info Creating a random string

If you don't want to use openssl, you can use any other random string generators. The only requirement is that the
string must be at least 32 characters.

:::

## Step 3. Set up your AuthConfig object

Set up an `AuthConfig` object in your project which uses this secret key.

```yaml title="In globals/metadata/auth-config.hml:"
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: "/claims.jwt.hasura.io"
      key:
        fixed:
          algorithm: HS256
          key:
            value: "<insert-the-key-generated-in-previous-step>"
      tokenLocation:
        type: Header
        name: Auth-Token
```

## Step 4. Create a new build

Create a supergraph build using this `AuthConfig`.

```ddn title="From the root of your project, run:"
ddn supergraph build create --description "use jwt-based authconfig"
```

## Step 5. Generate a JWT

For testing, you can use the `jwt-cli` to encode and generate a new token with the different claims written to match
your testing needs.

```bash title="Run the following with your own values:"
jwt encode --secret="<secret-key>" '{"exp": 1739905122,"iat": 1708369122,"claims.jwt.hasura.io":{"x-hasura-default-role": "admin","x-hasura-allowed-roles":["admin"]}}'
```

In the example above, we're setting the following values:

- The issued (`iat`) time as `Feb. 19 2024, at 18:58:42` as a Unix epoch timestamp.
- The expiration (`exp`) time as `Feb. 18, 2025 at 18:58:42`.
- The default role as `admin`.
- The allowed roles as `admin`.

For more information about the claims Hasura expects, check out [this page](/auth/jwt/jwt-configuration.mdx).

## Step 6: Test your AuthConfig

In the Playground, add the JWT generated as the value of a new header called `Auth-token`. You should now be able to
create threads scoped to your custom JWT.

:::info Using environment variables

If you're storing your secret key's value as an environment variable, ensure you've updated the `subgraph.yaml` in the
`globals` subgraph to include this envMapping.

:::



==============================



# webhook-mode.mdx

URL: https://hasura.io/docs/promptql/auth/webhook/webhook-mode


# Webhook Mode

## Introduction

You can enable Hasura DDN to use an auth webhook in just a few steps.

You will need to provide a URL that your application will call with the original request headers, and it should return a
body with the session variables after the request is authenticated.

<Thumbnail src="/img/auth/auth-webhook-overview-diagram.png" alt="Authentication using webhooks" width="1000px" />

## Session variable requirements

The only session variable required is `x-hasura-role` appearing in the response body.

In contrast to JWT mode, you do not have to pass `x-hasura-allowed-roles` or `x-hasura-default-role` session variables
and a `x-hasura-role` header will no be checked.

Session variable keys are case-insensitive. Values are case-sensitive.

## Enabling Webhook authentication

## Step 1. Update your AuthConfig {#update-authconfig}

Hasura utilizes an [AuthConfig](/reference/metadata-reference/auth-config.mdx) object that allows you to define the
configuration for your authentication service. In a standard setup the `auth-config.hml` file can be found in your
`globals` directory.

:::tip Hasura VS Code extension

You can use [Hasura's VS Code extension](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura) to
scaffold out your `AuthConfig` object by typing `AuthConfig` and selecting this object from the list of available
options. As you navigate through the skeleton, you can type `CTRL+SPACEBAR` at any point to reveal options for the
different key-value pairs.

:::

In the example below, we're demonstrating a sample authentication webhook.

```yaml title="globals/metadata/auth-config.hml"
kind: AuthConfig
version: v3
definition:
  mode:
    webhook:
      url:
        valueFromEnv: AUTH_WEBHOOK_URL
      method: POST
      customHeadersConfig:
        body:
          headers:
            forward:
              - authorization
              - content-type
        headers:
          additional:
            user-agent: "Hasura DDN"
```

```yaml title="Example .env file"
AUTH_WEBHOOK_URL=http://auth_hook:3050/validate-request
```

### GET vs POST

For a POST request, the headers received by Hasura on the query can be forwarded to the webhook in a JSON object in the
body of the request under a `headers` key.

For a GET request, the headers received by Hasura on the query can be forwarded to the webhook as actual headers on the
request and the body will be empty.

What we've provided above is a sample configuration as a POST request but it can be configured as a GET request. See the
reference for `AuthHookConfig` [here](/reference/metadata-reference/auth-config.mdx#authconfig-authhookconfigv3). For
the sample configuration above, the webhook will receive the `Authorization` and `Content-Type` headers in the body of
the request:

```json title="Example body of a POST request"
{
  "headers": {
    "Authorization": "Bearer some-token",
    "Content-Type": "application/json"
  }
}
```

Also, the webhook will receive the `user-agent` header as an actual header on the request.

:::tip Environment variables

You can use environment variables to dynamically and securely add the webhook URL to your AuthConfig. You can add these
values to your root-level `.env` and then map them in the `globals` subgraph.yaml file. Alternatively, you can include
raw strings here using `value` instead of `valueFromEnv`.

:::

## Step 2. Shaping the webhook request and response

### Request

Below is an example of the header object your webhook might receive in the body of a POST request:

```json title="Example header object"
{
  "headers": {
    "Authorization": "Bearer some-token",
    "Content-Type": "application/json"
  }
}
```

Headers are forwarded to the auth webhook from the client on each request received by your application either as headers
for a GET request or as a JSON object in the body of a POST request under a `headers` key.

:::tip Custom Headers Configuration

You can configure the headers that are sent to the webhook in the optional `customHeadersConfig` field. If the field is
not provided, the default behavior is to forward all headers (after filtering out commonly used headers) received by
Hasura on the query to the webhook.

```yaml title="Ignored headers list"
Accept Accept-Datetime Accept-Encoding Accept-Language Cache-Control Connection Content-Length Content-MD5 Content-Type
DNT Host Origin Referer User-Agent
```

When using `customHeadersConfig`, you can explicitly forward any headers, including those that are ignored by default.
This gives you full control over which headers are sent to the webhook.

It is recommended to use the `customHeadersConfig` field to configure only the headers that are required by the webhook
and not forward any other headers. This will help in reducing the size of the request and improve the performance of the
webhook.

:::

In this example, we're passing an encoded JWT in the `Authorization` header, however webhook mode is flexible and you
can pass any headers you wish.

:::tip Forward all headers

If you want to forward all headers received by Hasura on the query to the webhook, you can set `forward: "*"`. This will
forward all headers received by Hasura on the query to the webhook, excluding the ignored headers.

:::

### Token Parsing

In this example, the webhook is then responsible for validating and parsing the token passed in the header. It will need
to:

- **Extract the Token:** Retrieve the Authorization header from the incoming request and extract the token.

- **Validate the Token:** Use a library or your own logic to validate the token. This involves verifying the token's
  signature with the secret key.

- **Extract Claims:** Decode the token to extract the claims.

### Response

Based on the validation result, the webhook will need to respond with either a `200` status code (for a valid token) or
a `401` status code (for an invalid or missing token).

You should respond with session variables beginning with `X-Hasura-*` in an object in the **body** of your response. The
value of each session variable can be any JSON value. These will be available to your
[permissions](/reference/metadata-reference/permissions.mdx) in Hasura.

You will, at least, need to set the `X-Hasura-Role` session variable to let the Hasura DDN know which role to use for
this request. Unlike [JWT auth mode](auth/jwt/jwt-mode.mdx), you do not have to pass `X-Hasura-Allowed-Roles` or
`X-Hasura-Default-Role` session variables.

In the example below the `X-Hasura-Is-Owner` and `X-Hasura-Custom` are examples of custom session variables which can be
used to enforce permissions in your application.

```json title="Example response from your webhook to Hasura DDN"
HTTP/1.1 200 OK
Content-Type: application/json

{
    "X-Hasura-Role": "user",
    "X-Hasura-User-Id": 25,
    "X-Hasura-Is-Owner": "true",
    "X-Hasura-Custom": "custom value"
}
```

## Step 3. Define permissions

Let's add some example `TypePermissions` so that an admin role can access all fields in the Orders type, but we restrict
a user role from accessing the `deliveryDate` field.

```bash title="Example TypePermissions for Orders type"
---
kind: TypePermissions
version: v1
definition:
  typeName: Orders
  permissions:
    - role: admin
      output:
        allowedFields:
          - createdAt
          - deliveryDate
          - id
          - isReviewed
          - productId
          - status
          - updatedAt
          - userId
  # highlight-start
    - role: user
      output:
        allowedFields:
          - createdAt
          - id
          - isReviewed
          - productId
          - status
          - updatedAt
          - userId
  # highlight-end
```

Let's also add some example `ModelPermissions` so that an admin role can access all rows in the Orders model, but a user
role can only access rows where the `userId` field matches the user id session variable in the JWT.

```bash title="Example ModelPermissions for Orders model"
---
kind: ModelPermissions
version: v1
definition:
  modelName: Orders
  permissions:
    - role: admin
      select:
        filter: null
        allowSubscriptions: true
# highlight-start
    - role: user
      select:
        filter:
          fieldComparison:
            field: userId
            operator: _eq
            value:
              sessionVariable: x-hasura-user-id
# highlight-end
```

## Step 4. Rebuild your application

```ddn title="For example, from the root of your project, run:"
ddn supergraph build local
```

## Step 5. Make an authenticated request

Here we're making a request to our instance which will be validated by our webhook which returns a payload of session
variables.

```json title="Example response from our webhook to Hasura:"
{
  "x-hasura-user-id": "7cf0a66c-65b7-11ed-b904-fb49f034fbbb",
  "x-hasura-role": "user"
}
```

If we run a query for Orders, we can see that we only get the orders which this user has made and are not able to access
the `deliveryDate` field.



==============================



# index.mdx

URL: https://hasura.io/docs/promptql/auth/webhook/

# Authentication Using a Webhook

## Introduction

You can configure Hasura DDN to use webhook mode in order to authenticate incoming requests.

This requires specifying a URL - which Hasura calls with the original request headers - that then returns a body
containing the session variables after authenticating the request.

## Webhook mode setup

- [How to set up webhook mode](/auth/webhook/webhook-mode.mdx)



==============================



# noauth-mode.mdx

URL: https://hasura.io/docs/promptql/auth/noauth-mode


# NoAuth

## Introduction

NoAuth mode is a simple way to run your Hasura DDN and PromptQL instance without authentication and is enabled by
default. This is useful for testing or development purposes or to run your project as fully public without any
authentication.

:::danger Production Warning

Using NoAuth mode in production environments is not advisable unless you intend for your application to be completely
public. If your connected sources contain any sensitive data, you should enable authentication and avoid using NoAuth
mode in production.

:::

When you create a new project with the `ddn supergraph init` command, noAuth mode is enabled by default.

## Enabling NoAuth Mode

### Step 1. Update your AuthConfig

On default, in a standard new PromptQL project, you'll find an `AuthConfig` file in your `globals` directory already
configured for noAuth mode.

```yaml title="An AuthConfig for noAuth mode:"
kind: AuthConfig
version: v3
definition:
  mode:
    noAuth:
      role: admin
      sessionVariables: {}
```

#### `role`

The role to be assumed while running the application in `noAuth` mode. If you intend for your supergraph to be fully
public, you can set this to a value such as `public` or `anonymous` so that it's explicit.
[Read more about the `Role` value](/reference/metadata-reference/auth-config.mdx#authconfig-role)

#### `sessionVariables`

Static session variables that will be used while running the application without authentication. This is helpful when
you want to test requests using particular session variables, such as `x-hasura-user-id` with a non-admin role.
[Read more about the `SessionVariables` value](/reference/metadata-reference/auth-config.mdx#authconfig-sessionvariables)

### Step 2. Check permissions

On a default, standard new project you will find permissions such as the below for an example `Posts` model:

```yaml title="TypePermissions:"
kind: TypePermissions
version: v1
definition:
  typeName: Posts
  permissions:
    - role: admin
      output:
        allowedFields:
          - authorId
          - content
          - postId
          - title
```

```yaml title="ModelPermissions:"
kind: ModelPermissions
version: v1
definition:
  modelName: Posts
  permissions:
    - role: admin
      select:
        filter: null
        allowSubscriptions: true
```

You can see that the `admin` role has full access to the `Posts` model with no filtering in the `ModelPermissions` and
all fields being allowed in the `TypePermissions`.

### Step 3. Build your application

```ddn
ddn supergraph build local
```

### Step 4. Make an unauthenticated request

```title="Open the Hasura DDN console and make a request:"
ddn console --local
```

You can query your data without any authentication.



==============================



# model-permissions.mdx

URL: https://hasura.io/docs/promptql/auth/permissions/model-permissions

# Model Permissions

To limit what **data** in a model is available to a role, you define a `ModelPermissions` object with a `filter`
expression.

By default, whenever a new model is created in your supergraph, all records are only accessible to the `admin` role. You
can think of these as permissions on rows in a typical relational database table.

You can restrict access to certain data by adding a new item to the `permissions` array in the `ModelPermissions`
object. Each item in the array should have a `role` field and a `select` field. The `select` field should contain a
`filter` expression that determines which rows are accessible to the role when selecting from the model.

Most commonly, you'll use session variables — accessed by Hasura via your configured
[authentication mechanism](/auth/overview.mdx) in a JWT or body of a webhook response — to restrict access to rows based
on the user's role, identity or other criteria.

This filter expression can reference

- The fields in your Model
- Logical operators: `and`, `or` and `not`
- `fieldIsNull` predicate
- `fieldComparison` predicate
- Relationship predicates
- `null`

:::info Remote Relationships in Permissions

Remote relationships (relationships between different data connectors) across subgraphs are not supported in permission
filters.

:::

To make a new `ModelPermission` or role available in your application, after updating your metadata, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

### Examples

```yaml title="Allow admin to access all rows in the Articles model but allow user to access rows where the author_id field matches the user id session variable. Basically, their own articles."
---
kind: ModelPermissions
version: v1
definition:
  modelName: Articles
  # highlight-start
  permissions:
    - role: admin
      select:
        filter: null
    - role: user
      select:
        filter:
          fieldComparison:
            field: author_id
            operator: _eq
            value:
              sessionVariable: x-hasura-user-id
  # highlight-end
```

## Reference

See the [ModelPermissions](/reference/metadata-reference/permissions.mdx) reference for more information.



==============================



# type-permissions.mdx

URL: https://hasura.io/docs/promptql/auth/permissions/type-permissions

# Type Permissions

To make API **fields** available to a role, you define a `TypePermissions` object.

You can think of TypePermissions as being similar to column-level permissions in a relational database. Just as you can
restrict access to specific columns in a table based on the user's role, TypePermissions allow you to control access to
specific fields in a type within your supergraph.

By default, whenever a new type is created in your supergraph, each field is defined as being only accessible to the
`admin` role.

To add a new role, add a new item to the `permissions` array in the TypePermissions object.

Each item in the array should have a `role` field and an `output` field. The `output` field should contain an
`allowedFields` array, which lists the fields that are accessible to the role when the type is used in an output
context.

To make a new `TypePermission` object or role available in your application, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

## Example

```yaml title="Allow admin to access all fields in the article type, disallow user from accessing the author_id field."
---
kind: TypePermissions
version: v1
definition:
  typeName: article
  permissions:
    # highlight-start
    - role: admin
      output:
        allowedFields:
          - article_id
          - author_id
          - title
    - role: user
      output:
        allowedFields:
          - article_id
          - title
  # highlight-end
```

## Reference

See the [TypePermissions](/reference/metadata-reference/permissions.mdx) reference for more information.



==============================



# command-permissions.mdx

URL: https://hasura.io/docs/promptql/auth/permissions/command-permissions

# Command Permissions

To limit what **commands** are available to a role, you define a `CommandPermissions` object.

By default, whenever a new command is created in your supergraph, it is only executable by the `admin` role.

You can enable or restrict access to commands by adding a new item to the `permissions` array in the
`CommandPermissions` object. Each item in the array should have a `role` field and an `allowExecution` field. The
`allowExecution` field should be set to `true` if the command is executable by the role.

You can also use argument presets to pass actual logical expressions to your data sources to control how they do things.

For example, a data connector might expose a `Command` called `delete_user_by_id` with two arguments - `user_id` and
`pre_check`. `user_id` is the primary key of the user you'd like to remove, and `pre_check` lets you provide a custom
boolean expression.

```yaml
kind: CommandPermissions
version: v1
definition:
  commandName: delete_user_by_id
  # highlight-start
  permissions:
    - role: admin
      allowExecution: true
    - role: user
      allowExecution: true
      argumentPresets:
        - argument: pre_check
          value:
            booleanExpression:
              fieldComparison:
                field: is_invincible
                operator: _eq
                value:
                  literal: false
  # highlight-end
```

Now, when `admin` role runs this command, once again, they can do what they want, and provide their own `pre_check` if
they want.

The `user` role however, is able to pass a `user_id` argument, but the `pre_check` expression is passed to the data
connector which will only let them delete the row if the row's `is_invincible` value is set to `false`.

To make a execution of a command available to a role in your application, after updating your metadata, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

### Examples

```yaml title="Allow admin to execute the get_article_by_id command, restrict user to execute the get_article_by_id command with an id argument preset of 100."
---
kind: CommandPermissions
version: v1
definition:
  commandName: get_article_by_id
  # highlight-start
  permissions:
    - role: admin
      allowExecution: true
    - role: user
      allowExecution: true
      argumentPresets:
        - argument: id
          value:
            literal: 100
  # highlight-end
```

## Reference

See the [CommandPermissions](/reference/metadata-reference/permissions.mdx) reference for more information.



==============================



# index.mdx

URL: https://hasura.io/docs/promptql/auth/permissions/tutorials/

# Permissions Tutorials

## Introduction

In this section of tutorials, we'll provide step-by-step guides for common patterns in controlling users' access to data
via your supergraph.

If you're unfamiliar with how Hasura DDN handles authorization, [check out the docs](/auth/overview.mdx) before diving
deeper into one of these tutorials.

## Recipes

- [Limit data to users](/auth/permissions/tutorials/1-simple-user-permissions.mdx)



==============================



# 1-simple-user-permissions.mdx

URL: https://hasura.io/docs/promptql/auth/permissions/tutorials/1-simple-user-permissions

# Limit Data to Users

## Introduction

In this tutorial, you'll learn how to configure [permissions](/reference/metadata-reference/permissions.mdx) to limit
users to accessing only their data.

This can be done by passing a value in the header of each request to your application; Hasura will then use that value
to return only the data which matches conditions you specify.

:::info Prerequisites

Before continuing, ensure you have:

- A local Hasura project.
- Either JWT or Webhook mode enabled in your [AuthConfig](/reference/metadata-reference/auth-config.mdx).

:::

## Tutorial

### Step 1. Create your ModelPermissions {#step-one}

To create a new role, such as `user`, simply add the role to the list of `permissions` for the model to which you wish
to limit access. Then, set up your access control rules.

In the example below, we'll allow users with the role of `user` to access only their own rows from a `Users` model by
checking for a header value matching their `id`:

```yaml title="For example, in a Users.hml"
---
kind: ModelPermissions
version: v1
definition:
  modelName: Users
  permissions:
    - role: admin
      select:
        filter: null
        #highlight-start
    - role: user
      select:
        filter:
          fieldComparison:
            field: id
            operator: _eq
            value:
              sessionVariable: x-hasura-user-id
        #highlight-end
```

You can modify this to meet your own data modeling by ensuring that the `field` is a column or type that can be compared
to the value of the session variable you send in the header of your request. In this example, we're using
`x-hasura-user-id`, but you can use any `x-hasura-` value you wish.

:::info Authentication tutorials

We have tutorials for popular authentication providers available [here](/auth/jwt/tutorials/integrations/index.mdx)!

:::

### Step 2. Create your TypePermissions

By adding ModelPermissions, we've made the model available to the new role. However, this role is not yet able to access
any of the fields from the model. We can do that by adding the new role to the list of `permissions` and including which
fields are accessible to it.

```yaml title="For example, in a Users.hml"
---
kind: TypePermissions
version: v1
definition:
  typeName: Users
  permissions:
    - role: admin
      output:
        allowedFields:
          - createdAt
          - email
          - favoriteArtist
          - id
          - isEmailVerified
          - lastSeen
          - name
          - password
          - updatedAt
    #highlight-start
    - role: user
      output:
        allowedFields:
          - createdAt
          - email
          - favoriteArtist
          - id
          - isEmailVerified
          - lastSeen
          - name
          - password
          - updatedAt
    #highlight-end
```

If you want to restrict which fields the new role can access, simply omit them from the list of `allowedFields`.

### Step 3. Test your permissions

```ddn title="Create a new build:"
ddn supergraph build local
```

Then, in a request, pass a header with the [session variable](#step-one) you identified earlier according to your
authentication configuration. You should see a schema limited to whatever ModelPermissions you defined for your new role
and — when executing a query — only see data meeting the filtering rule you included in the first step.

## Wrapping up

In this guide, you learned how to limit a user to see only their own data from a single type. However, you can use this
same tutorial and apply it to a variety of scenarios.

As you continue building out your application, keep in mind that authentication and authorization are crucial
components. Always validate your configuration and regularly test your setup to ensure it functions as expected across
different roles and environments.

## Learn more about permissions and auth

- [Permissions](/auth/permissions/index.mdx) with Hasura DDN
- [Auth](/auth/overview.mdx) with Hasura DDN

## Similar tutorials

- [Authorization tutorials](/auth/permissions/tutorials/index.mdx)



==============================



# overview.mdx

URL: https://hasura.io/docs/promptql/project-configuration/overview

# Projects

## Introduction

A **project** in PromptQL is the foundation for building and managing your application. This foundational layer is known
as the **Hasura DDN (Data Delivery Network)** and contains a structured collection of metadata files that define the
behavior, relationships, and permissions for your application. These metadata files can be organized into **subgraphs**,
each representing a distinct data domain.

Projects are designed to support both local development and cloud deployment. During development, you work with a local
version of your project, which is linked to a cloud project through a
[context file](/project-configuration/project-management/manage-contexts.mdx). This linkage enables seamless
development, testing, and deployment workflows. You can also define multiple contexts (e.g., `staging`) to manage
different environments, with unique configurations for environment variables and cloud resources.

## Data domains

A **data domain** represents a distinct area of responsibility or focus within your project, typically aligned with a
specific team or business function. These domains are managed as **subgraphs**, which are collections of metadata files
that describe the relationships, permissions, and structure for the data within that domain.

Organizing your project into subgraphs provides several benefits:

- **Team Ownership**: Each team can focus on their own data domain without interfering with others, making collaboration
  simpler and reducing bottlenecks.
- **Clear Boundaries**: Subgraphs establish clear boundaries between domains, making it easier to define and enforce
  data access permissions and relationships.
- **Scalability**: By breaking your project into manageable pieces, you can scale your application incrementally as your
  organization and data requirements grow.
- **Flexibility**: Subgraphs can be independently updated and extended, allowing you to adapt your project to changing
  needs without disrupting the overall application.

This structure ensures that your project remains organized, collaborative, and adaptable, enabling you to build and
maintain robust applications efficiently.

:::info Multi-repo projects

PromptQL also supports multi-repository setups, giving teams greater autonomy in their development process. With this
setup:

- Each team can maintain their **subgraph** in a separate repository while still contributing to the shared
  **supergraph**.
- Teams can develop, test, and deploy their subgraphs independently, enabling faster iteration and minimizing
  dependencies on other teams.
- The supergraph integrates these subgraphs into a single application, ensuring that the overall application remains
  consistent and cohesive.

This approach is ideal for large organizations where multiple teams work on distinct data domains but need to
collaborate through a unified API. For more information, check out
[this section](/project-configuration/subgraphs/index.mdx) of the docs.

:::

## Find out more

- [Tutorials](/project-configuration/tutorials/index.mdx)
- [Learn more about the supergraph concept](/project-configuration/supergraph.mdx)
- [Learn more about provisioning subgraphs](/project-configuration/subgraphs/index.mdx)
- [Learn how to manage a project across environments](/project-configuration/project-management/index.mdx)



==============================



# index.mdx

URL: https://hasura.io/docs/promptql/project-configuration/tutorials/

# Tutorials

## Introduction

This section provides tutorials to guide you through essential project configuration tasks. You'll learn how to manage
multiple environments, structure your project around subgraphs, and distribute your project across separate repositories
to enable independent ownership and development.

## Available tutorials

- [Manage multiple environments](/project-configuration/tutorials/manage-multiple-environments.mdx)
- [Work with multiple subgraphs](/project-configuration/tutorials/work-with-multiple-subgraphs.mdx)
- [Work with multiple subgraphs across separate repositories](/project-configuration/tutorials/work-with-multiple-repositories.mdx)



==============================



# manage-multiple-environments.mdx

URL: https://hasura.io/docs/promptql/project-configuration/tutorials/manage-multiple-environments

# Manage Multiple Environments

## Introduction

Managing multiple contexts in Hasura allows you to replicate the functionality of traditional environments like
`staging` and `production`, but with greater flexibility. Instead of rigidly-defined environments, your application uses
contexts to store key details such as project configurations, metadata, and environment variables. This approach lets
you switch between different setups without disrupting your workflow or end users.

In this tutorial, you'll learn how to:

- Set up multiple contexts to test and collaborate on your project
- Use the CLI to manage your project's contexts
- Build and deploy projects with shared metadata across different contexts

This tutorial should take less than twenty minutes.

## Setup

### Step 1. Initialize a new local project

```ddn title="Create a new local project:"
ddn supergraph init environments-example --with-promptql && cd environments-example
```

This will scaffold out the necessary files for a project in a new `environments-example` directory.

### Step 2. Add a data source and seed data

In this tutorial, we'll use the PostgreSQL connector and a sample PostgreSQL database:

```plaintext
postgresql://read_only_user:readonlyuser@35.236.11.122:5432/v3-docs-sample-app
```

```ddn title="In your project directory, run the following, choose hasura/postrges, and pass the connection URI above when prompted:"
ddn connector init my_pg -i
```

### Step 3. Generate the Hasura metadata

```ddn title="Next, use the CLI to introspect the PostgreSQL database:"
ddn connector introspect my_pg
```

After running this, you should see a representation of your database's schema in the
`app/connector/my_pg/configuration.json` file; you can view this using `cat` or open the file in your editor.

```ddn title="Now, track the table from your PostgreSQL database as a model in your DDN metadata:"
ddn models add my_pg users
```

Open the `app/metadata` directory and you'll find a newly-generated file: `Users.hml`. The DDN CLI will use this Hasura
Metadata Language file to represent the `users` table from PostgreSQL in your API as a
[model](/reference/metadata-reference/models.mdx).

### Step 4. Create a new local build and test the API

```ddn title="To create a local build, run:"
ddn supergraph build local
```

The build is stored as a set of JSON files in `engine/build`.

```ddn title="Start your local Hasura instance and PostgreSQL connector:"
ddn run docker-start
```

Your terminal will be taken over by logs for the different services.

```ddn title="In a new terminal tab, open your local console:"
ddn console --local
```

You can now query it using the PromptQL Playground.

## Create a new context

### Step 5. Create a new project context and switch to it

```ddn title="From the project directory, run:"
ddn context create-context staging
```

Verify this by opening the `.hasura/context.yaml` file. You'll see a `contexts` array with two entries: `default` and
your newly-created `staging`.

:::tip What can be stored in context?

Contexts store a series of key-value pairs that make it easy to switch between different setups. You can learn more
[here](/project-configuration/project-management/manage-contexts.mdx).

:::

### Step 6. Update the context values

```ddn title="First, set the supergraph configuration:"
ddn context set supergraph "supergraph.yaml"
```

```ddn title="Then, define which subgraphs to include:"
ddn context set subgraph "app/subgraph.yaml"
```

```sh title="Finally, stub out a local .env.staging file and add it to your staging context:"
touch .env.staging && ddn context set localEnvFile ".env.staging"
```

You'll see each of these key-value pairs added to your `staging` context.

:::tip Customize these values

The examples provided here are starting points and can be adapted to suit your project's requirements. Ideally, the
values included in your `.env.staging` file will reference resources such as testing database instances. Keep the keys
consistent with those in your production `.env` files, but assign different values tailored for staging or testing
environments.

:::

### Step 7. Create a new staging cloud project

```ddn title="Using your new context, create a cloud project:"
ddn project init --env-file-name ".env.staging.cloud"
```

The CLI will add the project's name and your `cloudEnvFile` to your `staging` context.

### Step 8. Create a new build on your staging project

```ddn
ddn supergraph build create
```

The CLI will output information about the build, including a PromptQL URL which you can open in your browser. Your local
metadata was used to create this build on your `staging` project.

### Step 9. Create a new build on your production project

```ddn title="First, switch contexts:"
ddn context set-current-context default
```

```ddn title="Since our current context is default, we can now use the same metadata to create a productioun build:"
ddn supergraph build create
```

Just as with our `staging` project, you can navigate to the PromptQL URL output by the CLI and explore your `production`
build, which should be identical to your `staging` build.

## Next steps

Now that you know how contexts can help you manage environments, see how easy it is to
[set up CI/CD](/deployment/hasura-ddn/ci-cd.mdx) using the CLI and contexts.



==============================



# work-with-multiple-subgraphs.mdx

URL: https://hasura.io/docs/promptql/project-configuration/tutorials/work-with-multiple-subgraphs

# Work with Multiple Subgraphs

## Introduction

Learn how to manage multiple subgraphs to streamline team ownership, enforce clear data boundaries, and scale your
application easily. This tutorial will show you how to structure your project for flexibility and collaboration,
ensuring efficient and adaptable application development.

In this tutorial, you'll learn how to:

- How to organize your project into subgraphs
- How to create a subgraph as part of your local project
- How to create relationships across subgraphs
- How to create a subgraph on a cloud project

We'll demonstrate this by creating a supergraph with two subgraphs: one owned by a `customers` team and another owned by
the `billing` team.

This tutorial should take less than twenty minutes.

## Setup

### Step 1. Initialize a new local project

```ddn title="Create the project directory and move into it:"
ddn supergraph init subgraph-example --with-promptql && cd subgraph-example
```

### Step 2. Add a data source and seed data

In this tutorial, we'll use the PostgreSQL connector and a sample PostgreSQL database:

```plaintext
postgresql://read_only_user:readonlyuser@35.236.11.122:5432/v3-docs-sample-app
```

```ddn title="In your project directory, run the following, choose hasura/postrges, and pass the connection URI above when prompted:"
ddn connector init customers_pg -i
```

### Step 3. Generate the Hasura metadata

```ddn title="Next, use the CLI to introspect your PostgreSQL database:"
ddn connector introspect customers_pg
```

After running this, you should see a representation of your database's schema in the
`app/connector/customers_pg/configuration.json` file; you can view this using `cat` or open the file in your editor.

```ddn title="Now, track the table from your PostgreSQL database as a model in your DDN metadata:"
ddn models add customers_pg users
```

Open the `app/metadata` directory and you'll find a newly-generated file: `Users.hml`. The DDN CLI will use this Hasura
Metadata Language file to represent the `users` table from PostgreSQL in your application as a
[model](/reference/metadata-reference/models.mdx).

### Step 4. Create a new local build and test the application

```ddn title="To create a local build, run:"
ddn supergraph build local
```

The build is stored as a set of JSON files in `engine/build`.

```ddn title="Start your local Hasura instance and PostgreSQL connector:"
ddn run docker-start
```

Your terminal will be taken over by logs for the different services.

```ddn title="In a new terminal tab, open your local console:"
ddn console --local
```

From the PromptQL Playground, talk to your data.

```plaintext title="E.g.:"
Show me all the users in my application.
```

## Add a subgraph

### Step 5. Create a new `billing` subgraph

```ddn title="Use the DDN CLI to create the subgraph in your local metadata:"
ddn subgraph init billing --graphql-type-name-prefix billing
```

You'll see a new `billing` directory added to your local project. The CLI pre-configured this with all the necessary
files and subdirectories to contain data connectors and their metadata.

Additionally, by adding the `--graphql-type-name-prefix` flag, we're ensuring any types generated by the CLI will not
conflict with existing types from our other subgraph. If we had concerns about conflicts of root-level GraphQL fields,
we could also add the flag `--graphql-root-field-prefix`.

```ddn title="Then, add it to your supergraph.yaml config:"
ddn subgraph add billing --subgraph ./billing/subgraph.yaml --target-supergraph ./supergraph.yaml
```

```yaml title="You can verify this by opening the supergraph.yaml in the root of your project. You should see the following:"
kind: Supergraph
version: v2
definition:
  subgraphs:
    - globals/subgraph.yaml
    - app/subgraph.yaml
    # highlight-start
    - billing/subgraph.yaml
    # highlight-end
```

```ddn title="We'll also create a companion subgraph in the cloud project:"
ddn project subgraph create billing
```

This will provision the subgraph as part of the cloud project that was initialized in our first step.

### Step 6. Switch contexts {#switch-contexts}

```ddn title="Switch contexts to the billing subgraph:"
ddn context set subgraph billing/subgraph.yaml
```

This will simplify our subsequent CLI commands as the CLI will now know that — whenever the `--subgraph` flag is
required — we're referencing the `billing` subgraph.

### Step 7. Add a data source and seed data

As before, we'll use the PostgreSQL connector with the same sample database:

```plaintext
postgresql://read_only_user:readonlyuser@35.236.11.122:5432/v3-docs-sample-app
```

```ddn title="In your project directory, run:"
ddn connector init billing_pg -i
```

:::info Notice where this connector was added

Since you created the `billing` subgraph and switched contexts in [Step 6](#switch-contexts), the CLI added the
connector in the `billing` subgraph directory.

:::

### Step 8. Generate the Hasura metadata

```ddn title="Next, use the CLI to introspect the PostgreSQL database:"
ddn connector introspect billing_pg
```

After running this, you should see a representation of the database's schema in the
`billing/connector/billing_pg/configuration.json` file; you can view this using `cat` or open the file in your editor.

```ddn title="Now, track the table from your PostgreSQL database as a model in your DDN metadata:"
ddn models add billing_pg orders
```

Open the `billing/metadata` directory and you'll find a newly-generated file: `PaymentInformation.hml`. The DDN CLI will
use this Hasura Metadata Language file to represent the `payment_information` table from PostgreSQL in your API as a
[model](/reference/metadata-reference/models.mdx).

### Step 9. Create a new local build and test the application

```ddn title="To create a local build, run:"
ddn supergraph build local
```

```sh title="Kill your local services from their terminal tab:"
CTRL+C
```

```ddn title="Restart your local Hasura DDN Engine and PostgreSQL connector:"
ddn run docker-start
```

```ddn title="In a new terminal tab, open your local console:"
ddn console --local
```

From your Console's Explorer view, you should see separate subgraphs organized under your supergraph.

### Step 10. Create a relationship across subgraphs

As our models are in different subgraphs, we'll need to explicitly define a
[relationship](/reference/metadata-reference/relationships.mdx) between these using metadata.

```yaml title="Add the following to your Users.hml file:"
---
kind: Relationship
version: v1
definition:
  name: orders
  sourceType: Users
  target:
    model:
      #highlight-start
      subgraph: billing
      #highlight-end
      name: Orders
      relationshipType: Array
  mapping:
    - source:
        fieldPath:
          - fieldName: id
      target:
        modelField:
          - fieldName: userId
```

By calling out the `billing` subgraph in the relationship object, the
[Hasura VS Code extension](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura) can help you with
validating your configuration.

```ddn title="Then, create a new build:"
ddn supergraph build local
```

```ddn title="And kill your services with CTRL+C before restarting them:"
ddn run docker-start
```

You can now ask PromptQL for information about both customers and orders.

```plaintext title="E.g.:"
Tell me about each customers' order histories.
```

### Step 11. Create a new build on your production project

```ddn title="Finally, create a new build on your cloud project:"
ddn supergraph build create
```

The CLI will return a console URL which you can navigate to; within the `Explorer` tab, you'll find your project's
subgraphs, including `billing`.

## Next steps

In the example above, you learned the steps to organize your project into multiple subgraphs for clearer ownership
between teams. To take this a step further, many teams prefer to implement multi-repository setups wherein a single
subgraph can be added to an existing or private team repository. Learn more
[here](/project-configuration/tutorials/work-with-multiple-repositories.mdx).



==============================



# work-with-multiple-repositories.mdx

URL: https://hasura.io/docs/promptql/project-configuration/tutorials/work-with-multiple-repositories

# Work with Multiple Repositories

## Introduction

Managing multiple repositories lets you distribute development across independent subgraphs located in separate
repositories while maintaining a unified supergraph. This approach provides flexibility for teams to work autonomously
in their respective data domains, while contributing to a single coordinated application.

By organizing your project into multiple repositories, you can iterate and deploy changes efficiently without impacting
other subgraphs, ensuring a smooth and collaborative development experience.

You'll learn:

- How to organize your project into subgraphs
- How to provision a "parent" repo
- How to set up your cloud project for multi-subgraph development
- How to create independent subgraph repositories
- How to create, test, and deploy your supergraph as subgraphs develop independently
- How to create relationships across independent subgraphs

This tutorial takes about thirty minutes.

:::warning DDN Advanced Plan required

In order to utilize multi-repository collaboration, you must have an active
[DDN Advanced Plan](https://hasura.io/pricing).

:::

## Create the initial project

Begin by creating a "parent project" that will serve as the coordinated supergraph for your independent subgraph
repositories. The person creating this will assume the role of **supergraph admin** and have full control over
provisioning subgraphs, inviting collaborators, and managing the API as a whole.

### Step 1. Initialize a new local project

```ddn title="Create a new local project and initialize a git repository:"
ddn supergraph init parent-project --with-promptql && cd parent-project && git init
```

This will scaffold out the local configuration for a DDN project and initialize a git repository.

### Step 2. Create a commit

```sh title="Create an initial commit with your local project mapped to the cloud project via context:"
git add . && git commit -m "Initial commit"
```

### Step 3. Provision subgraphs

We'll add two subgraphs to this supergraph: `customers` and `billing`.

```ddn title="Create the subgraphs on the cloud project:"
ddn project subgraph create customers && ddn project subgraph create billing
```

### Step 4. Create a supergraph build

```ddn title="Create an initial supergraph build:"
ddn supergraph build create
```

This will serve as the foundation for your first **subgraph** build to expand upon.

### Step 5. Invite collaborators

Head to the project's console at [console.hasura.io](https://promptql.console.hasura.io) and navigate to
`Setetings/Collaborators`. Then, invite collaborators based on their role:

| Role               | Description                                                                                                    |
| ------------------ | -------------------------------------------------------------------------------------------------------------- |
| Subgraph Admin     | Users with permissions to create builds and deploy them to the parent project's endpoint.                      |
| Subgraph Developer | Developers responsible for implementing features and making iterative changes to the subgraph's functionality. |

:::info Learn more about collaborators

Learn more about collaborators and roles [here](/project-configuration/project-management/manage-collaborators.mdx).

:::

Each collaborator will receive an email inviting them to your project.

## Add independent subgraphs to the project

Each subgraph will live in its own repository. While this can be an existing repository, we're going to demonstrate
initializing a new repository below.

### Customers

#### Step 1. Create a new repo for the `customers` subgraph

```ddn title="Initialize a new local project and git repo while also mapping to the existing cloud project:"
ddn supergraph init customer-team --create-subgraph customers --with-project <project-name> --with-promptql && cd customer-team && git init
```

This will scaffold out the necessary project structure along with initializing a git repository for version control.

#### Step 2. Set the subgraph context

```ddn title="Change the context to your subgraph's configuration file:"
ddn context set subgraph ./customers/subgraph.yaml
```

This will set your unique subgraph in your `.hasura/context.yaml` file and make the CLI commands below more concise.

#### Step 3. Add prefixing

```sh title="Next, open the customers/subgraph.yaml file and add the following:"
kind: Subgraph
version: v2
definition:
  name: customers
  generator:
    rootPath: .
    namingConvention: snake_case
    #highlight-start
    graphqlRootFieldPrefix: customers_
    graphqlTypeNamePrefix: customers_
    #highlight-end
  includePaths:
    - metadata
```

These will prevent collisions of root fields and types when your supergraph is built.

#### Step 4. Add a data source and generate your first local build

In this tutorial, we'll use the PostgreSQL connector and a sample PostgreSQL database:

```plaintext
postgresql://read_only_user:readonlyuser@35.236.11.122:5432/v3-docs-sample-app
```

```ddn title="In your project directory, run the following, choose hasura/postrges, and pass the connection URI above when prompted:"
ddn connector init customers_pg -i
```

##### Generate the Hasura metadata

```ddn title="Next, use the CLI to introspect the PostgreSQL database:"
ddn connector introspect customers_pg
```

After running this, you should see a representation of your database's schema in the
`customers/connector/customers_pg/configuration.json` file; you can view this using `cat` or open the file in your
editor.

```ddn title="Now, track the table from your PostgreSQL database as a model in your DDN metadata:"
ddn models add customers_pg users
```

Open the `customers/metadata` directory and you'll find a newly-generated file: `Users.hml`. The DDN CLI will use this
Hasura Metadata Language file to represent the `users` table from PostgreSQL in your application as a
[model](/reference/metadata-reference/models.mdx).

##### Create a new local build and test the application

```ddn title="To create a local build, run:"
ddn supergraph build local
```

The build is stored as a set of JSON files in `engine/build`.

```ddn title="Start your local Hasura Engine and PostgreSQL connector:"
ddn run docker-start
```

Your terminal will be taken over by logs for the different services.

```ddn title="In a new terminal tab, open your local console:"
ddn console --local
```

From the PromptQL Playground, you should be able to ask questions about your users.

```plaintext title="E.g.:"
Show me all my customers' IDs and names.
```

#### Step 5. Create a subgraph build

Now that we've tested our local application and are happy with its state, we can create a build of our independent
subgraph on our cloud project.

```ddn title="Create a subgraph build on your cloud project:"
ddn subgraph build create
```

The CLI will return the subgraph's build version; you'll need this value in the next step.

#### Step 6. Create a supergraph build

```ddn title="Begin by listing the available supergraph builds:"
ddn supergraph build get
```

Grab the most recent build's version and use it — along with the subgraph build version — in the following command.

```ddn title="Finally, create a supergraph build incorporating your latest subgraph build:"
ddn supergraph build create --subgraph-version customers:<build-version> --base-supergraph-version <supergraph-build-id>
```

The CLI will return a build URL for the console so you can explore your build and test your subgraph changes with others
before applying the build to the supergraph. In order to apply the build, you must be a **subgraph admin** or higher:

```ddn
ddn supergraph build apply <supergraph-build-version>
```

:::tip Streamline team collaboration For more efficient development across multiple repositories, you can create and
apply the supergraph build in a single command using
`ddn supergraph build create --subgraph-version customers:<build-version> --base-supergraph-version <supergraph-build-id> --apply`
:::

:::warning Make sure to stop all running services

As a multi-repository setup is intended to be utilized by multiple persons across teams, you're not likely to have
multiple instances of your Hasura DDN Engine and other services running at the same time on your machine. Ensure you've
killed all active services before proceeding to the next step for the `billing` subgraph.

:::

### Billing

The steps below will allow you to incorporate your independent `billing` subgraph into your deployed supergraph API.

#### Step 1. Create a new repo for the `billing` subgraph

```ddn title="Initialize a new local project and git repo:"
ddn supergraph init billing-team --with-project <project-name> --create-subgraph billing --with-promptql && cd billing-team && git init
```

This will scaffold out the necessary project structure along with initializing a git repository for version control.

#### Step 2. Set the subgraph context

```ddn title="Change the context to your subgraph's configuration file:"
ddn context set subgraph ./billing/subgraph.yaml
```

This will set your unique subgraph in your `.hasura/context.yaml` file and make the CLI commands below more concise.

#### Step 3. Add prefixing

```sh title="Next, open the billing/subgraph.yaml file and add the following:"
kind: Subgraph
version: v2
definition:
  name: billing
  generator:
    rootPath: .
    namingConvention: snake_case
    #highlight-start
    graphqlRootFieldPrefix: billing_
    graphqlTypeNamePrefix: billing_
    #highlight-end
  includePaths:
    - metadata
```

Like before, this will ensure the types an fields are namespaced to your subgraph.

#### Step 4. Add a data source and generate your first local build

As before, we'll use the PostgreSQL connector with our sample database:

```plaintext
postgresql://read_only_user:readonlyuser@35.236.11.122:5432/v3-docs-sample-app
```

```ddn title="In your project directory, run:"
ddn connector init billing_pg -i
```

##### Generate the Hasura metadata

```ddn title="Next, use the CLI to introspect the PostgreSQL database:"
ddn connector introspect billing_pg
```

After running this, you should see a representation of your database's schema in the
`billing/connector/billing_pg/configuration.json` file; you can view this using `cat` or open the file in your editor.

```ddn title="Now, track the table from the PostgreSQL database as a model in your DDN metadata:"
ddn models add billing_pg orders
```

Open the `billing/metadata` directory and you'll find a newly-generated file: `Orders.hml`. The DDN CLI will use this
Hasura Metadata Language file to represent the `orders` table from PostgreSQL in your application as a
[model](/reference/metadata-reference/models.mdx).

##### Create a new local build and test the API

```ddn title="To create a local build, run:"
ddn supergraph build local
```

The build is stored as a set of JSON files in `engine/build`.

```ddn title="Start your local Hasura Engine and PostgreSQL connector:"
ddn run docker-start
```

Your terminal will be taken over by logs for the different services.

```ddn title="In a new terminal tab, open your local console:"
ddn console --local
```

You can test this by visiting the PromptQL Playground and asking questions about your orders.

```plaintext title="E.g.:"
Give me the ID, status, and when each order was created.
```

#### Step 5. Create a subgraph build

```ddn title="Once your local metadata is in its desired state, create a subgraph build on your cloud project:"
ddn subgraph build create
```

The CLI will return the subgraph's build version; you'll need this value in the next step.

#### Step 7. Create a supergraph build

```ddn title="Begin by listing the available supergraph builds:"
ddn supergraph build get
```

Grab the most-recent build's version and use it — along with the subgraph build version — in the following command.

```ddn title="Finally, create a supergraph build incorporating your latest subgraph build:"
ddn supergraph build create --subgraph-version billing:<build-version> --base-supergraph-version <supergraph-build-id>
```

The CLI will return a build URL for the console so you can explore your build and test your subgraph changes with others
before applying the build to the supergraph. In order to apply the build, you must be a **subgraph admin** or higher:

```ddn
ddn supergraph build apply <supergraph-build-version>
```

:::tip Streamline team collaboration For more efficient development across multiple repositories, you can create and
apply the supergraph build in a single command using
`ddn supergraph build create --subgraph-version billing:<build-version> --base-supergraph-version <supergraph-build-id> --apply`
:::

:::warning Make sure to stop all running services

As a multi-repository setup is intended to be utilized by multiple persons across teams, you're not likely to have
multiple instances of your Hasura DDN Engine and other services running at the same time on your machine. Ensure you've
killed all active services before proceeding to the next step.

:::

## Add a cross-subgraph relationship

**We can create relationships across subgraphs that are testable on a Hasura Cloud project**. As your locally-running
instance will only have access to your local subgraphs and their accompanying metadata, you'll need to define
relationships using your knowledge of other subgraphs and then test them using a supergraph build in your hosted
environment.

```yaml title="Add the following relationship object to your Users.hml file in your customer-team repo:"
---
kind: Relationship
version: v1
definition:
  name: orders
  sourceType: Users
  target:
    model:
      subgraph: billing
      name: Orders
      relationshipType: Array
  mapping:
    - source:
        fieldPath:
          - fieldName: id
      target:
        modelField:
          - fieldName: userId
```

```ddn title="Create a new subgraph build:"
ddn subgraph build create
```

```ddn title="Get the list of supergraph builds:"
ddn supergraph build get
```

```ddn title="Then, create a supergraph build incorporating your latest subgraph build:"
ddn supergraph build create --subgraph-version customers:<build-version> --base-supergraph-version <supergraph-build-id>
```

Now, you can ask PromptQL about customers and their orders:

```plaintext title="E.g.:"
Provide me with a listing of all customers and their orders.
```

## Recap

By organizing your project into multiple repositories, you can create a flexible and collaborative workflow for subgraph
development in Hasura. Starting with a parent project, you learned how to provision subgraphs, invite collaborators, and
manage builds to integrate subgraph changes into a unified supergraph. This structure ensures teams can work
independently while maintaining seamless integration and coordination across the entire application.



==============================



# remove-subgraph.mdx

URL: https://hasura.io/docs/promptql/project-configuration/tutorials/remove-subgraph

# Remove a subgraph

## Introduction

In this recipe, you'll learn how to remove a subgraph from your local project directory.

:::info Prerequisites

Before continuing, ensure you have:

- A [local Hasura project](/quickstart.mdx).
- Stopped any running docker services related to the project.

:::

## Recipe

### Step 1. Delete subgraph directory

Delete the directory containing the subgraph related config files, connectors and metadata of the subgraph. The subgraph
directory is typically located at `<subgraph-name>`.

### Step 2. Update supergraph config files

Remove the path to the subgraph config files in all [supergraph config files](/project-configuration/overview.mdx)
located at the project root, i.e., `<project-root>/supergraph.yaml`.

```yaml title="supergraph.yaml"
kind: Supergraph
version: v2
definition:
  subgraphs:
    - globals/subgraph.yaml
    #highlight-start
    - <subgraph-name>/subgraph.yaml
    #highlight-end
    ...
```

### Step 3. Update engine compose file

Remove references to any compose files of connectors in the deleted subgraph from the engine compose file. The engine
compose file is typically located at `<project-root>/compose.yaml`.

```yaml title="<project-root>/compose.yaml"
include:
  #highlight-start
  - path: <subgraph-name>/connector/<connector-1>/compose.yaml
  - path: <subgraph-name>/connector/<connector-2>/compose.yaml
  #highlight-end
  ...
services:
  engine: ...
```

### Step 4. Remove subgraph config file from context

The [context config file](/project-configuration/overview.mdx) subgraph config file path saved in the context. Remove
the `subgraph` key if set as the deleted subgraph config file.

```yaml title=".hasura/context.yaml"
kind: Context
version: v3
definition:
  current: default
  contexts:
    default:
      supergraph: ../supergraph.yaml
      #highlight-start
      subgraph: ../<subgraph-name>/subgraph.yaml
      #highlight-end
      ...
```

### Step 5. (Optional) Remove subgraph relevant environment variables

You can remove the environment variables that were defined for your subgraph from the env files that you might have. The
CLI-generated environment variables for a subgraph typically start with the `<SUBGRAPH_NAME>_` prefix.

```.env title="For example, .env"
...
#highlight-start
<SUBGRAPH_NAME>_<CONNECTOR>_READ_URL="<connector-read-url>"
<SUBGRAPH_NAME>_<CONNECTOR>_WRITE_URL="<connector-write-url>"
<SUBGRAPH_NAME>_<CONNECTOR>_AUTHORIZATION_HEADER="Bearer <roken>"
#highlight-end
...
```

## Learn more

- [Project configuration](/project-configuration/overview.mdx)



==============================



# rename-subgraph.mdx

URL: https://hasura.io/docs/promptql/project-configuration/tutorials/rename-subgraph

# Rename a subgraph

## Introduction

In this recipe, you'll learn how to rename a subgraph in your local project directory.

:::info Prerequisites

Before continuing, ensure you have:

- A [local Hasura DDN project](/quickstart.mdx).
- Stopped any running docker services related to the project.

:::

## Recipe

### Step 1. Update subgraph config files

Update the name of the subgraph in all the [subgraph config files](/project/configuration/overview.mdxthe subgraph. The
subgraph config is typically located at `<subgraph-name>/subgraph.yaml`.

```yaml title="<subgraph-name>/subgraph.yaml"
kind: Subgraph
version: v2
definition:
  #highlight-start
  name: <new-subgraph-name>
  #highlight-end
  generator:
    rootPath: .
  includePaths:
    - metadata
  envMapping: ...
```

### Step 2. Update connector config files of the subgraph

Update the reference of the subgraph in all the [connector config files](/project/configuration/overview.mdxsubgraph.
The connector config is typically located at `<subgraph-name>/connector/<connector-name>/connector.yaml`.

```yaml title="<subgraph-name>/connector/<connector-name>/connector.yaml"
kind: Connector
version: v2
definition:
  name: <connector-name>
  #highlight-start
  subgraph: <new-subgraph-name>
  #highlight-end
  source: hasura/<connector-type>
  context: .
  envMapping: ...
```

### Step 3. Update compose files of connectors of the subgraph

Update the name of the connector service in the [compose file](/project/configuration/overview.mdxthe connectors
belonging to the subgraph. The connector compose file is typically located at
`<project-root>/<subgraph-name>/connector/<connector-name>/compose.yaml`.

```yaml title="<subgraph-name>/connector/<connector-name>/compose.yaml"
services:
  #highlight-start
  <new-subgraph-name>_<connector-name>:
    #highlight-end
    build:
      context: .
      dockerfile_inline: ...
```

### Step 4. (Optional) Update subgraph directory name

The subgraph directory is typically named as the subgraph name itself. The directory name does not impact any
functionality, but you might want to rename it to maintain consistency. The subgraph directory is typically located at
the project root, i.e., `<project-root>/<subgraph-name>/`.

Rename the directory to `<project-root>/<new-subgraph-name>/`.

Renaming the subgraph directory name will require updates to any references to files within the directory. These should
typically be in:

#### Step 4.1. Supergraph config files

Update the path to the subgraph config files in all [supergraph config files](/project/configuration/overview.mdxlocated
at the project root, i.e., `<project-root>/supergraph.yaml`.

```yaml title="supergraph.yaml"
kind: Supergraph
version: v2
definition:
  subgraphs:
    - globals/subgraph.yaml
    #highlight-start
    - <new-subgraph-name>/subgraph.yaml
    #highlight-end
    ...
```

#### Step 4.2. Engine compose file

Update the path to the connector compose files included in the engine [compose
file](/project/configuration/overview.mdxproject root, i.e., `<project-root>/compose.yaml`.

```yaml title="compose.yaml"
include:
  #highlight-start
  - path: <new-subgraph-name>/connector/<connector-1>/compose.yaml
  - path: <new-subgraph-name>/connector/<connector-2>/compose.yaml
  #highlight-end
  ...
services:
  engine:
    build:
    ...
```

#### Step 4.3. Context config

The [context config file](/project/configuration/overview.mdxsubgraph config file path saved in the context. Update the
subgraph config path if set.

```yaml title=".hasura/context.yaml"
kind: Context
version: v3
definition:
  current: default
  contexts:
    default:
      supergraph: ../supergraph.yaml
      #highlight-start
      subgraph: ../<new-subgraph-name>/subgraph.yaml
      #highlight-end
      ...
```

## Learn more

- [Project configuration](/project/configuration/overview.mdx



==============================



# independent-connector-deployment.mdx

URL: https://hasura.io/docs/promptql/project-configuration/tutorials/independent-connector-deployment

# Deploy connectors and supergraph independently

## Introduction

By default, the CLI command to build the supergraph, i.e.
[ddn supergraph build create](/reference/cli/commands/ddn_supergraph_build_create.mdx), also builds the connectors
required by the supergraph for convenience. Though in certain cases, you might want to do these steps independently,
e.g. when the connectors are self-hosted and not deployed on Hasura DDN.

In this recipe, you'll learn how to manage independent deployments for your connectors and the supergraph.

:::info Prerequisites

Before continuing, ensure you have:

- A [local Hasura project](/quickstart.mdx).

:::

## Recipe

### Basics

The supergraph interacts with a connector via the
[DataConnectorLink](/reference/metadata-reference/data-connector-links.mdx) metadata object which contains the NDC
schema of the connector along with the URLs where the connector is running.

Hence, to independently manage the connector and supergraph deployments, we need to ensure that the corresponding
DataConnectorLink object is kept in sync with the connector.

### Step 1. Build the connector

Build and deploy your connector and note the:

- Read url, say `<deployed-connector-read-url>`
- Write url, say `<deployed-connector-write-url>`
- Authorization header, say `Bearer <deployed-connector-token>`

For example, to build the connector on DDN:

```ddn title="Build connector on DDN using config at <subgraph-name>/connector/<connector-name>/connector.yaml"
ddn connector build create --connector <subgraph-name>/connector/<connector-name>/connector.yaml \
  --env-file .env.cloud \
  --project <project-name>
```

:::info

The `--project` and `--env-file` flags can be skipped if the keys `project` and `cloudEnvFile` are set in the context.

:::

### Step 2. Update connector URLs and Auth header in the data connector link

Update the environment variables that correspond to the connector read, write URLs and the Authorization header in the
data connector link:

```yaml title="For example: <subgraph-name>/metadata/<connector-link-name>.hml"
kind: DataConnectorLink
version: v1
definition:
  name: <connector-link-name>
  #highlight-start
  url:
    readWriteUrls:
      read:
        valueFromEnv: <CONNECTOR>_READ_URL
      write:
        valueFromEnv: <CONNECTOR>_WRITE_URL
  headers:
    Authorization:
      valueFromEnv: <CONNECTOR>_AUTHORIZATION_HEADER
  #highlight-end
  schema: ...
```

Update the environment variables in the corresponding env file:

```env title="For example: .env.cloud"
...
#highlight-start
<CONNECTOR>_READ_URL="<deployed-connector-read-url>"
<CONNECTOR>_WRITE_URL="<deployed-connector-write-url>"
<CONNECTOR>_AUTHORIZATION_HEADER="Bearer <deployed-connector-token>"
#highlight-end
...
```

### Step 3. Build the supergraph without connectors

Build the supergraph to get a supergraph build using the connector deployed above:

```bash title="Create supergraph build on DDN without building the related connectors:"
#highlight-start
ddn supergraph build create --no-build-connectors \
#highlight-end
  --supergraph supergraph.yaml \
  --env-file .env.cloud \
  --project <project-name>
```

:::info

The `--project`, `--supergraph` and `--env-file` flags can be skipped if the keys `project`, `supergraph` and
`cloudEnvFile` are set in the context.

:::

### Step 4. (Optional) Add a custom script to build the supergraph

You can add the above command as a [custom script](/project/configuration/overview.mdxthe `--no-build-connectors` flag
each time.

For example, you can update your context config to the following:

```yaml title="<project-root>/.hasura/context.yaml:"
kind: Context
version: v3
definition:
  current: default
  contexts:
    default: ...
  scripts:
    docker-start:
      bash: HASURA_DDN_PAT=$(ddn auth print-access-token) docker compose --env-file .env up --build --pull always -d
      powershell:
        $Env:HASURA_DDN_PAT = (ddn auth print-access-token); docker compose --env-file .env up --build --pull always -d
    #highlight-start
    build-supergraph:
      bash:
        ddn supergraph build create --no-build-connectors --supergraph supergraph.yaml --env-file .env.cloud --project
        <project-name>
      powershell:
        ddn supergraph build create --no-build-connectors --supergraph supergraph.yaml --env-file .env.cloud --project
        <project-name>
    #highlight-end
```

Now you can run the following command to build the supergraph:

```ddn
ddn run build-supergraph
```

:::info

The `--project`, `--supergraph` and `--env-file` flags can be skipped if the keys `project`, `supergraph` and
`cloudEnvFile` are set in the context.

:::

## Learn more

- [Project configuration](/project/configuration/overview.mdx



==============================



# index.mdx

URL: https://hasura.io/docs/promptql/project-configuration/promptql-config/

# PromptQL Configuration

## Overview

You can manage PromptQL's LLM settings and system instructions from a single `promptql_config.yaml` file, automatically
created at the root of your DDN project when you initialize a project with the `--with-promptql` flag using the Hasura
DDN CLI.

## Examples

```yaml title="Minimal configuration:"
kind: PromptQlConfig
version: v1
definition:
  llm:
    provider: hasura
```

```yaml title="Custom providers for LLM & AI primitives and custom system instructions:"
kind: PromptQlConfig
version: v1
definition:
  llm:
    provider: openai
    model: o3-mini
  ai_primitives_llm:
    provider: openai
    model: gpt-4o
  system_instructions: |
    You are a helpful AI Assistant.
```

With this file, you can:

- Set the LLM provider and model used across the application.
- Define a separate LLM for AI Primitives such as **Classification**, **Summarization**, and **Extraction**.
- Add system instructions that apply to every PromptQL interaction.

## Next steps

- [Check out how to configure your LLM of choice](/project-configuration/promptql-config/providers.mdx).
- [Learn how to improve performance by using system instructions](/project-configuration/promptql-config/system-instructions.mdx).



==============================



# providers.mdx

URL: https://hasura.io/docs/promptql/project-configuration/promptql-config/providers

# Providers & Models

## Introduction

PromptQL supports configuring different LLM providers and models to tailor the experience to your application's needs.

This gives you the flexibility to choose the most performance-efficient and cost-effective solution for your use case,
and the freedom to switch between providers and models as needed depending the task.

For example, you can use one model for conversational tasks and another for advanced AI primitives:

```yaml {5-6,8-9}
kind: PromptQlConfig
version: v1
definition:
  llm:
    provider: openai
    model: o3-mini
  ai_primitives_llm:
    provider: openai
    model: gpt-4o
```

### `llm`

The `llm` configuration is used to define the LLM provider and model for **conversational tasks** in your application.
In the example above, we're using `openai` as the provider with the `o3-mini` model.

### `ai_primitives_llm`

The `ai_primitives_llm` configuration is used to define the LLM provider and model for
[**AI primitives** in your application](/promptql-apis/execute-program-api.mdx). This is used for tasks such as
**program generation and execution**. In the example above, we're using `openai` as the provider with the `gpt-4o`
model.

## Available providers & models

### Anthropic

To use an [Anthropic model](https://docs.anthropic.com/en/docs/about-claude/models/all-models), set the `provider` to
`anthropic`. The following have been tested with PromptQL:

- `claude-3-5-sonnet-latest`
- `claude-3-7-sonnet-latest`

### AWS Bedrock

To use a [Bedrock-wrapped model](https://aws.amazon.com/bedrock/), set the `provider` to `bedorck`. The following have
been tested with PromptQL:

- Claude 3.5 Sonnet
- Claude 3.7 Sonnet

**NB: For Bedrock models, you'll need to provide a `model_id` that resembles this string:**

```
arn:aws:bedrock:<AWS region>:<AWS account ID>:inference-profile/us.anthropic.claude-3-5-sonnet-20241022-v2:0
```

### Google Gemini

To use a [Google Gemini model](https://ai.google.dev/gemini-api/docs/models), set the `provider` to `gemini`. The
following have been tested with PromptQL:

- `gemini-1.5-flash`
- `gemini-2.0-flash`

### Hasura

With Hasura—used as the default provider—there is **no specific model necessary** in your configuration.

### Microsoft Azure

To use an [Azure foundational model](https://azure.microsoft.com/en-us/products/ai-model-catalog#Models), set the
`provider` to `azure`.

### OpenAI

To use an [OpenAI model](https://platform.openai.com/docs/models), set the `provider` to `openai`. The following have
been tested with PromptQL:

- `o1`
- `o3-mini`
- `gpt-4o`

:::info Base Model

When using the `hasura` provider, the default model is `claude-3-5-sonnet-latest`. This is the recommended model for
PromptQL program generation.

:::

## Considerations

- The `model` key is **not supported** when using the `hasura` provider.
- The value for a `model` key is always in the dialect of the provider's API.
- If `ai_primitives_llm` is not defined, it defaults to the provider specified in the `llm` configuration.
- [`system_instructions` are optional](/project-configuration/promptql-config/system-instructions.mdx) but recommended
  to customize the behavior of your LLM.



==============================



# system-instructions.mdx

URL: https://hasura.io/docs/promptql/project-configuration/promptql-config/system-instructions

# System Instructions

## Introduction

Custom system instructions provide flexibility to fine-tune your PromptQL experience. This optional configuration allows
you to tailor the application's behavior to your specific needs. They act as a guide for the model, shaping how it
interprets questions and crafts responses.

**While powerful, use this feature judiciously to maintain optimal performance.**

```yaml title="Consider this example where we set the system instructions using the PromptQlConfig file:"
kind: PromptQlConfig
version: v1
definition:
  llm:
    provider: hasura
  system_instructions: |

    You are an AI assistant that helps go-to-market teams make data-driven decisions.

    You have access to data from Salesforce (opportunities, accounts, leads), Clari (forecast categories, pipeline movement), and Postgres (custom product and revenue data). 

    Use this data to answer questions about sales performance, forecast accuracy, pipeline health, rep activity, and account trends. Prioritize clarity, brevity, and business relevance in your answers.

    If a question is ambiguous, ask for clarification.
```

:::info Configuring system instructions on cloud builds

The steps above will set the system instructions for projects during local development. To set the system instructions
for cloud deployments, navigate to your `Settings » PromptQL` and update the `System Instructions` field.

:::

## Best practices

When writing system instructions, your goal is to **narrow the model's focus** and **reduce ambiguity**. Here are some
guidelines.

- **Be clear and concise:** The model performs better when instructions are direct and avoid unnecessary complexity.
- **Define the role and audience:** Tell the model who it is (e.g., a sales analyst, a support bot) and who it is
  helping.
- **Specify sources of knowledge:** Mention any data sources, systems, or domains the model should rely on when
  answering.
- **Encourage appropriate behavior:** Highlight priorities like brevity, clarity, formality, creativity, or
  error-handling.
- **Provide code patterns or examples:** You can include sample Python functions, formulas, or logic patterns to guide
  how the model generates its own code. These are not backend functions to call, but templates for the LLM to adapt and
  build from.
- **Anticipate ambiguity:** If the model might face unclear inputs, instruct it on when and how to ask for
  clarification.
- **Set limits if needed:** If certain topics, behaviors, or types of output should be avoided, explicitly mention them.
- **Test and iterate:** Start simple, evaluate outputs, and refine your instructions based on observed model behavior.

:::info System instructions shape behavior

As outlined above, system instructions shape the behavior of a PromptQL application.

However, through connecting data sources and writing your own custom business logic, you're actively providing new tools
which PromptQL can use. While context in the form of system instructions is helpful, you can explain with greater
detail—and with much tighter scope—what each tools does using metadata descriptions.

Learn more [here](/data-modeling/semantic-information.mdx).

:::



==============================



# supergraph.mdx

URL: https://hasura.io/docs/promptql/project-configuration/supergraph

# Supergraph

## Introduction

A supergraph is the foundation that enables PromptQL to accurately talk to all your data. It provides a unified
structure for your data sources, allowing PromptQL to generate insightful queries and visualizations through a natural
language conversation.

## Components

The supergraph is made up of several key components that work together to deliver a seamless data interaction
experience:

- **Subgraphs**: These represent different data domains and can include multiple data connectors to bring in data from
  various sources.
- **Builds**: A supergraph consists of immutable builds. At any given time, one build is applied, and it can be easily
  rolled back to a previous state if necessary.

## How it all works

The supergraph is defined using Hasura Metadata Language, which PromptQL uses to understand and interact with your data.
This metadata acts as the blueprint for everything, including:

- Defining roles and permissions to control access.
- Configuring authentication mechanisms.
- Defining relationships across and between data sources.

By centralizing these configurations, the supergraph ensures PromptQL can provide accurate insights and meaningful
visualizations from your data through natural conversations, enabling you to take action based on these insights.

## Next steps

- [Learn more about subgraphs](/project-configuration/subgraphs/index.mdx)



==============================



# index.mdx

URL: https://hasura.io/docs/promptql/project-configuration/subgraphs/

# Subgraphs

## Introduction

A subgraph is a focused component of your overall supergraph, typically organized around specific data domains or
business functions.

Subgraphs are often team-specific, aligning with the responsibilities and expertise of individual teams. This structure
not only enables efficient development but also makes it easier to onboard new teams as your unified supergraph evolves.
By defining clear boundaries and responsibilities, subgraphs allow new teams to integrate seamlessly without impacting
existing functionality, fostering a scalable and collaborative development environment.

## Organization

Subgraphs provide flexibility and modularity in your data access strategy. A single project can include one or more
subgraphs, depending on its complexity and the distribution of responsibilities among teams. Subgraphs are designed to
be iterated on and developed independently, allowing teams to work at their own pace and with their own preferred tools
and languages while ensuring PromptQL maintains a consistent and accurate understanding of your entire data ecosystem.

In setups with multiple repositories, subgraphs also enhance governance by limiting access. Individual developers or
teams are only granted permissions to their specific subgraph, ensuring they cannot modify or disrupt other parts of the
data structure. This separation not only protects the integrity of the overall system but also enables streamlined
collaboration across teams by maintaining a well-defined scope of access, which is crucial for building a reliable
foundation for PromptQL's data interactions.

## Next steps

- [Learn how to create a subgraph](/project-configuration/subgraphs/create-a-subgraph.mdx)
- [Learn how to establish relationships across subgraphs to unify your data for PromptQL access](/project-configuration/subgraphs/working-with-multiple-subgraphs.mdx)
- [Learn how to split subgraphs across repositories to enable decentralized development](/project-configuration/subgraphs/working-with-multiple-repositories.mdx)



==============================



# create-a-subgraph.mdx

URL: https://hasura.io/docs/promptql/project-configuration/subgraphs/create-a-subgraph

# How to Create a Subgraph

## Introduction

You can easily create subgraphs using the CLI. By default, the CLI will generate an `app` subgraph when a new project is
initialized.

## In your local metadata {#local}

```ddn title="Within a local project directory, run the following:"
ddn subgraph init <subgraph-name>
```

You can verify this by identifying the new subdirectory in your project with the subgraph's name.

```ddn title="Then, add the subgraph to your supergraph's config file:"
ddn subgraph add <subgraph-name> --subgraph ./<subgraph-name>/subgraph.yaml --target-supergraph ./supergraph.yaml
```

```sh title="This will add the new subgraph to your supergraph.yaml:"
kind: Supergraph
version: v2
definition:
  subgraphs:
    - globals/subgraph.yaml
    - app/subgraph.yaml
    # highlight-start
    - <subgraph-name>/subgraph.yaml
    # highlight-end
```

## On a cloud project {#cloud}

By default, when a cloud project is initialized, any subgraphs present in your supergraph configuration will be
initialized as well. However, to explicitly add a new subgraph to a cloud project — such as when iterating on an
existing project — follow these steps:

```ddn title="The folowing will create a new subgraph on the current context's cloud project:"
ddn project subgraph create <subgraph-name>
```

:::info Subgraph mapping

Be careful of mismatching subgraphs between your local metadata and a cloud project: if you create a subgraph locally
and add it to your supergraph configuration without also creating the subgraph on your cloud project, cloud builds will
fail.

To resolve this, ensure that when you create a local subgraph in your metadata, you also create the companion subgraph
on the cloud project.

:::

## Next steps

- [Check out an end-to-end tutorial for working with multiple subgraphs](/project-configuration/tutorials/work-with-multiple-subgraphs.mdx)
- [Learn how to work with multiple subgraphs in a project](/project-configuration/subgraphs/working-with-multiple-subgraphs.mdx)



==============================



# working-with-multiple-subgraphs.mdx

URL: https://hasura.io/docs/promptql/project-configuration/subgraphs/working-with-multiple-subgraphs

# How to Work with Multiple Subgraphs

## Introduction

In your daily development, you'll often encounter scenarios where your project requires multiple subgraphs to represent
distinct domains. Whether you're working in a single repository or planning to split subgraphs across multiple
repositories for team independence, understanding how to manage multiple subgraphs is crucial.

We'll walk you through the best practices for working with multiple subgraphs, setting their context, managing
namespacing, and establishing relationships across them to create a unified supergraph.

## Subgraph Namespacing

Each subgraph in your project operates in its own namespace, which means internal metadata objects (like models,
relationships, and permissions) cannot conflict with other subgraphs. However, interacting via PromptQL is where all
subgraphs come together, and conflicts can occur with root field and type names.

To prevent naming conflicts, you can:

1. Use subgraph prefixing (recommended for multi-team setups)
2. Manually ensure unique names across subgraphs
3. Use explicit type names in your schema

For more information about prefixing, see the
[subgraph prefixing guide](/project-configuration/subgraphs/subgraph-prefixing.mdx).

## Set subgraph context

If you're working in a project with multiple subgraphs, there will be the need to switch between subgraphs when
executing commands with the DDN CLI. While the CLI supports a `--subgraph` flag, it is much more convenient to set your
subgraph in context.

```ddn title="Switch between context using the path to the subgraph's configuration file:"
ddn context set subgraph ./<subgraph-name>/subgraph.yaml
```

```ddn title="You can verify this by checking the current context:"
ddn context get subgraph
```

Which should return the path you entered. This will ensure any commands you run — such as adding a new data connector —
are run in the appropriate subgraph.

## Relationships across subgraphs

Relationships across subgraphs work in nearly the exact same way as relationships within a single subgraph; the only
caveat is to add the `subgraph` name of the target type.

```yaml title="In the example below, we're creating a simple relationship between models, with the target model belonging to the billing subgraph:" {8}
kind: Relationship
version: v1
definition:
  name: orders
  sourceType: Users
  target:
    model:
      subgraph: billing
      name: Orders
      relationshipType: Array
  mapping:
    - source:
        fieldPath:
          - fieldName: id
      target:
        modelField:
          - fieldName: userId
```

### Single-repo relationships

In a single-repo setup, relationships are straightforward to manage. All subgraphs are in the same repository and the
[Hasura VS Code extension](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura) can be used to assist
with authoring relationships, providing auto-complete and validation.

### Cross-repo relationships {#cross-repo-relationships}

:::info Advanced plan

You will need a project on the [DDN Advanced plan](https://hasura.io/pricing) to use multi-repo federation and
cross-repo relationships.

:::

In a [multi-repo setup](/project-configuration/subgraphs/working-with-multiple-repositories.mdx), while working with an
independent subgraph in its own repository, you may want to relate objects that reside in different repositories, some
of which you may not have access to.

In these cases the Hasura VS Code extension cannot validate the entirety of the `Relationship` object and you will
manually author cross-repo relationships and ensure that the field mappings are correct. However, upon creating a
supergraph build, all cross-subgraph metadata is validated to prevent mistakes from being deployed to the final API.

You can still easily use the Hasura DDN console to explore the supergraph and test relationships across subgraphs once
you have created a build.

If you perform a _local_ supergraph build using the CLI (ie. `ddn supergraph build local`), cross-repo relationships
will be ignored and will not be validated. If you run the build locally you will only see the subgraphs in that
repository, and any relationships to subgraphs from other repositories will be missing.

### Example

Let's say you have a supergraph with two subgraphs, each managed in different repositories: `users` and `products`.

The `users` subgraph in repo 'A' has a `User` type with a field called `user_favorite_product_id`.

The `products` subgraph in repo 'B' has a `Product` type with a field called `id`.

To create a relationship between these two types in different repositories, you would create a `Relationship` object in
the `users` subgraph metadata as normal.

The LSP is able to understand that the `Product` type is in a different subgraph to which it does not have access and
will not give a warning on the foreign type.

```yaml
kind: Relationship
version: v1
definition:
  name: favorite_product
  sourceType: User
  target:
    model:
      name: Product
      subgraph: products
      relationshipType: Object
  mapping:
    - source:
        fieldPath:
          - fieldName: user_favorite_product_id
      target:
        modelField:
          - fieldName: id
```

This `Relationship` object defines a relationship called `favorite_product` from the `User` type to the `Product` type.
The `mapping` field specifies how the `user_favorite_product_id` field in the `User` type maps to the `id` field in the
`Product` type.

After defining the cross-repo relationship, it's important to note that you won't be able to test this locally. To see
the relationship in action, you'll need to follow these steps:

1. Create a new supergraph build on DDN using the `ddn supergraph build create` command. (Subgraph builds do not get an
   API, so supergraph builds are required to test.)
2. You can then use the Hasura DDN console to explore and test the relationship across subgraphs.
3. If you have admin permissions, you can apply the subgraph to the supergraph with the `ddn subgraph apply` command.

Remember, cross-repo relationships only come into effect when the subgraphs are combined in the DDN environment. Local
development and testing are limited to the scope of your current repository.

With this relationship defined, you can now ask PromptQL about the `favorite_product` field on the `User` type to
retrieve the related `Product`.

## Next steps

- [Learn how to split subgraphs across repositories](/project-configuration/subgraphs/working-with-multiple-repositories.mdx)

:::info Multi-Repository Development

For larger teams, you can split subgraphs across multiple repositories to enable independent development lifecycles.
This advanced feature requires the DDN Advanced plan. Learn more in our guide about
[splitting subgraphs across repositories](/project-configuration/subgraphs/working-with-multiple-repositories.mdx).

:::



==============================



# working-with-multiple-repositories.mdx

URL: https://hasura.io/docs/promptql/project-configuration/subgraphs/working-with-multiple-repositories


# How to Split Subgraphs across Repositories

## Introduction

For larger teams and larger projects, it can make sense to split your subgraphs into multiple repositories contributing
to the supergraph in the "parent" project. This approach enables teams to have independent software development
lifecycles (SDLC) and CI/CD pipelines, providing better governance and control over their specific domains.

In this setup, one or more subgraphs are defined in each repository and depending on the role of the user on the cloud
project, can be built and applied to the supergraph independently. Each subgraph typically represents a distinct data
domain (e.g., users, orders, products) and can be managed by different teams.

:::tip Key Concepts

- Supergraph and subgraph builds are immutable and have unique IDs
- A subgraph must exist in the DDN project before inviting collaborators to it
- Each subgraph is namespaced and internal metadata objects cannot conflict with other subgraphs
- The supergraph is where subgraphs meet and conflicts can occur with root field and type names
- [Subgraph prefixing](/project-configuration/subgraphs/subgraph-prefixing.mdx) can automatically remedy naming
  conflicts

:::

Deployment with a multi-repository setup is effectively the same as
[deploying from a single repository](/deployment/hasura-ddn/index.mdx), but with the considerations of managing your
context to reference the shared project, and also being aware of the differences in abilities of the users building and
deploying.

See the tables [below](#invite-collaborators) for the roles and permissions of users on a Hasura DDN project.

:::warning DDN Advanced Plan required

In order to utilize multi-repository collaboration, you must have an active
[DDN Advanced Plan](https://hasura.io/pricing).

:::

## How it works

### Parent Project:

The project `Owner` or `Admin` will need to:

1. Initialize a new local parent project
2. Initialize the corresponding cloud parent project
3. Initialize git for the parent project
4. Provision subgraphs on the cloud parent project
5. Create a base build of the cloud parent project
6. Invite collaborators to the cloud parent project

### Independent Subgraphs:

The invited user will need to:

1. Create a new supergraph with a new subgraph and initialize a new repository
2. Initialize their cloud project as the parent project and link the new subgraph
3. Set the subgraph context
4. Create a subgraph build
5. Integrate the subgraph build into the parent project
6. Apply the build to be the official supergraph API (`Owner`, `Admin`, `Subgraph Admin` roles)

## Create the Initial Parent Project

Begin by creating a "parent" project that will serve as the coordinated supergraph for your independent subgraph
repositories. This central project will manage provisioning subgraphs, inviting collaborators, and maintaining the
overall API.

### Step 1. Initialize a new local project

This will serve as the parent project.

```ddn
ddn supergraph init <parent-project> && cd <parent-project> && git init
```

This will scaffold the local configuration for your DDN project and initialize a Git repository.

### Step 2. Initialize the cloud project

This is based on the local parent project.

```ddn
ddn project init
```

In your configuration file (e.g., `.hasura/context.yaml`), you'll see a new `project` entry with the name of the project
returned by the CLI.

### Step 3. Create an initial commit

```sh
git add . && git commit -m "Initial commit"
```

Push this repository to your preferred hosting service to share it with collaborators.

### Step 4. Provision subgraphs on the cloud parent project

If you know the subgraphs to include, you can provision them using the DDN CLI. Replace `<subgraph-name>` with the
desired name:

```ddn
ddn project subgraph create <subgraph-name>
```

You are also able to add a subgraph on the console in the `Share > Invite a user > Granular access` section.

:::info On-Demand Subgraphs

Subgraphs can be added as needed when collaborators are onboarded.

:::

### Step 5. Create a base build of the cloud parent project

```ddn
ddn supergraph build create
```

This initial build serves as the foundation for future subgraph builds.

### Step 6. Invite collaborators to the cloud parent project {#invite-collaborators}

Navigate to your project's console and invite collaborators based on their roles:

| Role                      | Abilities                                                                                            |
| ------------------------- | ---------------------------------------------------------------------------------------------------- |
| **Owner**                 | All Project abilities including deletion. At this time, project ownership is not transferable.       |
| **Admin**                 | Same as a Project Owner, excluding deletion of the project.                                          |
| **Read Only**             | Only explore and visualize the supergraph.                                                           |
| **Subgraph Admin** \*     | All subgraph development-related abilities: create subgraph build, apply subgraph build to endpoint. |
| **Subgraph Developer** \* | Same as a subgraph admin, excluding the ability to apply subgraph build to endpoint.                 |

\* Subgraph roles are only available on [DDN Advanced projects](https://hasura.io/pricing/ddn).


The following are the detailed permissions for the above roles:

| Permissions                                                         | Owner | Admin | Read Only | Subgraph<br/> Admin \* | Subgraph<br/> Developer \* |
| ------------------------------------------------------------------- | ----- | ----- | --------- | ---------------------- | -------------------------- |
| View Supergraph Explorer                                            | ✅    | ✅    | ✅        | ✅                     | ✅                         |
| View project insights                                               | ✅    | ✅    | ✅        | ✅                     | ✅                         |
| Create supergraph builds<br/> - using all subgraphs' metadata       | ✅    | ✅    | ❌        | ❌                     | ❌                         |
| Create supergraph builds<br/> - using single subgraph's metadata \* | ✅    | ✅    | ❌        | ✅                     | ✅                         |
| Apply supergraph builds to project endpoint                         | ✅    | ✅    | ❌        | ❌                     | ❌                         |
| Create subgraph builds \*                                           | ✅    | ✅    | ❌        | ✅                     | ✅                         |
| Apply subgraph builds to project endpoint \*                        | ✅    | ✅    | ❌        | ✅                     | ❌                         |
| Admin permissions on all subgraphs                                  | ✅    | ✅    | ❌        | ❌                     | ❌                         |
| Create / Delete subgraphs                                           | ✅    | ✅    | ❌        | ❌                     | ❌                         |
| Add / Remove collaborators                                          | ✅    | ✅    | ❌        | ❌                     | ❌                         |
| Manage project plans and billing                                    | ✅    | ✅    | ❌        | ❌                     | ❌                         |
| Delete project                                                      | ✅    | ❌    | ❌        | ❌                     | ❌                         |

\* Only available on [DDN Advanced projects](https://hasura.io/pricing/ddn).


Each collaborator will receive an invitation to join the project and can proceed to add their subgraphs.

## Independent Subgraphs

Each independent subgraph contributing to the parent project resides in its own separate repository and has it's own
local supergraph. The repository could be an existing or newly initialized one.

### Step 1. Create a new supergraph with a new subgraph and initialize a repository

The invited user will need to create a new supergraph on their machine with a new subgraph and initialize a repository.

They will use the same name for the subgraph as was created in the parent project.

```ddn
ddn supergraph init <subgraph-name-as-supergraph-name> --create-subgraph <subgraph-name> && cd <subgraph-name> && git init
```

This scaffolds the necessary structure and initializes a Git repository.

### Step 2. Initialize the cloud project as the parent project and link the new subgraph

This will initialize the cloud project as the parent project.

```ddn
ddn project init --with-project <parent-project-name>
```

Your configuration file will now link the local subgraph to the parent project.

:::info Local Development

You can add data sources and develop locally at this point. Check out relevant tutorials for adding sources.

:::

### Step 3. Set the subgraph context

```ddn
ddn context set subgraph ./<subgraph-name>/subgraph.yaml
```

This sets the subgraph as the default for future CLI commands.

### Step 4. Create a subgraph build

```ddn
ddn subgraph build create
```

The CLI returns a build version for later integration into the parent supergraph.

### Step 5. Integrate the subgraph build into the parent project

```ddn
ddn supergraph build get
```

Use the latest build version from the parent supergraph and the subgraph build version in the following command:

```ddn
ddn supergraph build create --subgraph-version <subgraph-name:build-version> --base-supergraph-version <parent-build-id>
```

This integrates the subgraph changes into an existing build of the parent supergraph.

### Step 6. Apply the build

```ddn
ddn supergraph build apply <supergraph-build-version>
```

This finalizes the integration, making the changes available to all collaborators. This command is only available to
`Owner` and `Admin` roles. A `Subgraph Admin` can run:

```ddn title="Apply a single subgraph. Unavailable for Subgraph Developer role"
ddn subgraph build apply <subgraph-build-version>
```

:::tip Simplify multi-team workflows For more efficient collaboration across repositories, you can create and apply the
supergraph build in a single command using
`ddn supergraph build create --subgraph-version <subgraph-name:build-version> --base-supergraph-version <parent-build-id> --apply`
:::

## Build Summary

### Supergraph and all subgraphs

Even though all subgraphs are not in the same repo, the `ddn supergraph build create` command will build the supergraph
including all subgraphs which are on DDN cloud.

Once your [context is set to the shared project](/project-configuration/project-management/manage-environments.mdx), an
`Owner` or an `Admin` role can build and apply the supergraph including all subgraphs.

As always, `ddn supergraph build apply <supergraph-build-version>` will make it the active supergraph API.

`Subgraph Admin` and `Subgraph Developer` cannot build or apply supergraphs.

### Supergraph and specific subgraphs

An `Owner` or an `Admin` role can build and apply new supergraph and specify subgraphs to be built and applied.

You can also list the available builds of subgraphs to use with:

```ddn
ddn subgraph build get
```

```ddn title="Build a supergraph based on a specific build and with mutiple specific subgraphs"
ddn supergraph build create --subgraph-version <subgraph_name:subgraph_version> --subgraph-version <subgraph_name:subgraph_version>  --base-supergraph-version <supergraph_version>
```

See more about incremental builds [here](/deployment/hasura-ddn/incremental-builds.mdx).

### A single subgraph

An `Owner`, `Admin`, `Subgraph Admin` and `Subgraph Developer` role can build a single subgraph. All except
`Subgraph Developer` can apply a single subgraph.

```ddn title="Build a single subgraph"
ddn subgraph build create --subgraph-version <subgraph_name:subgraph_version> --base-supergraph-version <supergraph_version>
```

```ddn title="Apply a single subgraph. Unavailable for Subgraph Developer role"
ddn subgraph build apply --subgraph-version <subgraph_name:subgraph_version>
```

## Merging Existing Projects

If you have two independently developed projects on Hasura DDN that you want to merge into a single project with
independent subgraph development, follow these steps:

1. Choose which project will be the main project and which will be the subgraph project
2. In the main project, create a new subgraph placeholder:

```ddn
ddn project subgraph create <subgraph-name>
```

3. Invite the subgraph project collaborators with appropriate permissions
4. Once collaborators accept the invitation, they should set their project context:

```ddn
ddn context set project <main-project-name>
```

5. Set up subgraph prefixes if needed to prevent naming conflicts
6. Create a subgraph build on the main project:

```ddn
ddn subgraph build create
```

7. The main project owner/admin can then apply the subgraph build:

```ddn
ddn subgraph build apply <subgraph-build-version>
```

## Next steps

- [Follow an end-to-end tutorial](/project-configuration/tutorials/work-with-multiple-repositories.mdx)



==============================



# subgraph-prefixing.mdx

URL: https://hasura.io/docs/promptql/project-configuration/subgraphs/subgraph-prefixing

# Subgraph Prefixing

Subgraphs are namespaced and metadata object names are independent from one another and cannot conflict. However, the
supergraph is where the subgraphs meet, potentially leading to naming collisions.

To avoid collisions between GraphQL root fields and type names across when federating subgraphs, you can optionally
customize the prefixes for each.

For example, if two subgraphs both have a `Users` type, you can apply different prefixes to distinguish one from the
other. This ensures that each subgraph remains unique and prevents any naming conflicts.

You can make these modifications in the `subgraph.yaml` file for a subgraph.

```yaml title="Add the highlighted lines:"
kind: Subgraph
version: v2
definition:
  name: my_subgraph
  generator:
    rootPath: .
    #highlight-start
    graphqlRootFieldPrefix: my_subgraph_
    graphqlTypeNamePrefix: My_subgraph_
    #highlight-end
```

By default, the `subgraph.yaml` file is generated without any prefixes. You can read more about these fields
[here](reference/metadata-reference/build-configs.mdx#subgraph-subgraphgeneratorconfig).

## Renaming GraphQL root fields and GraphQL type name prefixes

This codemod will rename prefixes in already generated metadata. It can also be used to add or remove prefixes
altogether.

The `--from-graphql-root-field` prefix will be stripped if provided, and the new prefix, `--graphql-root-field`, will be
added. If the new prefix is already present, it will not be reapplied.

Examples:

```bash
# Add root field and type name prefixes to the subgraph set in the context
ddn codemod rename-graphql-prefixes --graphql-root-field 'app_' --graphql-type-name 'App_'

# Change the root field prefix for the specified subgraph
ddn codemod rename-graphql-prefixes --subgraph app/subgraph.yaml --from-graphql-root-field 'app_' --graphql-root-field 'new_'
```



==============================



# index.mdx

URL: https://hasura.io/docs/promptql/project-configuration/project-management/

# Project Management

## Introduction

Broadly, projects can be managed using two tools: context and collaborators.

**Context** allows you to swap out local and cloud configuration files and values. This makes it easier to execute
commands via the CLI, switch between environments when testing your API, and executing automated CI/CD scripts.

**Collaborators** can be added to any [Hasura DDN Base or Hasura DDN Advanced project](https://hasura.io/pricing).
Depending on the project's plan, you can add collaborators with read-only access all the way to granular access,
enabling them to only contribute to certain [subgraphs](/project-configuration/subgraphs/index.mdx).

## Next steps

- [Learn how to manage context](/project-configuration/project-management/manage-contexts.mdx)
- [Learn how to invite collaborators](/project-configuration/project-management/manage-collaborators.mdx)



==============================



# manage-contexts.mdx

URL: https://hasura.io/docs/promptql/project-configuration/project-management/manage-contexts

# How to Manage Project Contexts

## Introduction

Contexts simplify your development workflow by making CLI commands more concise and ensuring easy transitions between
environments. They act as predefined configurations that the CLI uses to interact with specific sets of values, making
processes like deploying, testing, or managing your project in CI/CD pipelines easier.

You can manage your project's contexts using the `.hasura/context.yaml` file, which is generated in the root of your
project when the CLI initializes it.

## Contexts

By default, the CLI sets up your project with a `default` context. To add new contexts, use the
[`create-context` CLI command](/reference/cli/commands/ddn_context_create-context.mdx).

For a full list of context-related commands, refer to
[this category of commands](/reference/cli/commands/ddn_context.mdx). Although you can modify these values directly in
the file, we recommend using the CLI for consistency and ease. Examples are provided below.

### Project

The `project` key-value pair can be mapped to a Hasura DDN cloud project. This is used to dictate which cloud-hosted
project should be referenced by the CLI when cloud-based and project-related commands are executed.

```yaml title="When the default context is set, the great-ddn-1234 project will be used by the CLI:"
contexts:
  default:
    #highlight-start
    project: great-ddn-1234
    #highlight-end
    supergraph: ../supergraph.yaml
    subgraph: ../app/subgraph.yaml
    localEnvFile: ../.env
    cloudEnvFile: ../.env.cloud
```

When you initialize a new Hasura Cloud project, the `project` key-value will automatically be set by the CLI. You can
also override this manually using [the `ddn context set` command](/reference/cli/commands/ddn_context_set.mdx).

This key-value pair simplifies switching contexts across development stages. For instance, the `default` context could
be used for production builds, while a separate `staging` context could map to a dedicated project for API collaboration
and testing before moving to production.

### Supergraph

The `supergraph` key-value pair maps to a
[Supergraph metadata object](reference/metadata-reference/build-configs.mdx#supergraph-supergraph), which contains a
list of the subgraphs to include in any supergraph build.

```yaml title="When the default context is set, the root supergraph.yaml will be used by the CLI:"
contexts:
  default:
    project: great-ddn-1234
    #highlight-start
    supergraph: ../supergraph.yaml
    #highlight-end
    subgraph: ../app/subgraph.yaml
    localEnvFile: ../.env
    cloudEnvFile: ../.env.cloud
```

By default, this is set to the root `supergraph.yaml` when you initialize a local project. However, you can customize
this to use any valid Supergraph metadata object.

As with subgraphs below, setting this value makes any supergraph-related CLI commands more concise by eliminating the
need for extra flags.

### Subgraph

The `subgraph` key-value pair maps to a
[Subgraph metadata object](reference/metadata-reference/build-configs.mdx#subgraph-subgraph) which contains information
about which connectors are included in the subgraph and mappings for environment variables.

```yaml title="With the configuration below, if the current context is default, the app/subgraph.yaml configuration file will be used:"
contexts:
  default:
    project: great-ddn-1234
    supergraph: ../supergraph.yaml
    #highlight-start
    subgraph: ../app/subgraph.yaml
    #highlight-end
    localEnvFile: ../.env
    cloudEnvFile: ../.env.cloud
```

For projects with multiple subgraphs, it's handy to use this command to switch between subgraphs:

```ddn
ddn context set subgraph <path-to-subgraph-configuration-file>
```

This ensures common actions — like adding data connectors and generating metadata after introspection — are executed by
the CLI in the correct subgraph, keeping your supergraph organized and resources with their appropriate owners.

### localEnvFile

Your project is initialized with a root-level `.env` file and this is automatically set in the `default` context.
Whenever a new environment variable is added — such as when initializing a new connector — the CLI will add the
appropriate key-value pairs to this file. It is used in local development such as when running your local services
(i.e., the engine and connectors).

```yaml title="When the default context is set, the root .env file will be used by the CLI for any local development commands:"
contexts:
  default:
    project: great-ddn-1234
    supergraph: ../supergraph.yaml
    subgraph: ../app/subgraph.yaml
    #highlight-start
    localEnvFile: ../.env
    #highlight-end
    cloudEnvFile: ../.env.cloud
```

Ideally, the values here will — such as connection strings to data sources — will be mapped to development instances,
whether that be locally or hosted elsewhere. This makes it easy to work with what will eventually become a production
build, but using local resources for quicker iteration and shorter feedback loops.

Any time you need to add additional environment variables to a connector, you can
[use the CLI](/reference/cli/commands/ddn_connector_env_add.mdx).

### cloudEnvFile

Your project includes a root-level `.env.cloud` file, which is automatically set in the current context whenever a new
cloud project is initialized. This file is specifically used for cloud-related operations, such as deploying or managing
your services in a cloud environment.

```yaml title="When the default context is set, the root .env.cloud file will be used by the CLI for any cloud-related commands:"
contexts:
  default:
    project: great-ddn-1234
    supergraph: ../supergraph.yaml
    subgraph: ../app/subgraph.yaml
    localEnvFile: ../.env
    #highlight-start
    cloudEnvFile: ../.env.cloud
    #highlight-end
```

:::tip A localEnvFile is required before setting the cloudEnvFile

A `localEnvFile` is required before setting the `cloudEnvFile`. Ensure your `localEnvFile` is set for the current
context **before** initializing a new cloud project to avoid any deployment issues.

:::

## Next steps

Now that you have a better idea of configuring your project for various contexts,
[learn how to collaborate with others](/project-configuration/project-management/manage-collaborators.mdx) by adding
collaborators and defining their roles.



==============================



# manage-collaborators.mdx

URL: https://hasura.io/docs/promptql/project-configuration/project-management/manage-collaborators


# How to Manage Project Collaborators

## Introduction

Hasura allows multiple users and teams to work together as collaborators on projects by assigning each user specific
roles and permissions.

You can [invite users](#invite-collaborators) to a project, or allow users to [request access](#request-access).

:::info Only available on DDN Base and higher

In order to add collaborators, your project must either be a
[DDN Base or DDN Advanced project](https://hasura.io/pricing).

:::

## Available roles {#roles}

| Role                      | Abilities                                                                                            |
| ------------------------- | ---------------------------------------------------------------------------------------------------- |
| **Owner**                 | All Project abilities including deletion. At this time, project ownership is not transferable.       |
| **Admin**                 | Same as a Project Owner, excluding deletion of the project.                                          |
| **Read Only**             | Only explore and visualize the supergraph.                                                           |
| **Subgraph Admin** \*     | All subgraph development-related abilities: create subgraph build, apply subgraph build to endpoint. |
| **Subgraph Developer** \* | Same as a subgraph admin, excluding the ability to apply subgraph build to endpoint.                 |

\* Subgraph roles are only available on [DDN Advanced projects](https://hasura.io/pricing/ddn).


The following are the detailed permissions for the above roles:

| Permissions                                                         | Owner | Admin | Read Only | Subgraph<br/> Admin \* | Subgraph<br/> Developer \* |
| ------------------------------------------------------------------- | ----- | ----- | --------- | ---------------------- | -------------------------- |
| View Supergraph Explorer                                            | ✅    | ✅    | ✅        | ✅                     | ✅                         |
| View project insights                                               | ✅    | ✅    | ✅        | ✅                     | ✅                         |
| Create supergraph builds<br/> - using all subgraphs' metadata       | ✅    | ✅    | ❌        | ❌                     | ❌                         |
| Create supergraph builds<br/> - using single subgraph's metadata \* | ✅    | ✅    | ❌        | ✅                     | ✅                         |
| Apply supergraph builds to project endpoint                         | ✅    | ✅    | ❌        | ❌                     | ❌                         |
| Create subgraph builds \*                                           | ✅    | ✅    | ❌        | ✅                     | ✅                         |
| Apply subgraph builds to project endpoint \*                        | ✅    | ✅    | ❌        | ✅                     | ❌                         |
| Admin permissions on all subgraphs                                  | ✅    | ✅    | ❌        | ❌                     | ❌                         |
| Create / Delete subgraphs                                           | ✅    | ✅    | ❌        | ❌                     | ❌                         |
| Add / Remove collaborators                                          | ✅    | ✅    | ❌        | ❌                     | ❌                         |
| Manage project plans and billing                                    | ✅    | ✅    | ❌        | ❌                     | ❌                         |
| Delete project                                                      | ✅    | ❌    | ❌        | ❌                     | ❌                         |

\* Only available on [DDN Advanced projects](https://hasura.io/pricing/ddn).


## How to invite collaborators {#invite-collaborators}

### Step 1. Navigate to the Collaborators section

Open your project's console at [https://promptql.console.hasura.io](https://promptql.console.hasura.io) and select it
from the list of available projects. Once the project is open, click `Settings` in the bottom-left corner and then
select `Collaborators` from the `Project Settings` menu:

<Thumbnail src="/img/ci-cd/0.0.1_console_invite-collaborator.png" alt="Invite a collaborator" width="1000" />

### Step 2. Enter information

Click the `+ Invite Collaborator` button in the top-right corner of the `Collaborators` section and enter the
collaborator's email address, select the access level you'd like to assign them, and click `Invite`.

<Thumbnail src="/img/ci-cd/0.0.1_console_assign-collaborator-role.png" alt="Invite a collaborator" width="1000" />

<br />
:::info Granular Access (Subgraph Collaborators) Only available on DDN Advanced

In order to add subgraph collaborators, your project must be a [DDN Advanced project](https://hasura.io/pricing/ddn).

:::

The invitee will receive an email with a link allowing them to accept the invite and join the project.

## How to accept a collaboration invite {#accept-invite}

### Step 1. Click the link in your email

From your email, click the `View invitation` button. This will send you to
[https://promptql.console.hasura.io](https://promptql.console.hasura.io) where you can accept it and then explore and
contribute to the project according your [role](#roles).

### Step 2. Explore the project

From your new project, you can explore the console by:

- Visualizing the supergraph with the Explorer tab.
- Seeing other collaborators present in the project.

### Step 3. Learn how to develop locally

The owner of the project most likely has a Git repository with the project's contents available on a service such as
GitHub. To run the supergraph locally, and make contributions to the deployed supergraph,
[pick up here](/quickstart.mdx) in our getting started docs.

## Allow users to request access {#request-access}

You can adjust your project's settings to allow users to request access when navigating to your project's URL. To do
this, click the `Share` button in the top navigation of your project and select `Request Access`.

<Thumbnail
  src="/img/get-started/console_read-only.png"
  alt="Allow users to request access to your project"
  width="1000"
/>

Each time a user requests access, you'll be able to approve or deny the request from this modal.



==============================



# manage-environments.mdx

URL: https://hasura.io/docs/promptql/project-configuration/project-management/manage-environments

# Alternative Configuration Files per Environment

## Introduction

Following on from setting up and
[managing multiple contexts](/project-configuration/project-management/manage-contexts.mdx), you can also specify
different configuration files per environment. This is useful if you want to use different setups for each.

## Example

For example, if in `development` you want to use `noAuth` mode but for `staging` and `production` you want to use JWT
mode, you can create a supergraph config file for each environment and then specify the correct file in the context.

In the following example we have a `supergraph-development.yaml` file which specifies a chain to the
`subgraph-development.yaml` to the `metadata_development` directory to include for the metadata which sets the `noAuth`
mode for development context.

```yaml title="supergraph-development.yaml"
kind: Supergraph
version: v2
definition:
  subgraphs:
    # highlight-start
    - globals/subgraph-development.yaml
    # highlight-end
    - my_subgraph/subgraph.yaml
```

```yaml title="globals/subgraph-development.yaml"
kind: Subgraph
version: v2
definition:
  name: globals
  generator:
    rootPath: .
  # highlight-start
  includePaths:
    - metadata_development
  # highlight-end
```

```yaml title="globals/metadata_development/auth-config.hml"
kind: AuthConfig
version: v3
definition:
  mode:
    noAuth:
      role: admin
      sessionVariables: {}
```

Then similarly, we would have the supergraph file for the other environments to use which specifies JWT mode in the
`auth-config.yaml` file. You can read more about [auth here](/auth/overview.mdx).



==============================



# service-accounts.mdx

URL: https://hasura.io/docs/promptql/project-configuration/project-management/service-accounts


# Service Accounts

## Introduction

A service account represents an application, service, or automated process rather than a human user. These accounts are
employed for running background jobs, connecting systems, or performing specific operations without requiring manual
intervention.

Project owners or administrators can create and delete service accounts as needed. These accounts can act as
collaborators on other projects, just like regular users, and can hold roles such as supergraph admin or subgraph admin.

Additionally, project owners or administrators can regenerate service account tokens, making them a secure and ideal
alternative to Personal Access Tokens (PAT) for use in CI/CD workflows.

:::info Only available on DDN Base and higher

In order to create service accounts, your project must either be a
[DDN Base or DDN Advanced project](https://hasura.io/pricing/ddn).

:::

## How to create service account

### Step 1: Navigate to the Service Accounts section

As an owner or administrator on the project, open your project's console at
[https://promptql.console.hasura.io](https://promptql.console.hasura.io) and click `Settings` in the bottom-left corner.
Then select `Service Accounts` from the `Project Settings` menu and click `New Service Account`:

<Thumbnail
  src="/img/service-account/0.0.1_console_create_service_account.png"
  alt="Create a service account"
  width="1000"
/>

### Step 2: Enter details

Enter service account name and click `Continue`:

<Thumbnail
  src="/img/service-account/0.0.1_console_enter_service_account_name.png"
  alt="Create a service account"
  width="1000"
/>

### Step 3: Save service account token

After creating the service account, copy the service account token and store it securely. This token allows the CLI to
authenticate your service account You will not be able to view the token again after this step.

<Thumbnail
  src="/img/service-account/0.0.1_console_copy_service_account_token.png"
  alt="Create a service account"
  width="1000"
/>

### Step 4: Setup service account permissions

After saving the service account token, select the project and access level you want to grant to the service account,
and click `Give Access`:

<Thumbnail
  src="/img/service-account/0.0.1_console_setup_service_account_permissions.png"
  alt="Create a service account"
  width="1000"
/>

<br />
:::info Granular Access Only available on DDN Advanced

In order to add service account as a subgraph administrator, your project must be a
[DDN Advanced project](https://hasura.io/pricing/ddn).

:::

You can skip this step and assign permissions later by
[inviting the service account](/project-configuration/project-management/manage-collaborators.mdx) as a project
collaborator.

## How to use service account token

### Step 1. Login via CLI

```ddn
ddn auth login --access-token <service-account-token>
```

### Step 2. Create and apply supergraph build

```ddn
# Create supergraph build
ddn supergraph build create [flags]

# Apply supergraph build
ddn supergraph build apply <supergraph-build-version> [flags]
```

:::tip Streamline CI/CD workflows

For more efficient automation with service accounts, you can create and apply the supergraph build in a single command
using `ddn supergraph build create [flags] --apply`

:::

## How to regenerate service account token

Navigate to the `Service Accounts` section and click the `Regenerate` button next to the service account for which you
want to regenerate the token:

<Thumbnail
  src="/img/service-account/0.0.1_console_regenerate_service_account_token.png"
  alt="Create a service account"
  width="1000"
/>

## How to delete service account

### Step 1: Navigate to the Service Accounts section

Click the `Delete` button next to the service account you want to delete:

<Thumbnail
  src="/img/service-account/0.0.1_console_delete_service_account.png"
  alt="Create a service account"
  width="1000"
/>

### Step 2: Verify and confirm deletion

<Thumbnail
  src="/img/service-account/0.0.1_console_confirm_service_account_deletion.png"
  alt="Create a service account"
  width="1000"
/>



==============================



# console-collaborator-comments.mdx

URL: https://hasura.io/docs/promptql/project-configuration/project-management/console-collaborator-comments


# Commenting on Metadata

## Introduction

Hasura provides a commenting feature that allows your data teams to start conversations directly on the supergraph
metadata. This feature enhances collaboration by closing the feedback loop and helps teams communicate more effectively
about about their supergraph design and implementation.

:::tip Getting Access

This feature is available for all users on [Base and Advanced plans](https://hasura.io/pricing).

:::

## How to comment

You can add comments on various objects from your metadata.

1. Navigate to any model page in your project via the `Explorer` tab.
2. Hover over the field or section you want to comment on.
3. Click on the comment icon that appears.

<Thumbnail src="/img/get-started/console_comment-create.png" alt="Hover over fields to add comments" width="1000" />

## Commenting areas

### Explorer Tab

The **Explorer Tab** is the primary interface where users interact with the supergraph metadata, making it a crucial
place for collaboration. Comments here enable data producers and consumers to discuss design decisions, clarify data
models, and suggest improvements to enhance PromptQL's ability to talk accurately with your data.

- **Supergraph Page** - Comment on the overall schema design, structure, and implementation details of the supergraph.
- **Subgraph Page** - Provide feedback on individual subgraphs, ensuring alignment with the larger supergraph design.
- **Models → General**
  - **Description** - Clarify the model's purpose and usage to improve documentation.
  - **Signature** - Discuss function signatures, return types, and argument structures.
- **Models → Fields & Relationships**
  - **Output Fields** - Ask questions or provide insights on field usage and expected data.
  - **Arguments** - Discuss argument types, required vs. optional parameters, and potential defaults.
  - **Relationships** - Ensure relationships between models are well-defined and documented.
- **Models → Permissions**
  - **Role** - Comment on role-based access control settings.
  - **Read / Create / Update / Delete** - Discuss permission settings for different CRUD operations.

This feature is especially useful for teams working on building comprehensive supergraphs, as it ensures everyone stays
aligned on data structure and permissions.

<Thumbnail src="/img/get-started/comments_explorer.png" alt="Comments on Explorer" width="800" />

---

### Insights Tab

The **Insights Tab** provides performance metrics, traces, and reports about API usage. Commenting here helps teams
analyze and discuss API behavior, identify bottlenecks, and track improvements over time.

- **Performance** – Leave feedback on latency, throughput, and error rates.
- **Platform Report** – Discuss API usage patterns and suggest improvements.
- **Traces** – Analyze request traces and collaborate on optimizing execution paths.

This feature is valuable for DevOps, backend engineers, and API consumers looking to enhance API efficiency.

<Thumbnail src="/img/get-started/comments_insights.png" alt="Comments on Insights" width="800" />

---

### Builds Tab

The **Builds Tab** allows teams to track and validate schema changes across supergraph, subgraph, and connector builds.
Commenting in this area helps teams coordinate schema evolution and avoid breaking changes.

- **Supergraph Builds** – Discuss schema changes at the supergraph level and their impact on consumers.
- **Subgraph Builds** – Leave comments about specific subgraph updates and dependencies.
- **Connector Builds** – Provide feedback on connector integrations and compatibility.
- **Schema Diff** – Highlight breaking changes or inconsistencies between schema versions.

By facilitating discussions on builds, teams can ensure smooth API evolution and prevent unexpected failures.

<Thumbnail src="/img/get-started/comments_builds.png" alt="Comments on Builds" width="800" />

## Notifications

In-app notifications show new comments made on your project.

The notification hub can be found in the top right corner of the console. On clicking the comments button, you will see
all the comments where you are tagged in one place. You can click on a particular comment (deep linking) and go to the
original thread on the console. You can also delete notifications from that menu.

{/* <Thumbnail src="/img/get-started/comments_notification.png" alt="Notifications" width="100" height="100" /> */}
![Alt text](/img/get-started/comments_notification.png)

<br />
<br />

:::info Invite collaborators

You can learn how to invite collaborators [here](/project-configuration/project-management/manage-collaborators.mdx).

:::

## Limitations

The feature is in early access and has known limitations, which are in our backlog. Let us know if you would like to
prioritize any specific functionality.

1. Tagging users on comments
2. Resolving comments
3. Email notifications
4. Ability to auto notify subgraph admin and developers.
5. History Tab for comments.



==============================



# manage-environment-variables.mdx

URL: https://hasura.io/docs/promptql/project-configuration/project-management/manage-environment-variables


# Manage Environment Variables

## Introduction

Environment variables are a crucial part of configuring your PromptQL project. They allow you to manage various
deployments of your application and its connectors. This guide covers how to manage environment variables at both the
project and connector levels, and how to collaborate effectively with your team.

## Project-level variables

By default, your project uses two main environment files:

1. `.env` - Used for local development
2. `.env.cloud` - Used for cloud deployments

The root-level `.env` file is automatically loaded by Docker Compose and contains variables that control your local
development environment. Key variables include:

- `HASURA_DDN_PAT` - Your Hasura Data Delivery Network Personal Access Token
- `PROMPTQL_SECRET_KEY` - Secret key for PromptQL services
- `HASURA_DDN_URL` - URL for the Hasura Data Delivery Network
- `HASURA_LLM_URI` - URI for the LLM service
- `CORS_ORIGINS` - Allowed CORS origins for the playground
- `OTEL_EXPORTER_OTLP_ENDPOINT` - OpenTelemetry endpoint

The `.env.cloud` file follows the same structure but contains values specific to your cloud deployment. This separation
ensures that your local development environment remains isolated from production settings.

:::info You can specify environment variable files using contexts

While these filenames follow our default conventions, you can configure and specify filenames using your project's
`./hasura/context.yaml` file. Learn more about managing contexts
[here](/project-configuration/project-management/manage-contexts.mdx).

:::

## Connector-level variables

Each connector in your project can define its own environment variables. These are prefixed with the subgraph name
(e.g., `APP`) followed by the connector name (e.g., `IMDB`). For example, from our [quickstart](/quickstart.mdx):

For the IMDB connector:

- `APP_IMDB_CONNECTION_URI`
- `APP_IMDB_HASURA_SERVICE_TOKEN_SECRET`
- `APP_IMDB_OTEL_EXPORTER_OTLP_ENDPOINT`
- `APP_IMDB_OTEL_SERVICE_NAME`

For the PromptFlix connector:

- `APP_PROMPTFLIX_HASURA_SERVICE_TOKEN_SECRET`
- `APP_PROMPTFLIX_OTEL_EXPORTER_OTLP_ENDPOINT`
- `APP_PROMPTFLIX_OTEL_SERVICE_NAME`

These variables are mapped in the respective `compose.yaml` files of each connector and are automatically loaded from
your root `.env` or `.env.cloud` file.

## Collaborate

When working with environment variables in a team setting:

1. **Never commit sensitive values**: Your `.env` and `.env.cloud` files should be in `.gitignore`
2. **Share template files**: Create `.env.example` and `.env.cloud.example` files that show the required variables
   without actual values
3. **Document requirements**: Use comments in your environment files to explain what each variable does
4. **Use consistent naming**: Follow the established naming conventions (e.g., `APP_` prefix for connector variables)
5. **Version control**: Keep your `.env.cloud` file in version control (without sensitive values) to track deployment
   configurations

## Add custom environment variables

The [`ddn connector env add` command](reference/cli/commands/ddn_connector_env_add.mdx) is a powerful tool that manages
environment variables across your entire project. When you add a variable using this command, it:

1. Updates your root-level environment variable files.
2. Ensures the variable is properly mapped in your connector's configuration.
3. Maintains consistency across your development and deployment environments.

### How it works

When you run the command, it performs several actions:

1. **Environment File Updates**: The command adds your variable to the specified environment file (defaulting to `.env`
   if not specified)
2. **Connector Configuration**: Updates the connector's configuration to recognize the new variable
3. **Subgraph Integration**: If specified, ensures the variable is properly integrated with your subgraph configuration

:::info Custom environment variables are often used with lambda connectors

While you can use this command to add custom environment variables to any connector, this is most often used with lambda
connectors which require environment variables for external services, sensitive keys, etc.

Learn more [here](/business-logic/add-env-vars-to-a-lambda.mdx).

:::

## Dynamic cloud variables

Some environment variables in your `.env.cloud` file are dynamic and will change with each new build or deployment.
These typically include:

- Connector read/write URLs
- Deployment-specific tokens
- Build-specific identifiers

### Managing dynamic variables

For production deployments, we strongly recommend using CI/CD pipelines to manage these dynamic variables. This ensures:

1. Variables are always up-to-date with the latest build
2. No conflicts between team members' local `.env.cloud` files
3. Consistent deployment configurations
4. Automated updates of dynamic values

### Handling local development

For local development with dynamic variables:

1. Use the development environment (`.env`) for most testing
2. Only use `.env.cloud` when testing production-specific features
3. Be aware that some features requiring dynamic URLs may not work locally
4. Use the development playground for testing instead of production endpoints

Remember that while you can manually update some values in `.env.cloud`, the dynamic variables should be managed through
your CI/CD pipeline to ensure consistency and prevent conflicts.



==============================



# Basics

URL: https://hasura.io/docs/promptql/deployment/overview


# Deployment

## Introduction

There are a few ways to deploy your PromptQL application.

## Hasura DDN

You can host your PromptQL project on fully-managed Hasura DDN.

- Zero infrastructure management
- Global CDN and edge caching
- Automatic scaling and high availability
- Built-in monitoring and observability
- Managed security and updates

[Read more](/deployment/hasura-ddn/index.mdx)

## Private DDN

Hasura Private DDN is an enterprise-grade deployment offering to run your application on dedicated infrastructure.

You can also host your [Data Plane](/help/glossary#data-plane) on your own infrastructure, and the
[Control Plane](/help/glossary#control-plane) on Hasura DDN.

- Enhanced security through private networking
- Direct private connectivity to your data sources
- Custom infrastructure requirements
- Compliance with specific regulatory requirements

[Read more](/private-ddn/overview.mdx)



==============================



# Overview

URL: https://hasura.io/docs/promptql/deployment/hasura-ddn/

# Deployment Overview

Deploying to Hasura DDN is the optimal way to enable PromptQL to accurately talk to all your data. Hasura DDN provides
the data foundation that powers PromptQL's intelligent data interaction capabilities. For more details on dedicated
infrastructure, see [Hasura Private DDN](/private-ddn/overview.mdx). For more details on pricing plans, see
[our pricing page](https://hasura.io/pricing).

## Hasura DDN

With Hasura DDN, PromptQL runs in a globally-distributed cloud where performance, availability, and security are handled
for you.

To deploy your PromptQL project, you will create builds of your connectors, subgraphs, and the supergraph. This can all
be done in a single command. You will then apply the supergraph build to your project, making it the "official" build
for the project.

PromptQL will use this supergraph to facilitate AI-powered conversations with your data, generating dynamic query plans
that can access and analyze information across all your connected data sources.

## Next steps

Use the How To guide for the most common steps to deploy your PromptQL project to DDN. Try the tutorial for more
detailed steps depending on your specific connector.

- [How to deploy your project to DDN](/deployment/hasura-ddn/deploy-to-ddn.mdx)
- [Deploy specific connectors and subgraphs to DDN](/deployment/hasura-ddn/incremental-builds.mdx)
- [Set up CI/CD for DDN](/deployment/hasura-ddn/ci-cd.mdx)

### Project configuration:

- [Deploy from multiple repositories.](/project-configuration/subgraphs/working-with-multiple-repositories.mdx)



==============================



# incremental-builds.mdx

URL: https://hasura.io/docs/promptql/deployment/hasura-ddn/incremental-builds

# Deploying Incrementally

Incremental deployments give you fine-grained control over the compositions of which versions of connectors, subgraphs,
and eventual supergraph are deployed to Hasura DDN to serve your PromptQL application. It is particularly helpful if you
have a complex project structure or need to maintain strict control over which services or subgraphs receive new
changes. For example:

- You need to test a new version of a connector in isolation before pushing it out to the rest of your application.
- A single subgraph requires frequent updates, and you don't want to rebuild or redeploy the entire supergraph each
  time.
- Multiple teams work on separate parts of the application, and you only want to incrementally roll out changes to one
  service at a time.

By deploying incrementally, you can control the roll-out process, test updates more thoroughly in smaller chunks, and
reduce potential downtime or bugs impacting the rest of your application. If you'd rather deploy everything at once and
don't need granular control, check out the more general approach in our
[Deploy to DDN](/deployment/hasura-ddn/deploy-to-ddn.mdx) guide.

## Building and deploying specific connectors

### Step 1. Create a new build of the connector on Hasura DDN.

```ddn title="To build the connector and have it running on the Hasura DDN infrastructure you can run the following command:"
ddn connector build create --connector <path-to-connector.yaml>
```

The output will include the ConnectorBuild Id, the ConnectorBuild Read URL, the ConnectorBuild Write URL, and the
Authorization Header used to communicate with the connector.

### Step 2. Update the env values

```env title="Update the env values for the connector URLs and Authorization Header in the corresponding .env.cloud file:"
APP_MY_CONNECTOR_CONNECTION_URI="<existing-connection-uri>"
APP_MY_CONNECTOR_AUTHORIZATION_HEADER="<new-authorization-header>"
APP_MY_CONNECTOR_READ_URL="<new-read-url>"
APP_MY_CONNECTOR_WRITE_URL="<new-write-url>"
```

### Step 3. Build the supergraph without building connectors

```ddn
ddn supergraph build create --no-build-connectors
```

The build version will be output after the build is complete.

Note that you can also build subgraphs individually without building connectors, see the section on
[building and deploying a specific subgraph build](#build-deploy-subgraph) below.

### Step 4. Deploy the supergraph

```ddn
ddn supergraph build apply <build-version>
```

This will apply the build to the supergraph to become the active build serving your PromptQL project.

:::tip Optimize incremental deployments

For a more efficient incremental deployment process, you can create and apply the supergraph build in a single command
using `ddn supergraph build create --no-build-connectors --apply`

:::

:::info Resource Limits

Connectors on Hasura DDN request specific memory and CPU allocations for their deployment. Resource limits can only be
modified for [Private DDN](/private-ddn/connector-deployment-resources.mdx). For Public DDN, the resource limits are
fixed to the values below, and cannot be downsized or upscaled.

- **Fixed CPU**: 1 vCPU
- **Fixed RAM**: 2GB

:::

## Building and deploying a specific subgraph {#build-deploy-subgraph}

You can build subgraphs independently, too.

### Step 1. Build

```ddn
ddn subgraph build create
```

This will create a build of the subgraph and any connectors that are associated with it and output the build version.

As always, this will use the current subgraph set in the `context.yaml` file. You can override this with the
`--subgraph` flag or change subgraph context with `ddn context set subgraph <path-to-subgraph.yaml>`.

:::info Hasura DDN only

There is no local subgraph-only build command.

:::

### Step 2 Option 1: Build the supergraph with the new subgraph build

You can create a build of the rest of the supergraph (with the currently applied supergraph build or a different
specific one) but direct Hasura DDN to use the new specific subgraph build.

```ddn title="Build the supergraph with a specific subgraph build and the currenly applied supergraph build"
ddn supergraph build create --subgraph-version <subgraph-name>:<subgraph-build-version> --base-supergraph-on-applied
```

```ddn title="Build the supergraph with multiple specific subgraph builds and a specific supergraph build"
  ddn supergraph build create --subgraph-version <subgraph-name>:<subgraph-build-version> --subgraph-version <subgraph-name>:<subgraph-build-version> --base-supergraph-version <supergraph-build-version>
```

This will output the build version and you can now use it for testing or apply the supergraph build to be the active
build for your PromptQL application.

```ddn
ddn supergraph build apply <build-version>
```

You can compose any combination of existing subgraph builds this way to create your supergraph builds.

### Step 2 Option 2: Apply a specific subgraph build to the active API

You can also apply the subgraph build directly to the active API servicing your PromptQL project.

```ddn
ddn subgraph build apply <build-version>
```

## Summary

You have full control over the composition of your supergraph and can build and deploy subgraphs and connectors
incrementally and independently to compose your supergraph.



==============================



# deploy-to-ddn.mdx

URL: https://hasura.io/docs/promptql/deployment/hasura-ddn/deploy-to-ddn

# Deploying your project to Hasura DDN

Deploying your project to Hasura DDN is a simple process and can be done in two steps.

## Deployment flow

1. Create a supergraph build on Hasura DDN.
2. Apply the supergraph build to your project on Hasura DDN.

To begin this guide you will need to have a local project set up. Check out the [quickstart](/quickstart.mdx) for more
information on how to get started.

:::info Hasura DDN Cloud projects

When you initialize a new project — like in the quickstart — we automatically provision a Hasura DDN Cloud project
that's paired with your local project.

:::

### Step 1. Create a supergraph build on Hasura DDN

```ddn title="The following will use the project name in your .hasura/context.yaml file:"
ddn supergraph build create
```

This command will create builds for each connector, subgraph, and the supergraph. Each of these can be built
independently but this command will create them all.

The CLI will respond with the build version, the Console URL, the PromptQL URL, the Project Name, and a hint to browse
the build on the console.

You can now use the PromptQL playground to test your build by running `ddn console --build-version <build-version>`
command.

The build is not yet the "official" applied API for the project. A project can have multiple builds, but only one
applied at a time as the "official" API.

### Step 2. Apply the build

```ddn
# E.g., ddn supergraph build apply 85b0961544
ddn supergraph build apply <build-version>
```

This build is now the "official" applied API for the project and is accessible via the API URL in the output of the
command, via the console, or any client accessing via the API URL.

:::tip Simplify your deployment

For a more efficient deployment process, you can create and apply the supergraph build in a single command using
`ddn supergraph build create --apply`

:::

## Summary

There are many more options and configurations available for deploying your project to Hasura DDN and we have detailed
the simplest and most common flow here.



==============================



# CI/CD

URL: https://hasura.io/docs/promptql/deployment/hasura-ddn/ci-cd

# CI/CD

Because the DDN CLI plays the central role in controlling and managing metadata builds and connector deployments, it
allows developers to easily create effective CI/CD pipelines for managing PromptQL projects, creating and applying
builds or promoting them to another environment.

## Generic CI/CD Setup

You should follow these generic implementation steps which we will go through below when setting up CI/CD for your
Hasura DDN projects.

1. Prepare a service account access token.
2. Use a linux environment to run the DDN CLI.
3. Install and login to the DDN CLI.
4. Clone the repo with the DDN supergraph metadata.
5. Check context or use flags.
6. Run DDN CLI commands.

### Step 1. Prepare a service account access token

See the [service account access token section](/project-configuration/project-management/service-accounts.mdx) to learn
how to create a service account access token.

### Step 2. Use a Linux environment to run the DDN CLI

Any service which can provide a Linux environment will work, e.g., GitHub Actions, GitLab CI, CircleCI, etc.

Indeed, a Windows or macOS environment will work and run the DDN CLI but for CI/CD pipelines, we recommend using a Linux
environment for speed, availability and convenience.

### Step 3. Install and login to the DDN CLI

```sh
curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/v4/get.sh | bash
```

:::info curl

Your environment should already have `curl` installed. If not, you can install it using your package manager.

```sh title="E.g."
sudo apt update && sudo apt upgrade
sudo apt install curl
curl --version
```

:::

### Step 4. Clone the repo with the metadata

Clone (or checkout) the repository that contains all of your DDN supergraph metadata.

```sh title="For example, if you have a public GitHub repository, run:"
git clone https://github.com/<your-org>/<your-repo>.git
```

If your repository is private, you may need a
[GitHub personal access token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens)
or similar for your git provider to clone it.

```sh
git clone https://username:<pat>@github.com/<your-account-or-org>/<repo>.git
```

### Step 5. Check context or use flags

Make sure your context is set properly to reference the correct projects, subgraphs, etc., or use the correct flags in
your DDN CLI commands.

You can set your context using the `ddn context set-current-context <context-name>` command. Read more about
[contexts](/project-configuration/project-management/manage-environments.mdx).

You can also of course use specific flags in your DDN CLI commands eg: `--project`, `--supergraph`,
`--base-supergraph-version`, `--log-level`, `--out` to make your commands more explicit.

### Step 6. Run DDN CLI commands

You can now run any DDN CLI commands required in the CI/CD workflow. You will likely be creating builds and applying
them to projects.

```ddn
ddn supergraph build create
```

```ddn
ddn supergraph build apply <build-id>
```

:::tip Optimize your CI/CD pipeline

For a more efficient CI/CD workflow, you can create and apply the supergraph build in a single command using
`ddn supergraph build create --apply` This reduces the number of CLI commands needed and simplifies your automation
scripts.

:::

## GitHub Actions Setup

We have a GitHub Action that you can use to deploy your Hasura DDN projects.
[See here](https://github.com/hasura/ddn-deployment).



==============================



# Region Routing

URL: https://hasura.io/docs/promptql/deployment/hasura-ddn/region-routing

# Region Routing

## Introduction

With region routing, you can define the deployment configuration of a data connector for different regions.

For data connectors that connect to a data source, e.g. [PostgreSQL](/how-to-build-with-promptql/with-postgresql.mdx),
it is recommended to deploy the connector in a region closest to the data source to ensure efficient communication
between the connector and the data source.

For other data connectors, e.g. [Typescript](https://hasura.io/connectors/nodejs), it is recommended to deploy the
connector in a region closest to the consumers of the API to ensure efficient communication between the connector and
the Hasura engine.

If you have a distributed data source, with multi-region routing, you can ensure that data is fetched from the data
source closest to the user, thus minimizing latency for the request, improving the performance of your application, and
providing a better user experience.

See the list of supported regions [below](#regions).

## Single-Region Routing

You can modify the `Connector` object as per the highlighted values in the example below to force the deployment of your
data connector to a specific region. If the region is not specified, the connector will be deployed randomly to one of
the [supported regions](#regions).

```yaml title="For example, in my_subgraph/connector/my_connector/connector.yaml:"
kind: Connector
version: v2
definition:
  name: my_connector
  subgraph: my_subgraph
  source: hasura/connector_name:<version>
  context: .
  #highlight-start
  regionConfiguration:
    - region: <region from the list below>
      mode: ReadWrite
      envMapping:
        <CONNECTOR_ENV_VAR>: # e.g. CONNECTION_URI
          fromEnv: <CONNECTOR_ENV_VAR> # e.g. Env Var set as DB read write URL
  #highlight-end
```

## Multi-Region Routing

You can modify the `Connector` object as per the highlighted values in the example below to define the deployment
configuration of your connector across multiple regions.

:::note Currently only supported for PostgreSQL

Multi-region routing is currently supported only for the
[PostgreSQL connector](/how-to-build-with-promptql/with-postgresql.mdx).

Support for other data connectors will be added soon.

:::

```yaml title="For example, in my_subgraph/connector/my_connector/connector.yaml:"
kind: Connector
version: v2
definition:
  name: my_connector
  subgraph: my_subgraph
  source: hasura/connector_name:<version>
  context: .
  #highlight-start
  regionConfiguration:
    - region: <region1: region from the list below>
      mode: ReadWrite
      envMapping:
          <CONNECTOR_ENV_VAR>: # e.g. CONNECTION_URI
              fromEnv: <CONNECTOR_ENV_VAR_REGION_1> # e.g. Env Var set as DB read write replica URL in region1
    - region: <region2: region from the list below>
      mode: ReadOnly
      envMapping:
          <CONNECTOR_ENV_VAR>: # e.g. CONNECTION_URI
              fromEnv: <CONNECTOR_ENV_VAR_REGION_2> # e.g. Env Var set as DB read only replica URL in region2
    - region: <region3: region from the list below>
      mode: ReadOnly
      envMapping:
          <CONNECTOR_ENV_VAR>: # e.g. CONNECTION_URI
              fromEnv: <CONNECTOR_ENV_VAR_REGION_3> # e.g. Env Var set as DB read only replica URL in region3
  #highlight-end
```

## Supported regions {#regions}

Currently, Hasura DDN supports the following regions in GCP for multi-region routing:

- `gcp-asia-south1`
- `gcp-asia-southeast1`
- `gcp-australia-southeast1`
- `gcp-europe-west1`
- `gcp-southamerica-east1`
- `gcp-us-east4`
- `gcp-us-west2`



==============================



# Private DDN

URL: https://hasura.io/docs/promptql/private-ddn/overview

# Hasura Private DDN

## Introduction

Hasura Private DDN allows you to provision private Data Planes — either on your own infrastructure, or dedicated Hasura
infrastructure — to give you complete control over your supergraphs powering your PromptQL applications. With Hasura
Private DDN, enterprises can ensure data privacy, security, and compliance by hosting the data plane within their own
Virtual Private Cloud (VPC) provided by Hasura or any infrastructure you choose.

:::info An enterprise contract is required for Private DDN

To enable Private DDN, you'll need an Enterprise contract. Reach out to sales [here](https://hasura.io/contact-us).

:::

## Data Planes

Data Planes are at the core of Hasura Private DDN. They serve as the runtime environment for your API, handling data
access, authorization, and other operations. By deploying Data Planes privately, you can:

- **Maintain data sovereignty**: Ensure data never leaves your private infrastructure.
- **Enhance security**: Leverage existing enterprise-grade security measures, including firewalls, IAM policies, and
  more.
- **Optimize performance**: Reduce latency by hosting the data plane close to your data sources.
- **Enable BYOC**: Bring your own cloud and deploy the data plane on the cloud provider of your choice, whether it's
  AWS, GCP, Azure.

Each Data Plane integrates seamlessly with Hasura DDN's Control Plane, enabling collaborative API development, version
control, and CI/CD integration while keeping runtime operations secure and private.

## Next Steps

Ready to get started with Hasura Private DDN? Here's what you can do next:

- [Architecture](/private-ddn/architecture/index.mdx)
- [Create a Data Plane](/private-ddn/creating-a-data-plane/index.mdx)
- [Add collaborators to a Data Plane](/private-ddn/data-plane-collaboration.mdx)
- [Create a project on a private Data Plane](/private-ddn/create-a-project-on-a-data-plane.mdx)
- [DDN workspace](/private-ddn/ddn-workspace.mdx)



==============================



# index.mdx

URL: https://hasura.io/docs/promptql/private-ddn/architecture/

# Private DDN

With Private deployment for Hasura DDN and PromptQL, you can run Hasura and your connectors either on dedicated
infrastructure hosted by Hasura or on your own infrastructure. In both cases, Hasura's automated systems will be
managing the infrastructure ensuring uptime and reliability.

Private Hasura DDN offers enhanced security and isolation by enabling private connectivity for your databases, APIs, and
connectors. Hasura communicates with your sources over a dedicated private network, bypassing the public internet.

- [Dedicated](/private-ddn/architecture/dedicated) private deployments
- [BYOC](/private-ddn/architecture/byoc) private deployments
- [Fully Self-Hosted](/private-ddn/architecture/self-hosted) private deployments



==============================



# dedicated.mdx

URL: https://hasura.io/docs/promptql/private-ddn/architecture/dedicated


# Private DDN: Dedicated

## Introduction

Hasura manages the deployment, uptime, and availability of the Data Plane for you.

Hasura-hosted DDN is available on GCP, AWS, and Azure. It can be deployed to any region of your choice. You can select
multiple regions to set up a distributed-edge configuration to provide the lowest possible latency to your clients.

The Data Plane operates on a dedicated and isolated compute and network infrastructure, ensuring enhanced security and
compliance for your workloads. The infrastructure is set up using the highest isolation levels provided by the cloud
provider.

Private network connectivity is available through VPC Network Peering, Private Link / Private Service Connect, Transit
Gateway, or any other equivalent connectivity option offered by the cloud provider.

Please reach out to us if you need support for another cloud or configuration that is not mentioned here.

<Thumbnail src="/img/deployment/private_ddn_hasura_hosted.png" alt="Private DDN Architecture Hasura Hosted" />

## Get started

To get started with Hasura DDN in Hasura Hosted VPC deployment mode, [contact sales](https://hasura.io/contact-us).



==============================



# byoc.mdx

URL: https://hasura.io/docs/promptql/private-ddn/architecture/byoc


# Private DDN: Bring Your Own Cloud (BYOC)

## Introduction

Private DDN BYOC allows you to deploy the Data Plane component in your own cloud environment. This approach is useful if
you have specific requirements or constraints which prevent you from using the dedicated Data Plane on Hasura's cloud.

BYOC is currently supported on AWS, GCP and Azure.

If you would like access to Private DDN BYOC, please [contact sales](https://hasura.io/contact-us).

## Architecture

In the BYOC mode, the Data Plane is running on the customer's cloud account, while the Control Plane is running on
Hasura's cloud. The Data Plane is managed by Hasura's Control Plane, ensuing timely updates and maintenance. Uptime and
reliability of the Data Plane is a shared responsibility of the customer's infrastrucure team and Hasura's automated
systems.

:::info Data Plane stability

While essential for managing and updating the Data Plane, the Control Plane is not on the critical path for serving API
requests. Even if the Control Plane becomes unavailable, the Data Plane continues to operate without interruption.

:::

<Thumbnail src="/img/deployment/private_ddn_byoc.png" alt="Private DDN Architecture BYOC " />

## Data Flow and security

All critical data operations occur exclusively within the customer infrastructure. When an API user sends a request,
it's received by Hasura in the Data Plane. Hasura then directly accesses the necessary data sources within the customer
infrastructure to fulfill the request. This direct access ensures that sensitive data never leaves the customer's
controlled environment, maintaining data residency and security.

While the Control Plane, situated in the Hasura infrastructure, does communicate with the Data Plane, this communication
is strictly limited to configuration and management purposes. Importantly, this communication does not involve customer
data or API responses, further enhancing data security.

The distinction between the Control and Data Planes creates well-defined security boundaries. By keeping the Data Plane
and all data sources within the customer's security perimeter, the architecture ensures that sensitive information
remains under the customer's control at all times.

## Interactions with the Control Plane

The Data Plane running on your infrastructure communicates with the Control Plane only in very specific scenarios:

1. Metadata Storage: The Data Plane accesses a metadata storage bucket to retrieve build artifacts; these artifacts are
   required to process API requests.
2. Build Metadata: The Data Plane accesses the Control Plane APIs to retrieve information about (applied) builds for the
   purposes of routing.
3. Connector Metadata: The Data Plane gets information about the connectors that needs to be deployed so that controller
   within the Data Plane can deploy them.
4. OTEL Gateway: (Optional) The Data Plane sends observability data to the Control Plane so you can visualize it on the
   console; it does not contain any API response data or variables.
5. BYOC Controller: The Control Plane interacts with the Kubernetes cluster to manage the Data Plane workloads.

<Thumbnail
  src="/img/deployment/private_ddn_byoc_detailed.png"
  alt="Detailed architecture diagram for Private DDN BYOC"
/>

## Upgrade Process

Regular software upgrades are rolled out to the Data Plane automatically. This incudes

- Hasura Engine version upgrades
- Other components on the Data Plane
- Kubernetes and other dependencies

Upgrades are typically seamless, utilizing rolling restarts. However, some upgrades (e.g., Kubernetes node upgrades) may
require customer-specified maintenance windows to minimize disruption.

## FAQ

##### What cloud providers are supported for BYOC deployment?

AWS, GCP and Azure are supported.

##### Which regions are supported for BYOC deployment?

Any region on AWS, GCP, Azure are supported, provided there is enough quota available for the workloads.

## Get started

To get started with Hasura DDN in your own cloud, [contact sales](https://hasura.io/contact-us).



==============================



# self-hosted.mdx

URL: https://hasura.io/docs/promptql/private-ddn/architecture/self-hosted


# Self-Hosted

## Introduction

For customers with strict security and compliance requirements, Private DDN Self-Hosted allows you to host both the
Control Plane and Data Plane on your own infrastructure.

This is a premium offering where the Hasura team will be helping you with setting up the entire platform
organization-wide. An enterprise license is required for this offering.

If you would like access to Private DDN Self-Hosted, please [contact sales](https://hasura.io/contact-us).

## Architecture

In self-hosted mode, the Control Plane and Data Plane are running on customer's cloud account, managed by customer's own
operations team, with the help of the Hasura team. Uptime and reliability of the Data Plane is the responsibility of the
customer's infrastructure team.

A Kubernetes cluster is required for installing DDN.

<Thumbnail src="/img/deployment/private_ddn_self_hosted.png" alt="Private DDN Architecture Self Hosted" />

## Get started

To get started with Hasura DDN in your own infrastructure, [contact sales](https://hasura.io/contact-us).



==============================



# index.mdx

URL: https://hasura.io/docs/promptql/private-ddn/creating-a-data-plane/

# Introduction

A Data Plane in Hasura provides dedicated infrastructure for running your GraphQL workloads. This guide walks you
through the process of creating a new Data Plane.

- [Dedicated](/private-ddn/creating-a-data-plane/dedicated.mdx) private deployment
- [BYOC](/private-ddn/creating-a-data-plane/byoc.mdx) private deployment



==============================



# dedicated.mdx

URL: https://hasura.io/docs/promptql/private-ddn/creating-a-data-plane/dedicated


# Dedicated: Create a Data Plane

## Introduction

A Data Plane in Hasura provides dedicated infrastructure for running your GraphQL workloads. This guide walks you
through the process of creating a new Data Plane for [dedicated Private DDN](/private-ddn/architecture/dedicated.mdx).

:::info Prerequisite

To create a Data Plane in Private DDN, you'll need an Enterprise contract. Reach out to sales
[here](https://hasura.io/contact-us) to get started.

:::

## How to create a new Data Plane on Dedicated Private DDN

### Step 1. Create a new Data Plane

Navigate to the `Private DDN` section in your [Hasura console](https://promptql.console.hasura.io).

<Thumbnail src="/img/data-plane/private-ddn.png" alt="Data Plane Management" width="1000px" />

<br></br>

Click the `Create Data Plane` button.

<Thumbnail src="/img/data-plane/pending-provision-data-planes.png" alt="Data Plane Creation Pending" width="1000px" />

### Step 2. Complete the Data Plane Configuration Form

- **Name:** Enter a descriptive name using only letters and spaces, between 4-30 characters. This will be used to
  identify your Data Plane.
- **Domain:** This field is automatically generated from your Data Plane name. This is the base domain for the GraphQL
  API on this Data Plane.
- **Cloud Provider:** Select your preferred cloud provider. Currently available options:
  - Amazon Web Services (AWS)
  - Google Cloud Platform (GCP)
  - Microsoft Azure
- **Region:** Select the geographical region for your Data Plane.
- **Zones:** Select availability zones for your Data Plane. The maximum number of zones depends on your Hasura Private
  DDN subscription.

  - For **AWS**: Use AZ IDs (e.g., use1-az1, use1-az2)
  - For **GCP**: Use zone names (e.g., us-west2-a, us-west2-b)
  - For **Azure**: Use physical zone names (e.g., eastus-az1, eastus-az2)

  Expand below to view available zones for your cloud:

<details>
  <summary>AWS zones</summary>

**Availability Zone (AZ) IDs** are unique identifiers for AWS Availability Zones within a region. Using AZ IDs ensures
consistency across different AWS accounts and optimizes performance by aligning your Data Plane with your database's
physical location.

**Why Use AZ IDs Instead of AZ Names?**

- **Consistency:** AZ IDs are consistent across all AWS accounts, whereas AZ Names can differ.
- **Performance:** Deploying resources in the same physical AZ reduces latency.
- **Cost Efficiency:** Avoids cross-zone network costs by ensuring resources are within the same AZ.

Note: When selecting AZ IDs, ensure they correspond to the AZs where your database resides to maintain low latency and
avoid cross-zone network costs. It is recommended to choose at least two AZs for higher availability and fault
tolerance.

[Learn more about AWS AZ IDs](https://docs.aws.amazon.com/ram/latest/userguide/working-with-az-ids.html)

**Steps to View Available Zones:**

Using AWS CLI:

```bash
aws ec2 describe-availability-zones \
  --region us-east-1 \
  --output table \
  --query "AvailabilityZones[?State=='available'] | [].{ZoneName: ZoneName, ZoneId: ZoneId}"
```

Example output:

```
-----------------------------
| DescribeAvailabilityZones |
+-----------+---------------+
| ZoneId    | ZoneName      |
+-----------+---------------+
| use1-az1  | us-east-1a    |
| use1-az2  | us-east-1b    |
| use1-az3  | us-east-1c    |
+-----------+---------------+
```

Note: Replace `us-east-1` in the command with your desired region.

</details>

<details>
  <summary>GCP zones</summary>

**Zones in Google Cloud Platform (GCP)** are identified by their zone names, which follow a consistent pattern across
all GCP projects. Unlike AWS, GCP uses straightforward zone names that don't require separate ID mapping.

**Why Use Zone Names?**

- **Consistency:** Zone names are consistent across all GCP projects.
- **Performance:** Deploying resources in the same zone reduces latency.
- **Cost Efficiency:** Avoids cross-zone network costs by ensuring resources are within the same zone.

Note: When selecting zones, ensure they correspond to the zones where your database resides to maintain low latency and
avoid cross-zone network costs. It is recommended to choose at least two zones for higher availability and fault
tolerance.

[Learn more about GCP Zones](https://cloud.google.com/compute/docs/regions-zones)

**Steps to View Available Zones:**

Using gcloud CLI:

```bash
gcloud compute zones list \
  --filter="region:us-west2" \
  --format="table(name,region,status)"
```

Example output:

```
NAME        REGION    STATUS
us-west2-a  us-west2  UP
us-west2-b  us-west2  UP
us-west2-c  us-west2  UP
```

Note: Replace `us-west2` in the command with your desired region.

</details>

<details>
  <summary>Azure zones</summary>

**Physical Zone Names** are unique identifiers for Azure Availability Zones within a region. Using physical zone names
ensures consistency across different Azure subscriptions and optimizes performance by aligning your Data Plane with your
database's physical location.

**Why Use Physical Zone Names Instead of Logical Zones?**

- **Consistency:** Physical zone names are consistent across all Azure subscriptions, whereas logical zones (1,2,3) can
  map differently.
- **Performance:** Deploying resources in the same physical zone reduces latency.
- **Cost Efficiency:** Avoids cross-zone network costs by ensuring resources are within the same physical zone.

Note: When selecting physical zones, ensure they correspond to the zones where your database resides to maintain low
latency and avoid cross-zone network costs. It is recommended to choose at least two physical zones for higher
availability and fault tolerance.

[Learn more about Azure Physical vs Logical Zone Mapping](https://learn.microsoft.com/en-us/azure/availability-zones/az-overview#physical-and-logical-availability-zones)

**Steps to View Available Zones:**

Using Azure CLI:

```bash
az rest --method get \
  --uri '/subscriptions/{subscriptionId}/locations?api-version=2022-12-01' \
  --query 'value[?name==`eastus` && availabilityZoneMappings != `null`].{displayName: displayName, name: name, availabilityZoneMappings: availabilityZoneMappings}'
```

Example output:

```json
[
  {
    "availabilityZoneMappings": [
      {
        "logicalZone": "1",
        "physicalZone": "eastus-az1"
      },
      {
        "logicalZone": "2",
        "physicalZone": "eastus-az3"
      },
      {
        "logicalZone": "3",
        "physicalZone": "eastus-az2"
      }
    ],
    "displayName": "East US",
    "name": "eastus"
  }
]
```

Note: Replace `eastus` in the command with your desired region name.

</details>

- **VPC CIDR:** A /16 CIDR block that defines the IP address range for your Data Plane's Virtual Private Cloud (VPC).
  Default value: 10.0.0.0/16.
- **Kubernetes Service CIDR:** A /16-/20 CIDR block used for Kubernetes service cluster IP addresses in your Data Plane.
  This CIDR range is used internally by Kubernetes to assign IP addresses to services running in the cluster. Default
  value: 172.20.0.0/16.

:::warning Important When choosing your VPC CIDR and Kubernetes Service CIDR:

- Consider your current and future network topology, ensuring these CIDR ranges don't conflict with existing network
  routes or address spaces, especially for VPC peering or VPN connections.
- Consult with your network administrator if you're unsure about potential conflicts.
- Remember that VPC CIDR and Kubernetes Service CIDR cannot be modified once the Data Plane has been created.
- Kubernetes Service CIDR is only required for AWS.

:::

Data Plane Configuration form for AWS:

<Thumbnail src="/img/data-plane/vpc-form.png" alt="Data Plane Creation Form" width="800px" />

<br></br>

Data Plane Configuration form for GCP and Azure:

<Thumbnail src="/img/data-plane/vpc-form-gcp-azure.png" alt="Data Plane Creation Form" width="800px" />

### Step 3. Create and Monitor your Data Plane

Click the `Create` button after filling out all required fields. The creation process will begin, and you'll see your
Data Plane listed with a `Creating` status. The creation process typically takes 60 minutes to complete for AWS and GCP
deployments, and 2-3 business days for Azure deployments.

<Thumbnail src="/img/data-plane/data-plane-page.png" alt="Data Plane Page" width="1000px" />

<br></br>

Click on the `Creating` button in the status column to see detailed progress.

<Thumbnail src="/img/data-plane/provisioning-status-modal.png" alt="Data Plane Creating Status" width="100px" />

## After Creation

Once your data plane is successfully created and the status updates to Active, you and your team can begin
[creating projects](/private-ddn/create-a-project-on-a-data-plane.mdx) on your Data Plane. To access more information
about a specific Data Plane, simply click on its name within the Data Planes table. This action will navigate you to a
detailed view page of the selected Data Plane, as illustrated below.

<Thumbnail src="/img/data-plane/data-plane-detail.png" alt="Data Plane Detail" width="1000px" />

## Next steps

Now that you've created a Data Plane on dedicated Private DDN,
[learn how to add collaborators](/private-ddn/data-plane-collaboration.mdx) so they can create projects.



==============================



# byoc.mdx

URL: https://hasura.io/docs/promptql/private-ddn/creating-a-data-plane/byoc


# Bring Your Own Cloud (BYOC): Create a Data Plane

## Introduction

Private DDN BYOC allows you to deploy the Data Plane component in your own cloud environment. This approach is useful if
you have specific requirements or constraints which prevent you from using the dedicated Data Plane on Hasura's cloud.

BYOC is currently supported on AWS, GCP and Azure.

If you would like access to Private DDN BYOC, please [contact sales](https://hasura.io/contact-us).

## 📌 Onboarding Process

To get started with BYOC, customers are required to have one of the following:

- A dedicated AWS Account
- A dedicated project on Google Cloud
- A dedicated Resource Group on Microsoft Azure

## Instructions

### AWS

The setup involves creating an IAM role in your AWS account that establishes a trust relationship with Hasura's AWS automation role (PulumiDDNCli). This role will be used to deploy and manage workloads necessary for Hasura DDN.

##### Pre-requisites

- Dedicated AWS Account with administrative access
- AWS CLI installed and configured
- `AWS_REGION` environment variable set to your desired region (e.g., `export AWS_REGION=us-east-1`)
- Ensure the AWS region where you plan to deploy is enabled in your account
  ```bash
  aws account get-regions --region-opt-status-contains ENABLED --query 'Regions[*].RegionName'
  ```

##### Setup Instructions

1. Copy the following template and save it as `cloudformation.yaml`

<details>

<summary>cloudformation.yaml</summary>
```bash
Resources:
  BootstrapRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: HasuraCloudBYOC
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              AWS: arn:aws:iam::760537944023:role/PulumiDDNCli
            Action: sts:AssumeRole
            Condition:
              StringEquals:
                sts:ExternalId: hasura-cloud
  BootstrapPolicy:
    Type: AWS::IAM::Policy
    Properties:
      PolicyName: HasuraCloudBYOC
      Roles:
        - !Ref BootstrapRole
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Action:
              - ec2:DescribeAddresses
              - ec2:DescribeAddressesAttribute
              - ec2:DescribeAvailabilityZones
              - ec2:DescribeInternetGateways
              - ec2:DescribeNatGateways
              - ec2:DescribeNetworkInterfaces
              - ec2:DescribeRegions
              - ec2:DescribeRouteTables
              - ec2:DescribeSecurityGroupRules
              - ec2:DescribeSecurityGroups
              - ec2:DescribeSubnets
              - ec2:DescribeTags
              - ec2:DescribeVpcAttribute
              - ec2:DescribeVpcs
              - eks:DeleteAddon
              - eks:DescribeAddon
              - eks:DescribeCluster
              - eks:DescribeNodegroup
              - eks:ListClusters
              - iam:GetRole
              - iam:GetServiceLinkedRoleDeletionStatus
              - sqs:GetQueueAttributes
            Resource: '*'
          - Effect: Allow
            Action:
              - ec2:AllocateAddress
              - ec2:AssociateAddress
              - ec2:AssociateRouteTable
              - ec2:CreateInternetGateway
              - ec2:CreateNatGateway
              - ec2:CreateRoute
              - ec2:CreateRouteTable
              - ec2:CreateSubnet
              - ec2:CreateTags
              - ec2:CreateVpc
              - eks:CreateCluster
              - eks:CreateNodegroup
              - globalaccelerator:CreateAccelerator
              - globalaccelerator:CreateEndpointGroup
              - globalaccelerator:CreateListener
              - globalaccelerator:TagResource
              - sqs:CreateQueue
              - sqs:TagQueue
              - acm:RequestCertificate
              - events:PutRule
              - events:TagResource
              - iam:CreateOpenIDConnectProvider
              - iam:TagOpenIDConnectProvider
            Resource: '*'
            Condition:
              StringEquals:
                aws:RequestTag/Created-By: HasuraCloud
          - Effect: Allow
            Action:
              - ec2:CreateTags
            Resource:
              - !Sub arn:aws:ec2:*:${AWS::AccountId}:security-group/*
            Condition:
              StringEquals:
                aws:RequestTag/karpenter.sh/discovery: dataplane
          - Effect: Allow
            Action:
              - ec2:DeleteTags
            Resource:
              - !Sub arn:aws:ec2:*:${AWS::AccountId}:security-group/*
            Condition:
              StringEquals:
                aws:ResourceTag/karpenter.sh/discovery: dataplane
          - Effect: Allow
            Action:
              - eks:AssociateAccessPolicy
              - eks:DisassociateAccessPolicy
            Resource:
              - !Sub arn:aws:eks:*:${AWS::AccountId}:access-entry/dataplane/*
          - Effect: Allow
            Action:
              - iam:AttachRolePolicy
              - iam:CreateInstanceProfile
              - iam:CreatePolicy
              - iam:CreateRole
              - iam:DeleteInstanceProfile
              - iam:DeleteOpenIDConnectProvider
              - iam:DeletePolicy
              - iam:DeleteRole
              - iam:DeleteServiceLinkedRole
              - iam:DetachRolePolicy
              - iam:GetInstanceProfile
              - iam:GetOpenIDConnectProvider
              - iam:GetPolicy
              - iam:GetPolicyVersion
              - iam:GetRolePolicy
              - iam:ListAttachedRolePolicies
              - iam:ListInstanceProfilesForRole
              - iam:ListOpenIDConnectProviderTags
              - iam:ListPolicyVersions
              - iam:ListRolePolicies
              - iam:PassRole
              - iam:PutRolePolicy
              - iam:RemoveRoleFromInstanceProfile
              - iam:TagInstanceProfile
              - iam:TagOpenIDConnectProvider
              - iam:TagPolicy
              - iam:TagRole
            Resource:
              # Roles
              - !Sub arn:aws:iam::${AWS::AccountId}:role/KarpenterNodeRole
              - !Sub arn:aws:iam::${AWS::AccountId}:role/eksClusterRole-*
              - !Sub arn:aws:iam::${AWS::AccountId}:role/lb-controller-*
              - !Sub arn:aws:iam::${AWS::AccountId}:role/autoscaler-controller-*
              - !Sub arn:aws:iam::${AWS::AccountId}:role/global-accelerator-operator-*
              - !Sub arn:aws:iam::${AWS::AccountId}:role/karpenter-controller-*
              - !Sub arn:aws:iam::${AWS::AccountId}:role/HasuraWorkloadAutomationRole-*
              - !Sub arn:aws:iam::${AWS::AccountId}:role/vpc-cni-*
              - !Sub arn:aws:iam::${AWS::AccountId}:role/ebsCsiDriverRole-*
              # Instance Profiles
              - !Sub arn:aws:iam::${AWS::AccountId}:instance-profile/dataplane_*
              # Policies
              - !Sub arn:aws:iam::${AWS::AccountId}:policy/lb-controller-*
              - !Sub arn:aws:iam::${AWS::AccountId}:policy/autoscaler-controller-*
              - !Sub arn:aws:iam::${AWS::AccountId}:policy/global-accelerator-operator-*
              - !Sub arn:aws:iam::${AWS::AccountId}:policy/karpenter-controller-*
              # OIDC Providers
              - !Sub arn:aws:iam::${AWS::AccountId}:oidc-provider/oidc.eks.*
              # Service Roles
              - !Sub arn:aws:iam::${AWS::AccountId}:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot
          - Effect: Allow
            Action:
              - iam:CreateServiceLinkedRole
            Resource: '*'
            Condition:
              StringEquals:
                iam:AWSServiceName:
                  - spot.amazonaws.com
                  - globalaccelerator.amazonaws.com
                  - eks.amazonaws.com
                  - eks-nodegroup.amazonaws.com
          - Effect: Allow
            Action:
              - eks:*
              - globalaccelerator:*
              - sqs:*
              - acm:*
              - events:*
            Resource: '*'
            Condition:
              StringEquals:
                aws:ResourceTag/Created-By: HasuraCloud
          - Effect: Allow
            Action:
              - ec2:*
            Resource: '*'
            Condition:
              StringEquals:
                ec2:ResourceTag/Created-By: HasuraCloud

Outputs:
  RoleArn:
    Description: "ARN of the HasuraCloudBYOC IAM Role"
    Value: !GetAtt BootstrapRole.Arn
```
</details>

2. Deploy the CloudFormation stack:

   First, check if the stack already exists:
   ```bash
   aws cloudformation describe-stacks --stack-name hasura-cloud-byoc
   ```

   Then, based on the result:
   - If you see an error "Stack with id hasura-cloud-byoc does not exist":
     ```bash
     # Create new stack
     aws cloudformation create-stack \
       --stack-name hasura-cloud-byoc \
       --template-body file://cloudformation.yaml \
       --capabilities CAPABILITY_NAMED_IAM

     # Wait for creation to complete
     aws cloudformation wait stack-create-complete \
       --stack-name hasura-cloud-byoc
     ```

   - If the stack exists:
     ```bash
     # Update existing stack
     aws cloudformation update-stack \
       --stack-name hasura-cloud-byoc \
       --template-body file://cloudformation.yaml \
       --capabilities CAPABILITY_NAMED_IAM

     # Wait for update to complete
     aws cloudformation wait stack-update-complete \
       --stack-name hasura-cloud-byoc
     ```

3. Monitor stack status:
   ```bash
   aws cloudformation describe-stacks \
     --stack-name hasura-cloud-byoc \
     --query 'Stacks[0].StackStatus'
   ```

4. Once complete, retrieve the Role ARN:
   ```bash
   aws cloudformation describe-stacks \
     --stack-name hasura-cloud-byoc \
     --query 'Stacks[0].Outputs[?OutputKey==`RoleArn`].OutputValue' \
     --output text
   ```

##### Required Information

Share the following with the Hasura team:

- (Required) Role ARN (From output above)
- (Required) AWS Region
- (Optional) Preferred Availability Zones
  - Use AZ IDs (e.g., use1-az1, use1-az2) instead of AZ names (us-east-1a, us-east-1b)
  - You can get the AZ IDs by running:
    ```bash
    aws ec2 describe-availability-zones \
    --region <region> \
    --output table \
    --query "AvailabilityZones[?State=='available'] | [].{ZoneName: ZoneName, ZoneId: ZoneId}"
    ```
  - If you have specific zones which you'd like to use, please pass it along. Otherwise, Hasura will assign accordingly.
- (Optional) VPC CIDR (/16 CIDR)
  - If you have a specific CIDR in mind for the VPC setup, please pass it along. If not specified, Hasura will assign 10.0.0.0/16.
  - Note: If you are planning to use VPC Peering, this CIDR should not conflict with any networks on your side.
- (Optional) Kubernetes Service CIDR (/16-20 CIDR)
  - A /16-/20 CIDR block used for Kubernetes service cluster IP addresses in your Data Plane. If not specified, Hasura will assign 172.20.0.0/16.
  - Note: If you are planning to use VPC Peering, this CIDR should not conflict with any networks on your side.

***

### GCP

The setup involves enabling APIs & creating IAM policy bindings; The IAM policy bindings establish a trust relationship
with Hasura’s GCP service account which will be used to deploy and manage workloads necessary for Hasura DDN.

##### Pre-requisites

- Dedicated GCP Project

##### Setup instructions

- Enable APIs within GCP project:

```bash
gcloud services enable \
  compute.googleapis.com \
  dns.googleapis.com \
  gkehub.googleapis.com \
  multiclusterservicediscovery.googleapis.com \
  trafficdirector.googleapis.com \
  multiclusteringress.googleapis.com \
  container.googleapis.com \
  certificatemanager.googleapis.com --project ${GCP_PROJECT_ID}
```

- Create following IAM Policy bindings for Hasura's `ddn-automation` service account:

<details>

<summary>IAM Policy bindings</summary>
```bash
gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member "serviceAccount:ddn-automation@hasura-ddn.iam.gserviceaccount.com" \
  --role roles/compute.networkAdmin

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member "serviceAccount:ddn-automation@hasura-ddn.iam.gserviceaccount.com" \
  --role roles/dns.admin

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member "serviceAccount:ddn-automation@hasura-ddn.iam.gserviceaccount.com" \
  --role roles/gkehub.editor

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member "serviceAccount:ddn-automation@hasura-ddn.iam.gserviceaccount.com" \
  --role roles/logging.configWriter

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member "serviceAccount:ddn-automation@hasura-ddn.iam.gserviceaccount.com" \
  --role roles/container.clusterAdmin

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member "serviceAccount:ddn-automation@hasura-ddn.iam.gserviceaccount.com" \
  --role roles/monitoring.metricsScopesAdmin

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member "serviceAccount:ddn-automation@hasura-ddn.iam.gserviceaccount.com" \
  --role roles/certificatemanager.editor

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member='serviceAccount:ddn-automation@hasura-ddn.iam.gserviceaccount.com' \
  --role='roles/resourcemanager.projectIamAdmin' \
  --condition='^:^title=Restrict IAM Granting for ddn-automation:description=Restrict ddn-automation to granting specific roles to specific members:expression=api.getAttribute("iam.googleapis.com/modifiedGrantsByRole", []).hasOnly(["roles/container.defaultNodeServiceAccount"]) || api.getAttribute("iam.googleapis.com/modifiedGrantsByRole", []).hasOnly(["roles/container.admin"]) || api.getAttribute("iam.googleapis.com/modifiedGrantsByRole", []).hasOnly(["roles/compute.networkViewer"])'

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member "serviceAccount:ddn-automation@hasura-ddn.iam.gserviceaccount.com" \
  --role roles/container.admin

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member "serviceAccount:ddn-automation@hasura-ddn.iam.gserviceaccount.com" \
  --role roles/iam.serviceAccountAdmin

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member "serviceAccount:ddn-automation@hasura-ddn.iam.gserviceaccount.com" \
  --role roles/iam.workloadIdentityPoolAdmin
```
</details>

##### Provide Required Information

Share the following with the Hasura team:

- (Required) Project ID
- (Required) GCP Region
- (Optional) Preferred Availability Zones
  - If you have specific zones which you'd like to use, please pass it along.  Otherwise, Hasura will assign accordingly.
- (Optional) VPC CIDR (/16 CIDR)
  - If you have a specific CIDR in mind for the VPC setup, please pass it along.  If not specified, Hasura will assign 10.0.0.0/16.
  - Note: If you are planning to use VPC Peering, this CIDR should not conflict with any networks on your side.

***

### Azure

##### Pre-requisites

A dedicated resource group.  If you do not have one, create it.

##### Create App Entities for BYOC

###### For infra provisioning:

- Replace `$(customer_tenant_id)` with your Azure Tenant ID in the URL below:
   `https://login.microsoftonline.com/$(customer_tenant_id)/oauth2/authorize?client_id=4f7f1f59-f0b0-4adb-8603-2afacc50552b&response_type=code`
- Then open the URL in your browser to create the app entity named **PrivateDdnInfra** in your Azure account.
  1.  If you encounter a redirect error ("No reply address is registered for the application") after clicking on
      Approve, ignore this. The actual app will still be created behind the scenes successfully.

###### For workload provisioning:

- Replace `$(customer_tenant_id)` with your Azure Tenant ID in the URL below:
   `https://login.microsoftonline.com/$(customer_tenant_id)/oauth2/authorize?client_id=ce413986-b5c3-4e49-b45e-093f07776c14&response_type=code`
- Then open the URL in your browser to create the app entity named **ArgoCDDeployer** in your Azure account.
  1.  If you encounter a redirect error ("No reply address is registered for the application) after clicking on Approve,
      ignore this. The actual app will still be created behind the scenes successfully.

#### Grant Required Roles

Assign the following roles at the resource group level:

- For `PrivateDdnInfra`:
  - Contributor
  - Role Based Access Control (RBAC) Administrator
    1. Allow user to assign all roles except the roles you select
    2. Configure the constrained roles by adding Owner to exclude roles
- For `ArgoCDDeployer`:
  - Azure Kubernetes Service RBAC Cluster Admin

#### Additional requirements

- Register the `EnableAPIServerVnetIntegrationPreview` feature flag using the az feature register command: `az feature register --namespace "Microsoft.ContainerService" --name "EnableAPIServerVnetIntegrationPreview"`
- Hasura provisioning requires that various node pools be created in order to support DDN workloads. Hasura will be
provisioning 2 vCPU/8GB RAM instance types for this purpose (Using x64 VM Architecture). We therefore require that CPU
related quotas are set accordingly by the customer on a desired instance family type in order to allow Hasura the
ability to provision nodes without any issues. A base configuration of worker nodes will consume 20 vCPU. We however
recommend setting an appropriate threshold in order to accommodate workers scaling up and down.

#### Provide Required Information

Get the following details from your
[Azure Portal](https://portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/Overview) and pass them along
to the Hasura team.

- (Required) Subscription ID
- (Required) Tenant ID
- (Required) Resource Group Name
- (Required) Azure Region
- (Optional) Preferred Availability Zones
  - If you have specific zones which you'd like to use, please pass it along.  Otherwise, Hasura will assign accordingly.
- (Optional) VPC CIDR (/16-/19 CIDR)
  - If you have a specific CIDR in mind for the VPC setup, please pass it along.  If not specified, Hasura will assign 10.0.0.0/16.
  - Note: If you are planning to use VPC Peering, this CIDR should not conflict with any networks on your side.



==============================



# data-plane-collaboration.mdx

URL: https://hasura.io/docs/promptql/private-ddn/data-plane-collaboration


# How to Add Collaborators to a Data Plane

## Introduction

The Data Plane collaboration feature allows you to manage and facilitate collaboration on the Data Plane for which you
are an owner. The owner of the Data Plane can invite other users to collaborate/create projects on the Data Plane. The
invited users can create and manage projects on the Data Plane. This feature enables users to invite, accept, reject,
and remove collaborators for multi-team collaboration.

## How to get started with Data Plane collaboration

### Inviting a collaborator

To invite a user to your Data Plane, you need to open the
[Data Plane Management Dashboard](https://promptql.console.hasura.io/data-plane/). The dashboard will show all available
Data Planes. Select the Data Plane for which you have the `owner` role. Click `Invite Collaborator`. Enter the email
address of the user you want to invite and click `Send Invites`. The invited user will receive an email with an
invitation link.

<Thumbnail src="/img/deployment/invite_data_plane_collaborator.png" alt="Invite Data Plane Collaborator" />

### Accepting or rejecting an invitation

The invited user can accept or reject the invitation by clicking on the invitation link received in the email or going
to the [Data Plane Management Dashboard](https://promptql.console.hasura.io/data-plane/). The dashboard will show all
the invites received by the user. The user can accept or reject the invitation by clicking `Accept` or `Decline`.

<Thumbnail src="/img/deployment/accept_reject_data_plane_invitation.png" alt="Accept or Reject Data Plane Invitation" />

### Removing a collaborator

You can remove any Data Plane collaborator by going to the
[Data Plane Management Dashboard](https://promptql.console.hasura.io/data-plane/). Select the Data Plane for which you
have the `owner` role, you'll be able to see all the collaborators of the Data Plane. Click `Remove` to remove the
collaborator.

<Thumbnail src="/img/deployment/remove_data_plane_collaborator.png" alt="Remove Data Plane Collaborator" />

## Data Plane collaboration permissions

Currently there are only two roles available for Data Plane collaboration:

- `owner`: The owner of the Data Plane has full access to the Data Plane and can invite or remove collaborators.
- `member`: The member of the Data Plane can create projects on the Data Plane and manage them.

## Next steps

Learn how to [create a project on a Private Data Plane](/private-ddn/create-a-project-on-a-data-plane.mdx).



==============================



# create-a-project-on-a-data-plane.mdx

URL: https://hasura.io/docs/promptql/private-ddn/create-a-project-on-a-data-plane


# Create a Project on a Data Plane

## Introduction

:::tip Important

This guide will walk you through the steps for creating a project on a Data Plane in
[Private Hasura DDN](/private-ddn/overview.mdx). Before proceeding, you'll first need to be
[invited](/private-ddn/data-plane-collaboration.mdx) to a Data Plane.

If you are unable to meet the prerequisites listed below or your data source is inaccessible from the location where you
run DDN CLI commands, consider using a [DDN Workspace](/private-ddn/ddn-workspace.mdx) instead.

:::


## Prerequisites

**Install the DDN CLI**

:::info Minimum version requirements

To use this guide, ensure you've installed/updated your CLI to at least `v2.28.0`.

:::


<Tabs groupId="os-preference" className="api-tabs">

<TabItem value="macOS-or-linux" label="macOS and Linux">

Simply run the installer script in your terminal:

<CodeBlock language="bash">
  {`curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/get.sh | bash`}
</CodeBlock>

<Admonition type="info" title="ARM-based Linux Machines">
  Currently, the CLI does not support installation on ARM-based Linux systems.
</Admonition>

</TabItem>

<TabItem value="windows" label="Windows">
- Download the latest <a href={`https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/latest/DDN_CLI_Setup.exe`}>DDN CLI installer for Windows.</a> 
- Run the `DDN_CLI_Setup.exe` installer file and follow the instructions. This will only take a minute.
- By default, the DDN CLI is installed under `C:\Users\{Username}\AppData\Local\Programs\DDN_CLI`
- The DDN CLI is added to your `%PATH%` environment variable so that you can use the `ddn` command from your terminal.
<br />

</TabItem>
</Tabs>


**Install [Docker](https://docs.docker.com/engine/install/)**

The Docker-based workflow helps you iterate and develop locally without deploying any changes to Hasura DDN, making the
development experience faster and your feedback loops shorter. **You'll need Docker Compose `v2.20` or later.**

**Validate the installation**

You can verify that the DDN CLI is installed correctly by running:

```ddn
ddn doctor
```


## Step 1. Authenticate your CLI

```ddn title="Being by authenticating your CLI seession:"
ddn auth login
```

A browser window will open and prompt you for your login credentials. After successfully logging in, you'll be able to
return to the CLI.

The DDN CLI will scaffold out all the necessary files and directories for your project.

## Step 2. Scaffold out a new local project

```ddn title="Next, create a new local project and pass your data plane's ID:"
ddn supergraph init my-project --with-promptql --project-data-plane-id <data-plane-id> && cd my-project
```

Once you move into this directory, you'll see your project scaffolded out for you. You can view the structure by either
running ls in your terminal, or by opening the directory in your preferred editor.

## Step 3. Add a data source

The command below will launch a wizard via the DDN CLI which will assist you in adding a data connector. You can learn
more about adding sources via data connectors [here](/data-sources/connect-to-a-source.mdx).

```ddn title="Add a data source using the CLI:"
ddn connector init <connector_name> -i
```

This will provision the necessary files — using your configuration — for connecting a data source to your supergraph
API.

:::info Organize your data into subgraphs

You can also organize your API into **subgraphs**. Subgraphs are generally organized around team functions or business
outcomes and allow for independent ownership and governance of data sources. Learn more
[here](/project-configuration/subgraphs/index.mdx).

:::

## Step 4. Generate your metadata

```ddn title="First, introspect your data source:"
ddn connector introspect <connector_name>
```

```ddn title="Then, generate the metadata:"
ddn model add <connector_name> '*'
ddn command add <connector_name> '*'
ddn relationship add <connector_name> '*'
```

You'll see metadata files generated for each entity in your data source. You can learn more
[here](/data-modeling/overview.mdx).

## Step 5. Create a new local build

```ddn title="You can now create a local build:"
ddn supergraph build local
```

```ddn title="Then run your local services, including the Hasura DDN Engine and your connector(s):"
ddn run docker-start
```

```ddn title="Finally, open the local development console usign the CLI:"
ddn console --local
```

The development console will open wherein you can now chat with your data.

## Step 6. Build and deploy your supergraph

When you're ready, you can create a cloud build.

```ddn title="Build and deploy you supergraph:"
ddn supergraph build create
```

When this process is complete, the CLI will return a link to the hosted application where you can run talk to your data.

## Next steps

With your first project created, learn more about how Hasura handles [data modeling](/data-modeling/overview.mdx) before
diving into advanced features.



==============================



# index.mdx

URL: https://hasura.io/docs/promptql/private-ddn/private-endpoints/

# Private Endpoints

Private Endpoints provide secure, private connectivity between your infrastructure and Hasura's Data Planes without exposing traffic to the public internet. This feature is essential for enterprises with strict security and compliance requirements.

## Supported Cloud Providers

Private Endpoints are currently supported on:

- **AWS**: Using AWS PrivateLink
- **Azure**: Using Azure Private Link (coming soon)
- **GCP**: Using Private Service Connect (coming soon)

## Next Steps

Ready to set up Private Endpoints? Choose your cloud provider:

- [AWS](/private-ddn/private-endpoints/aws.mdx)



==============================



# aws.mdx

URL: https://hasura.io/docs/promptql/private-ddn/private-endpoints/aws


# AWS Private Endpoints

## Overview

AWS Private Endpoints leverage AWS PrivateLink technology to establish secure, private connectivity between your VPC and Hasura's Data Plane. This private connection ensures that your data traffic never traverses the public internet, significantly enhancing security and reducing exposure to potential threats.

### How It Works

AWS PrivateLink uses AWS's internal network to create private connections between services. The process works as follows:

1. You create a VPC Endpoint Service in your AWS account, backed by a Network Load Balancer
2. You provide Hasura with your VPC Endpoint Service name
3. Hasura creates a VPC Endpoint in the Data Plane VPC that connects to your endpoint service
4. AWS creates Elastic Network Interfaces (ENIs) in the Data Plane subnets
5. The Data Plane connects to your services through these ENIs
6. All traffic remains on the AWS private network

## Prerequisites

Before creating an AWS Private Endpoint, ensure you have the following:

1. **A Private DDN Data Plane deployed in AWS**
   - Your Data Plane must be deployed and in an active state
   - The Data Plane region should match the region where you want to create the private endpoint

2. **An AWS VPC Endpoint Service in your AWS account**
   - Created and configured with a Network Load Balancer
   - In the same region as your Data Plane
   - - The "Acceptance Required" setting can be enabled or disabled based on your security requirements

3. **Appropriate IAM permissions**
   - Permissions to manage your VPC Endpoint Service
   - Permissions to add allowed principals to your endpoint service
   - Typically requires the `ec2:ModifyVpcEndpointServicePermissions` permission

## Creating a Private Endpoint

### Step 1: Service Principal Information

1. Navigate to your Data Plane details page in the Hasura Console
2. Click the "Create Private Endpoint" button
3. In the modal, you'll see the **Hasura IAM Role ARN** that needs to be added to your endpoint service permissions
   - Format: `arn:aws:iam::DATAPLANE_ACCOUNT_ID:role/ROLE_NAME`
   - This is the AWS IAM role that Hasura's Data Plane will use to connect to your VPC Endpoint Service
   - You must add this ARN to the allowed principals in your VPC Endpoint Service settings

4. Add this ARN to your VPC Endpoint Service permissions in your AWS account:
   - Go to the AWS Console > VPC > Endpoint Services
   - Select your endpoint service
   - Under "Allowed principals", click "Add allowed principals"
   - Paste the Hasura IAM Role ARN and save

5. Enter your VPC Endpoint Service name in the input field
   - Format: `com.amazonaws.vpce.REGION.vpce-svc-XXXXXXXX`
   - You can find this in your AWS Console under VPC > Endpoint Services > Service name

6. Click "Validate" to proceed

> **Note**: If validation fails, verify that you've correctly added the Hasura IAM Role ARN to your endpoint service's allowed principals list and that you've entered the correct service name.

<Thumbnail src="/img/data-plane/validate-private-endpoint-form.png" alt="Validate Private Endpoint Form" />

### Step 2: Select Availability Zones and Configure Options

1. After successful validation, you'll be prompted to select availability zones

2. Choose the availability zones where you want Hasura to create the endpoint
   - It's recommended to select at least two zones for high availability
   - Only zones that match with the availability of your VPC Endpoint Service will be available for selection
   - Each selected zone will create an Elastic Network Interface (ENI) in the Data Plane VPC
   - Select zones that align with where your VPC Endpoint Service is deployed

3. Enter a name for your endpoint
   - The name must be unique within your Data Plane
   - Name must be between 2 and 20 characters
   - Name must start with a lowercase letter
   - Name can only contain lowercase letters and numbers
   - This name will be used to identify the endpoint in the Hasura Console

> **Best Practice**: Select availability zones where your application workloads run to minimize cross-AZ data transfer costs and latency.

<Thumbnail src="/img/data-plane/endpoint-creation-form.png" alt="Endpoint Creation Form" />

### Step 3: Create the Endpoint

1. Review your selections and click "Create"
2. The system will initiate the endpoint creation process, which typically takes 5-10 minutes
3. You'll be returned to the Data Plane details page where you can monitor the endpoint status
4. If you configured your VPC Endpoint Service to require acceptance, the endpoint will initially show a "Pending Acceptance" status
5. In this case, you must accept the endpoint connection request in your AWS account:
   - Go to the AWS Console > VPC > Endpoint Services
   - Find your endpoint service
   - Under the "Endpoint connections" tab, find the pending connection
   - Select it and click "Accept endpoint connection"
6. After acceptance (or immediately if acceptance is not required), the status will change to "In Progress" and then "Active" when ready

<Thumbnail src="/img/data-plane/private-endpoint-table.png" alt="Private Endpoint Table" />

## Updating a Private Endpoint

You can modify your private endpoint settings after creation to adjust availability zones or enable private DNS. Note that only endpoints in the **Active** status can be updated.

### Step 1: Access the Update Form

1. Navigate to your Data Plane details page in the Hasura Console
2. Locate the private endpoint you want to edit in the Private Endpoints table
3. Click the edit (pencil) icon next to the endpoint

### Step 2: Configure Update Options

1. In the "Edit Endpoint" modal that appears, you can:
   - Select additional availability zones from the dropdown menu
     - Only zones that aren't already configured will be available
     - You cannot remove existing zones, only add new ones
     - Additional zones must be supported by your VPC Endpoint Service
   - Toggle the "Enable Private DNS" option to use AWS private DNS names
     - This option is only available during edit/update operations, not during initial creation
     - Private DNS enablement allows you to use the service's private DNS name instead of the endpoint-specific DNS names
     - Note that private DNS can only be enabled for AWS VPC endpoint services that provide a private DNS name

### Step 3: Apply the Update

1. Review your changes and click "Update" to apply them
2. The endpoint will enter the "In Progress" state while changes are being applied
3. Changes typically take 5-10 minutes to complete
4. You'll be returned to the Data Plane details page where you can monitor the endpoint status

<Thumbnail src="/img/data-plane/edit-private-endpoint.png" alt="Update Private Endpoint" />

## Deleting a Private Endpoint

When you no longer need a private endpoint, you can delete it from your Data Plane. This action cannot be undone and will remove the connection between your VPC and Hasura's Data Plane. Note that only endpoints in the **Active**, **Failed**, or **Rejected** status can be deleted.

### Step 1: Initiate Deletion

1. Navigate to your Data Plane details page in the Hasura Console
2. Locate the private endpoint you want to delete in the Private Endpoints table
3. Click the delete (trash) icon next to the endpoint

### Step 2: Confirm Deletion

1. In the confirmation modal that appears, review the endpoint name to confirm it's the correct endpoint
2. The modal will display a warning message: "Are you sure you want to delete the private endpoint [endpoint-name]? This action cannot be undone."
3. Click "Delete Endpoint" to confirm and proceed with deletion
4. Click "Cancel" if you want to abort the deletion process

### Step 3: Monitor Deletion Progress

1. If confirmed, the endpoint will enter the "Deleting" state while being removed
2. The deletion process typically takes 5-10 minutes to complete
3. You'll be returned to the Data Plane details page where you can monitor the endpoint status

<Thumbnail src="/img/data-plane/delete-private-endpoint.png" alt="Delete Private Endpoint" />

## Endpoint Status Lifecycle

AWS Private Endpoints go through several status states during their lifecycle:

| Status | Description | Action Required | Available Operations |
|--------|-------------|----------------|----------------------|
| **Pending Acceptance** | The connection request has been sent to your AWS account and is waiting for acceptance | Accept the endpoint connection in AWS Console | None |
| **In Progress** | The connection has been accepted and the endpoint is being created or modified | No action needed; wait for completion | None |
| **Active** | The endpoint is active and ready for use | You can now use the endpoint DNS addresses | Update, Delete |
| **Deleting** | The endpoint is in the process of being deleted | No action needed; wait for completion | None |
| **Failed** | The endpoint creation failed | Contact Hasura support for assistance | Delete |
| **Rejected** | The request was rejected by the service provider | Contact Hasura support for assistance | Delete |
| **Expired** | The connection request expired | Contact Hasura support for assistance | None |

## Using Private Endpoints

Once your endpoint is in the "Active" status, you can use it to establish secure connectivity between Hasura's Data Plane and your data sources. Here's what you need to know:

1. The endpoint DNS addresses will be displayed in the private endpoints table
2. These DNS addresses represent the connection between Hasura's Data Plane and your VPC Endpoint Service
3. For Hasura to connect to your data sources through this private connection, you must configure your data sources in Hasura using the appropriate DNS names

### Connecting Your Data Sources

The private endpoint enables secure connectivity between your VPC (where your data sources are) and Hasura's Data Plane. Here's how it works:

1. Your data sources (databases, APIs, etc.) in your VPC are accessible through your VPC Endpoint Service
2. Hasura's Data Plane connects to your VPC Endpoint Service through the VPC Endpoint
3. This ensures that traffic between Hasura's Data Plane and your data sources remains on the private AWS network
4. Example architecture:

```
Hasura Data Plane VPC <---> VPC Endpoint <---> Your VPC Endpoint Service <---> Your VPC with data sources
```

> **Important**: When configuring your data sources in Hasura, you must use the DNS names provided by your VPC Endpoint Service to ensure traffic flows through the private connection. These DNS names are typically in the format of your service's DNS name or the endpoint-specific DNS names if private DNS is not enabled.

## Frequently Asked Questions

### Does using private endpoints affect performance?
Private endpoints typically provide more consistent performance compared to public internet connections since they use AWS's private network infrastructure and avoid internet congestion.

### Can I use the same private endpoint for multiple applications?
Yes, a single private endpoint can be used by multiple applications within the same VPC or connected VPCs (via VPC peering or Transit Gateway).

### How are private endpoints billed?
AWS charges for VPC endpoints based on hourly usage and data processing. Refer to the [AWS PrivateLink pricing page](https://aws.amazon.com/privatelink/pricing/) for current rates.

### Can I connect to my VPC Endpoint Service from outside AWS?
No, VPC Endpoint Services are only accessible from within AWS. To access from outside AWS, you would need to set up a VPN or Direct Connect to your AWS VPC.

## Troubleshooting

### Common Issues

| Issue | Possible Causes | Resolution |
|-------|----------------|------------|
| **Endpoint stuck in "Pending Acceptance" status** | - Connection request not accepted<br/>- AWS Console permissions issue<br/>- Endpoint Service requires acceptance | - Accept the endpoint connection in AWS Console under VPC > Endpoint Services<br/>- Verify you have permissions to accept connections |
| **Validation errors** | - Incorrect Hasura IAM Role ARN<br/>- ARN not added to allowed principals<br/>- Incorrect service name | - Verify the ARN is correctly added to your endpoint service permissions<br/>- Check for typos in the service name |
| **Cannot connect to endpoint** | - Security group restrictions<br/>- Route table configuration<br/>- DNS resolution issues | - Contact Hasura support for assistance |
| **Failed to create endpoint** | - Service quota limits<br/>- Network configuration issues | - Contact Hasura support for assistance |

### Before Contacting Support

1. **Verify VPC Endpoint Service Status**:
   - Check the status of your VPC Endpoint Service in AWS Console > VPC > Endpoint Services
   - Ensure it's available and properly configured with your Network Load Balancer

2. **Verify Allowed Principals**:
   - Confirm that the Hasura IAM Role ARN has been added to the allowed principals list
   - Check for any typos or formatting issues in the ARN

3. **Check Service Name**:
   - Verify that you've provided the correct VPC Endpoint Service name to Hasura
   - The format should be `com.amazonaws.vpce.REGION.vpce-svc-XXXXXXXX`

### Getting Help

If you encounter issues with private endpoints, especially if you see Failed, Rejected, or Expired statuses, or if you cannot connect to an endpoint, contact Hasura support through your enterprise support channel. These issues often require investigation on Hasura's side to resolve.



==============================



# Connector Deployment Resources

URL: https://hasura.io/docs/promptql/private-ddn/connector-deployment-resources

# Connector Deployment Resources

## Introduction

Connectors in Hasura DDN can request specific resources for their deployment. These include memory and CPU allocation.
Understanding these limits ensures optimized performance and cost efficiency.

## Default Resource Allocation

By default, connectors have the following resource allocations:

- **CPU**: 100m (0.1 vCPU)
- **Memory**: 256Mi

> **Note:** For Public DDN, resource limits are fixed to 1 vCPU and 2GB RAM and cannot be modified.

## Maximum Resource Limits

Connectors can request resources within the following limits:

- **Maximum CPU**: 4 vCPU
- **Maximum Memory**: 32Gi

These maximum limits can be increased upon request. Contact Hasura support if you need higher resource limits.

## Memory and CPU Specifications

Memory and CPU resources are specified using the same units as Kubernetes:

- ### Memory Resource Units

Limits and requests for memory are measured in bytes. Memory can be expressed as a plain integer or as a fixed-point
number using one of the following quantity suffixes:

- `E`, `P`, `T`, `G`, `M`, `k` (base-10 units)
- `Ei`, `Pi`, `Ti`, `Gi`, `Mi`, `Ki` (power-of-two units)

For example, the following values represent approximately the same amount of memory:

- `128974848`, `129e6`, `129M`, `128974848000m`, `123Mi`

> **Note:** Pay attention to the case of the suffixes. If you request `400m` of memory, this means `0.4` bytes, which is
> likely a mistake. Instead, you might intend to specify `400Mi` (400 mebibytes) or `400M` (400 megabytes).

- ### CPU Resource Units

Limits and requests for CPU resources are measured in CPU units. In Hasura DDN, `1` CPU unit is equivalent to `1`
physical CPU core or `1` virtual core, depending on whether the node is a physical host or a virtual machine.

Fractional requests are allowed. When a connector specifies `cpu: 0.5`, it is requesting half as much CPU time compared
to `1.0` CPU. For CPU resource units, the quantity `0.1` is equivalent to `100m`, which can be read as "one hundred
millicpu." Some refer to this as "one hundred millicores," and it is understood to mean the same thing.

CPU resources are always specified as an absolute amount, never as a relative amount. For example, `500m` CPU represents
the same amount of computing power whether the connector runs on a single-core, dual-core, or multi-core machine.

## Defining Resource Limits in `connector.yaml`

Resource limits can be set at both the top level and per region.

```yaml
title: "Example connector.yaml configuration"
kind: Connector
version: v2
definition:
  name: my_connector
  resources:
    memory: 128M
    cpu: 0.5
  regionConfiguration:
    - region: us-central1
      resources:
        memory: 128M
        cpu: 2
```

### Resource Limit Overrides

When resource limits are defined at both the top level and the region level, the region-specific values take precedence
over the top-level values for that particular region.

For example, if a connector is configured as follows:

```yaml
definition:
  resources:
    memory: 512M
    cpu: 1
  regionConfiguration:
    - region: us-central1
      resources:
        memory: 128M
        cpu: 2
```

- In `us-central1`, the connector will have **128MB memory and 2 vCPUs**, overriding the top-level `512MB` memory and
  `1 vCPU` settings.
- In any other region not explicitly defined, the top-level values (`512MB memory, 1 vCPU`) will apply.

This allows for fine-grained control over resource allocation based on deployment needs.

## Deployment Considerations

- If no region is specified for a connector, it is deployed to a random region.

By optimizing resource configurations, connectors can achieve optimal performance while maintaining cost efficiency in
Hasura DDN.



==============================



# ddn-workspace.mdx

URL: https://hasura.io/docs/promptql/private-ddn/ddn-workspace


# Developing with DDN Workspace

## Introduction

DDN Workspace provides a browser-based development environment for working with Private DDN, eliminating the need to
install DDN CLI and its dependencies locally. It features a familiar VS Code-like UI, runs directly on your data plane,
and includes all necessary dependencies required to work with DDN pre-installed.

This guide will walk you through the steps for using DDN workspace to develop your API on a Data Plane in
[Private Hasura DDN](/private-ddn/overview.mdx). Before proceeding, you'll first need to be
[invited](/private-ddn/data-plane-collaboration.mdx) to a Data Plane.

:::info Prerequisite

To provision a new workspace in Private DDN, please reach out to us [here](https://hasura.io/contact-us).

:::

We recommend a dedicated workspace for each developer working on Private DDN API development. Once the workspace has
been provisioned, it can be launched from the [Private DDN page](https://promptql.console.hasura.io/data-plane).

## Advantages

- Get started with just your browser. No need to install DDN CLI and its dependencies (e.g. Docker Engine) on your local
  machine.
- Network connectivity - Since the workspace runs on your data plane, it has direct access to all data sources and
  services within the data plane network. There is no need to set up complex VPN/network policies to access your DBs
  from your local machine.
- Collaboration - Multiple developers can work on the same project simultaneously within the same workspace, if needed.

## Step 1. Launch a workspace

- To launch a workspace, click on the `Launch Workspace` button on the Private DDN. You can find a list of all available
  Private DDN instances here. This will open a new tab in your browser with the workspace UI.

- Click on the `Copy Password` button to copy the password to your clipboard. Use this password to login to the
  workspace.

<Thumbnail src="/img/data-plane/launch-workspace.png" alt="Launch DDN workspace" width="1000px" />

## Step 2. Create a `<service-account-token>`

:::info Prerequisite

In order to perform a `ddn auth login --access-token <service-account-token>`, you need to have a project created in
your data plane under which a `<service-account-token>` has been generated. If you have already done this, continue to
the next step. Otherwise, proceed with the steps below and ensure that you are executing this on your local machine
since a browser login is required during this step.

**Alternate Approach:**

If you are unable to generate a `<service-account-token>`, you can instead create a Personal Access Token (PAT) by
heading on over to [Hasura dashboard](https://cloud.hasura.io/account-settings/access-tokens) and creating a new access
token there.

:::

:::info Get Data Plane ID

You can find all the Data Planes you have access to on this page: https://promptql.console.hasura.io/data-plane

:::

:::info Available Plans

- ddn_base
- ddn_advanced

:::

```sh title="Login & create project:"
# Perform a ddn auth login using your personal Hasura Cloud account
ddn auth login

# Create a project
ddn project create --data-plane-id <data-plane-id> --plan <plan-name>
```

Next, follow [these](/project-configuration/project-management/service-accounts/#how-to-create-service-account) steps to
generate a `<service-account-token>`.

## Step 3. Authenticate the DDN CLI

Once you're logged into the DDN Workspace, you will be greeted with the familiar VS Code UI. You can perform all typical
development tasks, such as installing extensions, customizing key bindings, and applying themes—all within your browser.

Bring up the terminal using the shortcut `Ctrl` + `` ` ``.

Run the command below to authenticate the CLI with a service account token. If you don't have a token, follow
[these](/project-configuration/project-management/service-accounts/#how-to-create-service-account) steps to generate
one.

```ddn
ddn auth login --access-token <service-account-token>
```

Alternatively, if you have opted to use a Personal Access Token (PAT), run the following command instead:

```ddn
ddn auth login --pat <personal-account-token>
```

## Step 4. Scaffold out a new local project

```ddn title="Next, create a new local project and pass your data plane's ID:"
ddn supergraph init my-project --with-promptql --project-data-plane-id <data-plane-id> && cd my-project
```

After navigating into the directory, you will see the project structure scaffolded for you. You can view the project
structure by running `ls` in the terminal or by exploring it in the VS Code interface.

## Step 5. Add a data source

:::tip

If you have established private connectivity to your data source, you will need to ensure that you are using a
hostname/IP for your database which is specific to your private connectivity connection. This hostname/IP will be
communicated to you by the Hasura team during the coordination of the private connectivity setup.

:::

The command below launches a wizard in the DDN CLI to guide you through adding a data connector. You can learn more
about adding sources via data connectors [here](/data-sources/connect-to-a-source.mdx).

```ddn title="Add a data source using the CLI:"
ddn connector init <connector_name> -i
```

This will provision the necessary files using your configuration for connecting a data source to your supergraph API.

:::info Organize your data into subgraphs

You can also organize your API into **subgraphs**. Subgraphs are generally organized around team functions or business
outcomes and allow for independent ownership and governance of data sources. Learn more
[here](/project-configuration/subgraphs/index.mdx).

:::

## Step 6. Generate your metadata

```ddn title="First, introspect your data source:"
ddn connector introspect <connector_name>
```

```ddn title="Then, generate the metadata:"
ddn model add <connector_name> '*'
ddn command add <connector_name> '*'
ddn relationship add <connector_name> '*'
```

Metadata files will be generated for each entity in your data source. You can learn more
[here](/data-modeling/overview.mdx).

:::info Note

`ddn supergraph build local`, `ddn run docker-start` and `ddn console --local` are not supported in DDN workspace. Once
the metadata is generated, deploy your supergraph to your Private DDN to talk with your data.

:::

## Step 7. Build and deploy your supergraph

```ddn title="Build and deploy your supergraph:"
ddn supergraph build create
```

When this process is complete, the CLI will return a link to the hosted application where you can chat with your data.

## Step 8. Exporting your metadata

To export metadata, right-click the Explorer pane and select `Download`. This will download the entire workspace
directory to your local machine. Since Git is pre-installed in the workspace, you can also configure it and push changes
directly to a remote repository.

<Thumbnail src="/img/data-plane/export-metadata.png" alt="Export metadata" width="1000px" />

## Logout

To log out of the workspace, click the Menu button in the top-left corner and select `Sign out of code-server`. This
will close the workspace and you will be redirected to the workspace login page. The state of your local project will be
preserved and you can login again to continue working on it.

<Thumbnail src="/img/data-plane/workspace-logout.png" alt="Logout from workspace" width="1000px" />



==============================



# data-transfer-cost.mdx

URL: https://hasura.io/docs/promptql/private-ddn/data-transfer-cost


# Data Transfer Costs

This page outlines how data transfer costs are incurred when using Hasura Private DDN.

:::info Only applicable to Dedicated Deployments

Data Transfer costs apply only for Private DDN Dedicated deployments. It does not apply to BYOC deployments.

:::

## Core Principle: Pass-Through Costs

Hasura DDN leverages underlying cloud infrastructure (AWS, GCP, Azure) to operate its Data Planes. Data transfer costs
charged by these cloud providers on the data plane are **passed through directly to the customer as-is**, without any
markup from Hasura. Billing typically aligns with the cloud provider's measurement and billing cycle (usually based on
GB transferred per month) across many dimensions. Each data plane is provisioned on an isolated cloud account so that
the usage is isolated to only that particular data plane.

## How data transfer costs are incurred

Data transfer generally refers to network traffic moving between different network locations. Costs are typically
associated with data _leaving_ a specific network boundary (egress traffic). Common scenarios incurring costs include:

1.  **Data Egress to Internet:** Data sent from the Hasura Data Plane to clients over the public internet. This is often
    the most significant source of data transfer costs (e.g., API query responses).
2.  **Inter-Region Transfer:** Data moving between different cloud regions (e.g., if your Data Plane is in `us-east-1`
    and communicates with a data source or client in `eu-west-1`).
3.  **Inter-Availability Zone (AZ) Transfer:** Data moving between different Availability Zones within the same cloud
    region. Costs for this vary significantly between cloud providers (some offer free transfer within a region).
4.  **Private Network Connections:** Data transferred over dedicated connections like VPC Peering, Private Link, VPN
    Gateways, etc., may have specific costs depending on the cloud provider and the connection type.
5.  **Data Ingress:** Data entering the Hasura Data Plane (e.g., incoming API requests). This is _typically free_ on
    major cloud providers, but always verify with the specific provider's pricing.
6.  **NAT Gateway:** All traffic (ingress and egress) to public IP addresses from within a data plane is going through a
    NAT Gateway, which is probably the most expensive of all the data transfer costs. This typically includes a per hour
    charge and per GB charge on bytes received and bytes sent.

## Cost considerations by deployment model

### Dedicated Data Plane

The Data Plane runs in a VPC managed by Hasura within their cloud account (AWS, GCP, or Azure).

- **Egress Costs:** Primarily driven by API responses sent from the Hasura-managed VPC to the internet or across
  regions/AZs.
- **Private Connectivity Costs:** Data transferred between the Hasura-managed VPC and your customer VPC via VPC Peering,
  Private Link, etc., will incur costs based on the cloud provider's pricing for that specific service.
- **Billing:** Costs are based on the cloud provider Hasura uses for your dedicated instance and its specific regional
  pricing. These costs are passed through on your Hasura invoice.

### BYOC (Bring Your Own Cloud) Data Plane

The Data Plane runs within your own cloud account and VPC.

- **Egress Costs:** Driven by API responses sent from _your_ VPC to the internet (via your NAT Gateway, Internet
  Gateway, etc.). Also includes any traffic leaving your VPC boundary, such as sending observability data or metadata
  communication with the Hasura Control Plane (though this is typically low volume).
- **Internal Transfer:** Traffic between the Data Plane and data sources _within the same VPC and AZ_ is often free, but
  traffic crossing AZs or VPC peering connections within your own environment may incur costs based on your cloud
  provider's policies.
- **Billing:** These data transfer costs will appear directly on _your cloud provider bill_ (AWS, GCP, Azure) as they
  relate to traffic originating from or traversing within your own VPC. Hasura does not bill separately for data
  transfer in the BYOC model, as the costs are incurred directly by the customer with their cloud provider.

## Estimating and monitoring costs

- Data transfer costs are highly dependent on your specific usage patterns: API payload sizes, request volume,
  geographic distribution of clients and data sources, and chosen network architectures.
- Use your cloud provider's cost management and monitoring tools (e.g., AWS Cost Explorer, GCP Billing Reports, Azure
  Cost Management) to track data transfer usage and associated costs, especially for the BYOC model.
- For the Dedicated model, data transfer costs passed through by Hasura will be itemized on your Hasura invoice and will
  be available on you Hasura Console.

## Minimizing costs

- **Optimize Payloads:** Reduce the size of API responses where possible.
- **Regional Locality:** Deploy Data Planes in the same region as your primary data sources and user base, if feasible,
  to minimize cross-region transfer costs.
- **Network Architecture:** Understand your cloud provider's pricing for different connectivity options (e.g., NAT
  Gateway vs. VPC Endpoints, Inter-AZ transfer costs).
- **Use private connectivity**: The most expensive component on data transfer is ingress and egress through a NAT
  Gateway. All internet traffic from the data plane, that is not via a Load Balancer goes through the NAT Gateway. So,
  make sure that you are connecting to your data sourcs via private connectivity options like VPC Peering, Private Link
  etc.

For detailed pricing specifics, always refer to the official documentation of the underlying cloud provider (AWS, GCP,
Azure) for the specific regions and services you are using.

Here are a few helpful links:

- [AWS: Understanding data transfer charges](https://docs.aws.amazon.com/cur/latest/userguide/cur-data-transfers-charges.html)
- [GCP: Network Pricing](https://cloud.google.com/vpc/network-pricing?hl=en)
- [Azure: Bandwidth Pricing](https://azure.microsoft.com/en-us/pricing/details/bandwidth/)



==============================



# Reference

URL: https://hasura.io/docs/promptql/reference/overview

# Reference

## Introduction

In this section of docs, you'll find reference material for:

- [Connectors](/reference/connectors/index.mdx) - How to configure and customize them.
- [Metadata](/reference/metadata-reference/index.mdx) - Configuration reference.
- [CLI](/reference/cli/index.mdx) - How to install and use the Hasura DDN CLI.

This section of documentation is designed to be an exhaustive **reference** of Hasura DDN, the underlying data layer for
PromptQL. To see implementation docs for these concepts, check out the tutorials and guides in sections like
[How to Build With PromptQL](/how-to-build-with-promptql/overview.mdx).



==============================



# index.mdx

URL: https://hasura.io/docs/promptql/reference/metadata-reference/

# Supergraph Modeling

## Introduction

Hasura believes the foundation for constructing your data supergraph should be in modeling it. Your API should be a
reflection of your data, not the other way around.

By focusing on modeling your data sources and business logic, you can create a robust data layer that is easy to
understand and uses a declarative approach to define data models. This empowers you to create a data layer that is
highly efficient and easy to maintain.

## Find out more

- [Working with metadata](/reference/metadata-reference/introduction.mdx)
- [Build configs](/reference/metadata-reference/build-configs.mdx)
- [Data Connector Links](/reference/metadata-reference/data-connector-links.mdx)
- [Types](/reference/metadata-reference/types.mdx)
- [Models](/reference/metadata-reference/models.mdx)
- [Commands](/reference/metadata-reference/commands.mdx)
- [Boolean Expressions](/reference/metadata-reference/boolean-expressions.mdx)
- [OrderBy Expressions](/reference/metadata-reference/orderby-expressions.mdx)
- [Aggregate Expressions](/reference/metadata-reference/aggregate-expressions.mdx)
- [Relationships](/reference/metadata-reference/relationships.mdx)
- [Permissions](/reference/metadata-reference/permissions.mdx)
- [GraphQL API Configuration](/reference/metadata-reference/graphql-config.mdx)
- [Auth Config](/reference/metadata-reference/auth-config.mdx)
- [Compatibility Config](/reference/metadata-reference/compatibility-config.mdx)
- [Engine Plugins](/reference/metadata-reference/engine-plugins.mdx)



==============================



# introduction.mdx

URL: https://hasura.io/docs/promptql/reference/metadata-reference/introduction

# Working with Metadata

## What is metadata?

Your Hasura metadata describes every aspect of your supergraph and is used to build it into the GraphQL schema that
defines your API.

This is done by validating and combining the various objects in your metadata, including subgraphs, connectors, models,
commands, relationships, permissions and more.

Metadata is written in Hasura Metadata Language (HML), which is a declarative extension of YAML.

## How is metadata generated?

When you initialize a new local supergraph project, the [DDN CLI](/reference/cli/index.mdx) scaffolds a set of metadata
files and default subgraphs and configurations for you to start building your API.

The CLI is also used to perform many other metadata creation and management operations such as:

- Creating new subgraphs
- Managing the connection to data sources
- Generating models and commands from existing data sources
- Updating models and commands
- Generating relationships

## How is metadata edited?

[The Hasura VS Code extension](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura) provides syntax
highlighting and auto-complete for your metadata.

The DDN CLI will also validate your metadata when you use it to create a build.

## Next steps

On each page in this section, you'll find detailed information about the various components of Hasura DDN which you can
author and modify in your metadata. Each page will provide you with an overview of the components, how they work, and
examples of how to use them. Below all of this, you'll find detailed reference information about the metadata structure
and the fields that you can use to define each component.



==============================



# build-configs.mdx

URL: https://hasura.io/docs/promptql/reference/metadata-reference/build-configs

# Build configs

## Introduction

Build configs are used to define the resources and other configurations required to build a **supergraph**, **subgraph**
or a **connector**. They are written in YAML and are defined in individual files.

It's helpful to think of build configs as a blueprint for building your supergraph. The following options are available
to you:

| Config type               | Description                                           |
| ------------------------- | ----------------------------------------------------- |
| [Supergraph](#supergraph) | Defines the configuration used to build a supergraph. |
| [Subgraph](#subgraph)     | Defines the configuration used to build a subgraph.   |
| [Connector](#connector)   | Defines the configuration used to build a connector.  |

### How the Supergraph config works {#supergraph}

#### Lifecycle

The [Supergraph](#supergraph-supergraph) object defines the configuration used to build the supergraph. While projects
are generated with default configs for building for local or Hasura DDN
[when initializing a new supergraph](/reference/cli/commands/ddn_supergraph_init.mdx), you can add as many `Supergraph`
objects as you need to define different configurations for building your supergraph.

You can then use these files to build your supergraph — either locally or on Hasura DDN —
[using the CLI](/reference/cli/commands/ddn_supergraph_build_local.mdx). These builds — when on Hasura DDN — can then be
[applied to a project](/reference/cli/commands/ddn_supergraph_build_apply.mdx) which will then serve the API using the
supergraph built from the configuration.

#### Examples

```yaml title="A supergraph.yaml config file:"
kind: Supergraph
version: v2
definition:
  subgraphs:
    - globals/subgraph.yaml
    - app/subgraph.yaml
```

| **Field**              | **Description**                                                                      | **Reference**                                                |
| ---------------------- | ------------------------------------------------------------------------------------ | ------------------------------------------------------------ |
| `kind`                 | Specifies the type of configuration, in this case, `Supergraph`.                     | [Supergraph](#supergraph-supergraph)                         |
| `version`              | The version of the `Supergraph` configuration, which is `v2`.                        | [Supergraph](#supergraph-supergraph)                         |
| `definition.subgraphs` | Paths to the subgraph configuration files that are included in the supergraph build. | [SupergraphDefinitionV2](#supergraph-supergraphdefinitionv2) |

### How the Subgraph config works {#subgraph}

#### Lifecycle

**Each** subgraph in your supergraph has its own config. The [Subgraph](#subgraph-subgraph) object defines the
configuration used to build the subgraph. While subgraphs are generated with a default config
[when initializing a new subgraph](/reference/cli/commands/ddn_subgraph_init.mdx), you can add as many `Subgraph`
objects as you need to define different configurations for building a subgraph.

You can then use these files to build a subgraph [using the CLI](/reference/cli/commands/ddn_subgraph_build_create.mdx).
These subgraph builds can then be [applied to a project](/reference/cli/commands/ddn_subgraph_build_apply.mdx).

#### Examples

```yaml title="A supergraph.yaml config file:"
kind: Subgraph
version: v2
definition:
  name: app
  generator:
    rootPath: .
  includePaths:
    - metadata
  envMapping:
    APP_MY_CONNECTOR_AUTHORIZATION_HEADER:
      fromEnv: APP_MY_CONNECTOR_AUTHORIZATION_HEADER
    APP_MY_CONNECTOR_READ_URL:
      fromEnv: APP_MY_CONNECTOR_READ_URL
    APP_MY_CONNECTOR_WRITE_URL:
      fromEnv: APP_MY_CONNECTOR_WRITE_URL
  connectors:
    - path: connector/my_connector/connector.yaml
      connectorLinkName: my_connector
```

| **Field**                                 | **Description**                                                                                                                                                                                                                                                    | **Reference**                                                |
| ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------ |
| `kind`                                    | Specifies the type of configuration, in this case, `Subgraph`.                                                                                                                                                                                                     | [Subgraph](#subgraph-subgraph)                               |
| `version`                                 | The version of the `Subgraph` configuration, which is `v2`.                                                                                                                                                                                                        | [Subgraph](#subgraph-subgraph)                               |
| `definition.name`                         | The name of the subgraph.                                                                                                                                                                                                                                          | [SubgraphDefinitionV2](#subgraph-subgraphdefinitionv2)       |
| `definition.generator.rootPath`           | Path to the directory containing all the subgraph metadata, in this case the current directory.                                                                                                                                                                    | [SubgraphGeneratorConfig](#subgraph-subgraphgeneratorconfig) |
| `definition.includePaths`                 | Paths to be included to construct the subgraph metadata.                                                                                                                                                                                                           | [SubgraphDefinitionV2](#subgraph-subgraphdefinitionv2)       |
| `definition.envMapping`                   | Environment variable mapping configuration. Typically, these will correspond to connector envs. Additionally, when you [initialize a connector](/reference/cli/commands/ddn_connector_init.mdx), the CLI will automatically add the required envs to the subgraph. | [EnvMapping](#subgraph-envmapping)                           |
| `definition.connectors.path`              | Path to the connector configuration file used in the subgraph.                                                                                                                                                                                                     | [SubgraphConnector](#subgraph-subgraphconnector)             |
| `definition.connectors.connectorLinkName` | Name of the connector link associated with the connector.                                                                                                                                                                                                          | [SubgraphConnector](#subgraph-subgraphconnector)             |

### How the Connector config works {#connector}

#### Lifecycle

**Each** [data connector](/data-sources/overview.mdx) in your supergraph has its own config. The
[Connector](#connector-connector) object defines the configuration used to build the connector. This allows you to
configure the capabilities of the connector and tailor it to your needs. While connectors are initialized with default
configs for building for local or Hasura DDN, you can add as many `Connector` objects as you need to define different
configurations for building your connector.

When you [initialize a connector](/reference/cli/commands/ddn_connector_init.mdx), the CLI will automatically create a
connector config file for you. You can then use this file to build your connector
[using the CLI](/reference/cli/commands/ddn_connector_build_create.mdx).

#### Examples

```yaml title="A connector.yaml config file:"
kind: Connector
version: v2
definition:
  name: MY_CONNECTOR
  subgraph: app
  source: hasura/postgres:v1.1.1
  context: .
  envMapping:
    CONNECTION_URI:
      fromEnv: APP_MY_CONNECTOR_CONNECTION_URI
    HASURA_SERVICE_TOKEN_SECRET:
      fromEnv: APP_MY_CONNECTOR_HASURA_SERVICE_TOKEN_SECRET
    OTEL_EXPORTER_OTLP_TRACES_ENDPOINT:
      fromEnv: APP_MY_CONNECTOR_OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
    OTEL_SERVICE_NAME:
      fromEnv: APP_MY_CONNECTOR_OTEL_SERVICE_NAME
```

| **Field**               | **Description**                                                     | **Reference**                                             |
| ----------------------- | ------------------------------------------------------------------- | --------------------------------------------------------- |
| `kind`                  | Specifies the type of configuration, in this case, `Connector`.     | [Connector](#connector-connector)                         |
| `version`               | The version of the `Connector` configuration, which is `v2`.        | [Connector](#connector-connector)                         |
| `definition.name`       | The name of the connector.                                          | [ConnectorDefinitionV2](#connector-connectordefinitionv2) |
| `definition.subgraph`   | The name of the DDN project subgraph associated with the connector. | [ConnectorDefinitionV2](#connector-connectordefinitionv2) |
| `definition.source`     | The versioned source of the connector.                              | [ConnectorDefinitionV2](#connector-connectordefinitionv2) |
| `definition.context`    | The path to the context directory used in the connector build.      | [ConnectorDefinitionV2](#connector-connectordefinitionv2) |
| `definition.envMapping` | Environment variable mapping configuration for the connector.       | [EnvMapping](#connector-envmapping)                       |

---

## Metadata structure


### Supergraph {#supergraph-supergraph}

Defines the configuration used to build the Supergraph.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `Supergraph` | true |  |
| `version` | `v2` | true |  |
| `definition` | [SupergraphDefinitionV2](#supergraph-supergraphdefinitionv2) | true |  |



#### SupergraphDefinitionV2 {#supergraph-supergraphdefinitionv2}

Supergraph Definition V2.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `subgraphs` | [string] | true | Paths to subgraph configuration. |

### Subgraph {#subgraph-subgraph}

Defines the configuration used to build the Subgraph.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `Subgraph` | true |  |
| `version` | `v2` | true |  |
| `definition` | [SubgraphDefinitionV2](#subgraph-subgraphdefinitionv2) | true |  |



#### SubgraphDefinitionV2 {#subgraph-subgraphdefinitionv2}

Subgraph Definition V2.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | string | true | Subgraph Name. |
| `generator` | [SubgraphGeneratorConfig](#subgraph-subgraphgeneratorconfig) | true | Subgraph generator Configuration. |
| `envFile` | string | false | Path to the Subgraph .env file. |
| `includePaths` | [string] | true | Paths to be included to construct Subgraph metadata. |
| `envMapping` | [EnvMapping](#subgraph-envmapping) | false | Environment Variable mapping config. |
| `connectors` | [[SubgraphConnector](#subgraph-subgraphconnector)] | false | Connectors used in subgraph. |



#### SubgraphConnector {#subgraph-subgraphconnector}

Subgraph Connector config.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `path` | string | true | Path to connector config file. |
| `connectorLinkName` | string | false | Name of connector link associated with the connector. |



#### EnvMapping {#subgraph-envmapping}

Environment Variables mapping config.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` | [EnvSource](#subgraph-envsource) | false | Target Environment variable. |



#### EnvSource {#subgraph-envsource}

Environment Variable Source.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fromEnv` | string | true | Source Environment variable. |



#### SubgraphGeneratorConfig {#subgraph-subgraphgeneratorconfig}

Subgraph generator Configuration.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `rootPath` | string | true | Path to the directory which holds all the Subgraph metadata. |
| `graphqlRootFieldPrefix` | string | false | Prefix to use while generating GraphQL root fields. |
| `graphqlTypeNamePrefix` | string | false | Prefix to use while generating GraphQL type names. |
| `namingConvention` | `none` / `graphql` / `snake_case` | false | Naming convention to use while generating GraphQL fields and types. |

### Connector {#connector-connector}

Defines the configuration used to build the connector.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `Connector` | true |  |
| `version` | `v2` | true |  |
| `definition` | [ConnectorDefinitionV2](#connector-connectordefinitionv2) | true |  |



#### ConnectorDefinitionV2 {#connector-connectordefinitionv2}

Connector deployment definition V2.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | string | true | Connector name. |
| `subgraph` | string | false | DDN project subgraph name. |
| `source` | string | true | Connector Hub ID. |
| `context` | string | true | Path to the context directory. |
| `envFile` | string | false | Path to the shared .env file. |
| `envMapping` | [EnvMapping](#connector-envmapping) | false | Environment Variable mapping config. |
| `regionConfiguration` | [[RegionConfigurationV2](#connector-regionconfigurationv2)] | false | Connector deployment Region configuration |
| `resources` | [Resources](#connector-resources) | false | Connector deployment Resources. |



#### RegionConfigurationV2 {#connector-regionconfigurationv2}

Connector deployment Region Configuration V2.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `region` | string | true | Region to deploy the connector to. |
| `mode` | `ReadOnly` / `ReadWrite` | true | Connector deployment mode. |
| `envMapping` | [EnvMapping](#connector-envmapping) | false | Environment Variable mapping config. |
| `resources` | [Resources](#connector-resources) | false | Connector deployment Resources. |



#### Resources {#connector-resources}

Connector deployment Resources.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `memory` | string | false | Connector deployment memory resource in bytes. Accepted units: k, M, G. Example: 128M, 1G |
| `cpu` | string | false | Connector deployment cpu resource in cores. Example: 1, 1.5 |



#### EnvMapping {#connector-envmapping}

Environment Variables mapping config.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` | [EnvSource](#connector-envsource) | false | Target Environment variable. |



#### EnvSource {#connector-envsource}

Environment Variable Source.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fromEnv` | string | true | Source Environment variable. |



==============================



# data-connector-links.mdx

URL: https://hasura.io/docs/promptql/reference/metadata-reference/data-connector-links

# Data Connector Links

## Introduction

A `DataConnectorLink` is used to specify the URLs and NDC schema of a [data connector](/data-sources/overview.mdx)
allowing to link it to [models](/reference/metadata-reference/models.mdx) and
[commands](/reference/metadata-reference/commands.mdx). It can be used to connect to various types of data connectors on
different data sources, like SQL databases, NoSQL databases, REST APIs, GraphQL APIs, files, and more.

## How DataConnectorLinks work

### Lifecycle

A `DataConnectorLink` can be [created using the CLI](/reference/cli/commands/ddn_connector-link_add.mdx). Out of
convenience, the CLI will scaffold this file automatically for you when
[initializing a new connector](/reference/cli/commands/ddn_connector_init.mdx).

A `DataConnectorLink` belongs to a single [subgraph's](/project-configuration/subgraphs/index.mdx) data source. It is
used to link the data source to the subgraph's models, commands, and relationships. The contents can be
[updated using the CLI](/reference/cli/commands/ddn_connector-link_update.mdx). This will introspect the data source and
update the schema of the data connector.

Any time your data source schema changes, you should update the `DataConnectorLink` to reflect those changes. This will
ensure that the schema of the data connector is up to date and that the data connector can be used to serve requests.

This configuration is then used to [generate the metadata](/reference/cli/commands/ddn_connector-link_add-resources.mdx)
representing collections present in the data source.

:::info Be more granular

The example we linked above is for adding all models, commands, and relationships present in a data source. However, you
can add each resource individually after updating the `DataConnectorLink` configuration:

- [Add models](/reference/cli/commands/ddn_model_add.mdx)
- [Add commands](/reference/cli/commands/ddn_command_add.mdx)
- [Add relationships](/reference/cli/commands/ddn_relationship_add.mdx)

:::

To make a new data connector link and it's cascading metadata available in your supergraph, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI after adding your resources.

### Examples

```yaml title="A sample DataConnectorLink:"
kind: DataConnectorLink
version: v1
definition:
  name: data_connector
  url:
    singleUrl:
      value: http://data_connector:8100
  headers: {}
  schema:
    version: v0.1
    schema:
      scalar_types: {}
      object_types: {}
      collections: []
      functions: []
      procedures: []
    capabilities:
      version: 0.1.3
      capabilities:
        query:
          nested_fields: {}
          variables: {}
        mutation: {}
```

| **Field**                                              | **Description**                                                                                       | **Reference**                                                           |
| ------------------------------------------------------ | ----------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------- |
| `kind`                                                 | Specifies the type of configuration, in this case, DataConnectorLink.                                 | [DataConnectorLink](#dataconnectorlink-dataconnectorlink)               |
| `version`                                              | The version of the DataConnectorLink configuration, which is `v1`.                                    | [DataConnectorLinkV1](#dataconnectorlink-dataconnectorlinkv1)           |
| `definition.name`                                      | The name given to this data connector configuration.                                                  | [DataConnectorName](#dataconnectorlink-dataconnectorname)               |
| `definition.url.singleUrl.value`                       | The URL used to access the data connector.                                                            | [DataConnectorUrlV1](#dataconnectorlink-dataconnectorurlv1)             |
| `definition.headers`                                   | A key-value map of HTTP headers to be sent with each request to the data connector.                   | [HttpHeaders](#dataconnectorlink-httpheaders)                           |
| `definition.schema.version`                            | The version of the schema that the data connector is using.                                           | [SchemaAndCapabilitiesV01](#dataconnectorlink-schemaandcapabilitiesv01) |
| `definition.schema.schema[]`                           | The schema of the data connector, representing various types, collections, functions, and procedures. | [SchemaAndCapabilitiesV01](#dataconnectorlink-schemaandcapabilitiesv01) |
| `definition.schema.capabilities.version`               | The version of the capabilities that the data connector supports.                                     | [SchemaAndCapabilitiesV01](#dataconnectorlink-schemaandcapabilitiesv01) |
| `definition.schema.capabilities.capabilities.query`    | The query capabilities of the data connector.                                                         | [SchemaAndCapabilitiesV01](#dataconnectorlink-schemaandcapabilitiesv01) |
| `definition.schema.capabilities.capabilities.mutation` | The mutation capabilities of the data connector.                                                      | [SchemaAndCapabilitiesV01](#dataconnectorlink-schemaandcapabilitiesv01) |

---

## Metadata structure


### DataConnectorLink {#dataconnectorlink-dataconnectorlink}

Definition of a data connector, used to bring in sources of data and connect them to OpenDD models and commands.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `DataConnectorLink` | true |  |
| `version` | `v1` | true |  |
| `definition` | [DataConnectorLinkV1](#dataconnectorlink-dataconnectorlinkv1) | true | Definition of a data connector - version 1. |

 **Example:**

```yaml
kind: DataConnectorLink
version: v1
definition:
  name: data_connector
  url:
    singleUrl:
      value: http://data_connector:8100
  headers: {}
  schema:
    version: v0.1
    schema:
      scalar_types: {}
      object_types: {}
      collections: []
      functions: []
      procedures: []
    capabilities:
      version: 0.1.3
      capabilities:
        query:
          nested_fields: {}
          variables: {}
        mutation: {}
```


#### DataConnectorLinkV1 {#dataconnectorlink-dataconnectorlinkv1}

Definition of a data connector - version 1.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [DataConnectorName](#dataconnectorlink-dataconnectorname) | true | The name of the data connector. |
| `url` | [DataConnectorUrlV1](#dataconnectorlink-dataconnectorurlv1) | true | The url(s) to access the data connector. |
| `headers` | [HttpHeaders](#dataconnectorlink-httpheaders) | false | Key value map of HTTP headers to be sent with each request to the data connector. This is meant for protocol level use between engine and the data connector. |
| `schema` | [VersionedSchemaAndCapabilities](#dataconnectorlink-versionedschemaandcapabilities) | true | The schema of the data connector. This schema is used as the source of truth when serving requests and the live schema of the data connector is not looked up. |
| `argumentPresets` | [[DataConnectorArgumentPreset](#dataconnectorlink-dataconnectorargumentpreset)] | false | Argument presets that applies to all functions and procedures of this data connector. Defaults to no argument presets. |
| `responseHeaders` | [ResponseHeaders](#dataconnectorlink-responseheaders) / null | false | HTTP response headers configuration that is forwarded from a data connector to the client. |



#### ResponseHeaders {#dataconnectorlink-responseheaders}

Configuration of what HTTP response headers should be forwarded from a data connector to the client in HTTP response.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `headersField` | [DataConnectorColumnName](#dataconnectorlink-dataconnectorcolumnname) | true | Name of the field in the NDC function/procedure's result which contains the response headers |
| `resultField` | [DataConnectorColumnName](#dataconnectorlink-dataconnectorcolumnname) | true | Name of the field in the NDC function/procedure's result which contains the result |
| `forwardHeaders` | [string] | true | List of actual HTTP response headers from the data connector to be set as response headers |



#### DataConnectorColumnName {#dataconnectorlink-dataconnectorcolumnname}

The name of a column in a data connector.


**Value:** string


#### DataConnectorArgumentPreset {#dataconnectorlink-dataconnectorargumentpreset}

An argument preset that can be applied to all functions/procedures of a connector

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `argument` | [DataConnectorArgumentName](#dataconnectorlink-dataconnectorargumentname) | true | The name of an argument as defined by a data connector. |
| `value` | [DataConnectorArgumentPresetValue](#dataconnectorlink-dataconnectorargumentpresetvalue) | true | The value of a data connector argument preset. |



#### DataConnectorArgumentPresetValue {#dataconnectorlink-dataconnectorargumentpresetvalue}

The value of a data connector argument preset.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `httpHeaders` | [HttpHeadersPreset](#dataconnectorlink-httpheaderspreset) | true | HTTP headers that can be preset from request |



#### HttpHeadersPreset {#dataconnectorlink-httpheaderspreset}

Configuration of what HTTP request headers should be forwarded to a data connector.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `forward` | [string] | true | List of HTTP headers that should be forwarded from HTTP requests |
| `additional` | [AdditionalHttpHeaders](#dataconnectorlink-additionalhttpheaders) | true | Additional headers that should be forwarded, from other contexts |



#### AdditionalHttpHeaders {#dataconnectorlink-additionalhttpheaders}

Key value map of HTTP headers to be forwarded in the headers argument of a data connector request.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` | [ValueExpression](#dataconnectorlink-valueexpression) | false |  |



#### ValueExpression {#dataconnectorlink-valueexpression}

An expression which evaluates to a value that can be used in permissions and various presets.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `literal` |  | false |  |
| `sessionVariable` | [OpenDdSessionVariable](#dataconnectorlink-openddsessionvariable) | false |  |
| `valueFromEnv` | string | false |  |



#### OpenDdSessionVariable {#dataconnectorlink-openddsessionvariable}

Used to represent the name of a session variable, like "x-hasura-role".


**Value:** string


#### DataConnectorArgumentName {#dataconnectorlink-dataconnectorargumentname}

The name of an argument as defined by a data connector.


**Value:** string


#### VersionedSchemaAndCapabilities {#dataconnectorlink-versionedschemaandcapabilities}

Versioned schema and capabilities for a data connector.


**One of the following values:**

| Value | Description |
|-----|-----|
| [SchemaAndCapabilitiesV01](#dataconnectorlink-schemaandcapabilitiesv01) | Version 0.1 of schema and capabilities for a data connector. |
| [SchemaAndCapabilitiesV02](#dataconnectorlink-schemaandcapabilitiesv02) | Version 0.2 of schema and capabilities for a data connector. |



#### SchemaAndCapabilitiesV02 {#dataconnectorlink-schemaandcapabilitiesv02}

Version 0.2 of schema and capabilities for a data connector.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `version` | `v0.2` | true |  |
| `schema` | [Schema Response](https://hasura.github.io/ndc-spec/specification/schema/index.html) | true |  |
| `capabilities` | [Capabilities Response](https://hasura.github.io/ndc-spec/specification/capabilities.html) | true |  |



#### SchemaAndCapabilitiesV01 {#dataconnectorlink-schemaandcapabilitiesv01}

Version 0.1 of schema and capabilities for a data connector.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `version` | `v0.1` | true |  |
| `schema` | [Schema Response](https://hasura.github.io/ndc-spec/specification/schema/index.html) | true |  |
| `capabilities` | [Capabilities Response](https://hasura.github.io/ndc-spec/specification/capabilities.html) | true |  |



#### HttpHeaders {#dataconnectorlink-httpheaders}

Key value map of HTTP headers to be sent with an HTTP request. The key is the header name and the value is a potential reference to an environment variable.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` | [EnvironmentValue](#dataconnectorlink-environmentvalue) | false |  |



#### DataConnectorUrlV1 {#dataconnectorlink-dataconnectorurlv1}

A URL to access a data connector. This can be a single URL or a pair of read and write URLs.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `singleUrl` | [EnvironmentValue](#dataconnectorlink-environmentvalue) | false |  |
| `readWriteUrls` | [ReadWriteUrls](#dataconnectorlink-readwriteurls) | false | A pair of URLs to access a data connector, one for reading and one for writing. |



#### ReadWriteUrls {#dataconnectorlink-readwriteurls}

A pair of URLs to access a data connector, one for reading and one for writing.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `read` | [EnvironmentValue](#dataconnectorlink-environmentvalue) | true |  |
| `write` | [EnvironmentValue](#dataconnectorlink-environmentvalue) | true |  |



#### EnvironmentValue {#dataconnectorlink-environmentvalue}

Either a literal string or a reference to a Hasura secret


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `value` | string | false |  |
| `valueFromEnv` | string | false |  |



#### DataConnectorName {#dataconnectorlink-dataconnectorname}

The name of a data connector.


**Value:** string
### DataConnectorScalarRepresentation {#dataconnectorscalarrepresentation-dataconnectorscalarrepresentation}

The representation of a data connector scalar in terms of Open DD types

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `DataConnectorScalarRepresentation` | true |  |
| `version` | `v1` | true |  |
| `definition` | [DataConnectorScalarRepresentationV1](#dataconnectorscalarrepresentation-dataconnectorscalarrepresentationv1) | true | The representation of a data connector scalar in terms of Open DD types. Deprecated in favour of `BooleanExpressionType`. |



#### DataConnectorScalarRepresentationV1 {#dataconnectorscalarrepresentation-dataconnectorscalarrepresentationv1}

The representation of a data connector scalar in terms of Open DD types. Deprecated in favour of `BooleanExpressionType`.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `dataConnectorName` | [DataConnectorName](#dataconnectorscalarrepresentation-dataconnectorname) | true | The name of the data connector that this scalar type comes from. |
| `dataConnectorScalarType` | [DataConnectorScalarType](#dataconnectorscalarrepresentation-dataconnectorscalartype) | true | The name of the scalar type coming from the data connector. |
| `representation` | [TypeName](#dataconnectorscalarrepresentation-typename) | true | The name of the Open DD type that this data connector scalar type should be represented as. |
| `graphql` | [DataConnectorScalarGraphQLConfiguration](#dataconnectorscalarrepresentation-dataconnectorscalargraphqlconfiguration) / null | false | Configuration for how this scalar's operators should appear in the GraphQL schema. |

 **Example:**

```yaml
dataConnectorName: data_connector
dataConnectorScalarType: varchar
representation: String
graphql:
  comparisonExpressionTypeName: String_Comparison_Exp
```


#### DataConnectorScalarGraphQLConfiguration {#dataconnectorscalarrepresentation-dataconnectorscalargraphqlconfiguration}

GraphQL configuration of a data connector scalar

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `comparisonExpressionTypeName` | [GraphQlTypeName](#dataconnectorscalarrepresentation-graphqltypename) / null | false |  |



#### GraphQlTypeName {#dataconnectorscalarrepresentation-graphqltypename}

The name of a GraphQL type.


**Value:** string


#### TypeName {#dataconnectorscalarrepresentation-typename}

The name of the Open DD type that this data connector scalar type should be represented as.


**One of the following values:**

| Value | Description |
|-----|-----|
| [InbuiltType](#dataconnectorscalarrepresentation-inbuilttype) | An inbuilt primitive OpenDD type. |
| [CustomTypeName](#dataconnectorscalarrepresentation-customtypename) |  |



#### CustomTypeName {#dataconnectorscalarrepresentation-customtypename}

The name of a user-defined type.


**Value:** string


#### InbuiltType {#dataconnectorscalarrepresentation-inbuilttype}

An inbuilt primitive OpenDD type.


**Value:** `ID` / `Int` / `Float` / `Boolean` / `String`


#### DataConnectorScalarType {#dataconnectorscalarrepresentation-dataconnectorscalartype}

The name of a scalar type in a data connector.


**Value:** string


#### DataConnectorName {#dataconnectorscalarrepresentation-dataconnectorname}

The name of a data connector.


**Value:** string



==============================



# types.mdx

URL: https://hasura.io/docs/promptql/reference/metadata-reference/types

# Types

## Introduction

Types serve as the fundamental elements that define the structure of your data.

Being able to define types in your data domain is beneficial because it provides you with the flexibility to define them
separately from the types referenced by a data connector's source.

The specification employs a concrete type system that includes both primitive and user-defined types. All subsequent
layers, such as [models](/reference/metadata-reference/models.mdx),
[commands](/reference/metadata-reference/commands.mdx), and
[relationships](/reference/metadata-reference/relationships.mdx) are defined in terms of these types.

The types can be one of the following:

| Type            | Description                                                                                              |
| --------------- | -------------------------------------------------------------------------------------------------------- |
| Primitive       | These are the basic types `ID`, `Int`, `Float`, `Boolean`, or `String`.                                  |
| Custom          | These are user-defined types, such as ScalarType or ObjectType.                                          |
| Type References | When specifying the types of a field or an argument, you can mark them as required `!` or repeated `[]`. |

The spec also allows you to map existing data connector scalars to types in your data domain.

:::info Primitive types and type references

Primitive types supported are `ID`, `Int`, `Float`, `Boolean` and `String`.

Type references follow [GraphQL type syntax](https://spec.graphql.org/June2018/#sec-Combining-List-and-Non-Null). Fields
and arguments are nullable by default. To represent non-nullability, specify a `!` after the type name. Similarly, array
fields and arguments are wrapped in `[]`.

:::

## How types work

### Lifecycle

Typically, types will be generated from the data connector schema when you
[introspect a data connector](/reference/cli/commands/ddn_connector_introspect.mdx). You can also define custom types to
represent data that doesn't exist in the data connector.

Further, you can define custom types by either aliasing existing types (such as primitives or custom), or you can define
a type with fields. In turn, the fields themselves can be a primitive or another custom type.

Type references are types of fields and arguments that refer to other primitive or custom types and which can be marked
as nullable, required or repeated (in the case of arrays).

To make a new types available in your supergraph, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

### Examples

#### ScalarType

```yaml title="A sample scalar type definition:"
kind: ScalarType
version: v1
definition:
  name: Uuid
  graphql:
    typeName: Uuid
```

| **Field**                     | **Description**                                                                 | **Reference**                                                                |
| ----------------------------- | ------------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| `kind`                        | Indicates that this configuration is for a custom scalar type.                  | [ScalarType](#scalartype-scalartype)                                         |
| `version`                     | The version of the ScalarType configuration.                                    | [ScalarTypeV1](#scalartype-scalartypev1)                                     |
| `definition.name`             | The unique name for your custom scalar type, used throughout your project.      | [CustomTypeName](#scalartype-customtypename)                                 |
| `definition.graphql.typeName` | The name to use for this scalar type in your GraphQL schema.                    | [ScalarTypeGraphQLConfiguration](#scalartype-scalartypegraphqlconfiguration) |
| `definition.description`      | An optional description of this scalar type that appears in the GraphQL schema. | [ScalarTypeV1](#scalartype-scalartypev1)                                     |

#### ObjectType

```yaml title="A sample object type definition:"
kind: ObjectType
version: v1
definition:
  name: CartItems
  fields:
    - name: cartId
      type: Uuid!
    - name: createdAt
      type: Timestamptz
    - name: id
      type: Uuid!
    - name: productId
      type: Uuid!
    - name: quantity
      type: Int4!
    - name: updatedAt
      type: Timestamptz
  graphql:
    typeName: CartItems
    inputTypeName: CartItemsInput
  dataConnectorTypeMapping:
    - dataConnectorName: my_pg
      dataConnectorObjectType: cart_items
      fieldMapping:
        cartId:
          column:
            name: cart_id
        createdAt:
          column:
            name: created_at
        id:
          column:
            name: id
        productId:
          column:
            name: product_id
        quantity:
          column:
            name: quantity
        updatedAt:
          column:
            name: updated_at
```

| **Field**                                              | **Description**                                                                                                   | **Reference**                                                                |
| ------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| `kind`                                                 | Indicates that this configuration is for a custom object type.                                                    | [ObjectType](#objecttype-objecttype)                                         |
| `version`                                              | The version of the ObjectType configuration.                                                                      | [ObjectTypeV1](#objecttype-objecttypev1)                                     |
| `definition.name`                                      | The unique name for your custom object type, used throughout your project.                                        | [CustomTypeName](#objecttype-customtypename)                                 |
| `definition.fields[].name`                             | The name of a field within your object type.                                                                      | [FieldName](#objecttype-fieldname)                                           |
| `definition.fields[].type`                             | The data type for the field, which can be a primitive, custom type, or a type reference (e.g., `Uuid!`, `Int4!`). | [TypeReference](#objecttype-typereference)                                   |
| `definition.fields[].description`                      | An optional description of this field that appears in the GraphQL schema.                                         | [ObjectFieldDefinition](#objecttype-objectfielddefinition)                   |
| `definition.graphql.typeName`                          | The name to use for this object type in your GraphQL schema.                                                      | [ObjectTypeGraphQLConfiguration](#objecttype-objecttypegraphqlconfiguration) |
| `definition.graphql.inputTypeName`                     | The name to use for this object type in input operations within your GraphQL schema.                              | [ObjectTypeGraphQLConfiguration](#objecttype-objecttypegraphqlconfiguration) |
| `definition.dataConnectorTypeMapping[]`                | The mapping of data connector object types to your object type.                                                   | [DataConnectorTypeMapping](#objecttype-dataconnectortypemapping)             |
| `definition.dataConnectorTypeMapping[].fieldMapping[]` | The mapping of fields in your object type to columns in a data connector.                                         | [ObjectTypeFieldMappings](#objecttype-fieldmappings)                         |

---

## Metadata structure


### ScalarType {#scalartype-scalartype}

Definition of a user-defined scalar type that that has opaque semantics.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `ScalarType` | true |  |
| `version` | `v1` | true |  |
| `definition` | [ScalarTypeV1](#scalartype-scalartypev1) | true | Definition of a user-defined scalar type that that has opaque semantics. |

 **Example:**

```yaml
kind: ScalarType
version: v1
name: CustomString
graphql:
  typeName: CustomString
description: A custom string type
```


#### ScalarTypeV1 {#scalartype-scalartypev1}

Definition of a user-defined scalar type that that has opaque semantics.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [CustomTypeName](#scalartype-customtypename) | true | The name to give this scalar type, used to refer to it elsewhere in the metadata. Must be unique across all types defined in this subgraph. |
| `graphql` | [ScalarTypeGraphQLConfiguration](#scalartype-scalartypegraphqlconfiguration) / null | false | Configuration for how this scalar type should appear in the GraphQL schema. |
| `description` | string / null | false | The description of this scalar. Gets added to the description of the scalar's definition in the graphql schema. |



#### ScalarTypeGraphQLConfiguration {#scalartype-scalartypegraphqlconfiguration}

GraphQL configuration of an Open DD scalar type

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `typeName` | [GraphQlTypeName](#scalartype-graphqltypename) | true | The name of the GraphQl type to use for this scalar. |



#### GraphQlTypeName {#scalartype-graphqltypename}

The name of a GraphQL type.


**Value:** string


#### CustomTypeName {#scalartype-customtypename}

The name of a user-defined type.


**Value:** string
### ObjectType {#objecttype-objecttype}

Definition of a user-defined Open DD object type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `ObjectType` | true |  |
| `version` | `v1` | true |  |
| `definition` | [ObjectTypeV1](#objecttype-objecttypev1) | true | Definition of a user-defined Open DD object type. |

 **Example:**

```yaml
kind: ObjectType
version: v1
definition:
  name: Author
  fields:
    - name: author_id
      type: Int!
      description: The id of the author
    - name: first_name
      type: String
      description: The first name of the author
    - name: last_name
      type: String
      description: The last name of the author
    - name: biography
      type: String
      description: AI generated biography for the author
      arguments:
        - name: ai_model
          argumentType: String!
          description: The AI model to use for generating the biography
  description: An author of a book
  globalIdFields:
    - author_id
  graphql:
    typeName: Author
  dataConnectorTypeMapping:
    - dataConnectorName: my_db
      dataConnectorObjectType: author
      fieldMapping:
        author_id:
          column:
            name: id
    - dataConnectorName: my_vector_db
      dataConnectorObjectType: author
      fieldMapping:
        biography:
          column:
            name: biography
            argumentMapping:
              ai_model: model
```


#### ObjectTypeV1 {#objecttype-objecttypev1}

Definition of a user-defined Open DD object type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [CustomTypeName](#objecttype-customtypename) | true | The name to give this object type, used to refer to it elsewhere in the metadata. Must be unique across all types defined in this subgraph. |
| `fields` | [[ObjectFieldDefinition](#objecttype-objectfielddefinition)] | true | The list of fields defined for this object type. |
| `globalIdFields` | array / null | false | The subset of fields that uniquely identify this object in the domain. Setting this property will automatically implement the GraphQL Relay Node interface for this object type and add an `id` global ID field. If setting this property, there must not be a field named `id` already present. |
| `graphql` | [ObjectTypeGraphQLConfiguration](#objecttype-objecttypegraphqlconfiguration) / null | false | Configuration for how this object type should appear in the GraphQL schema. |
| `description` | string / null | false | The description of the object. Gets added to the description of the object's definition in the graphql schema. |
| `dataConnectorTypeMapping` | [[DataConnectorTypeMapping](#objecttype-dataconnectortypemapping)] | false | Mapping of this object type to corresponding object types in various data connectors. |



#### DataConnectorTypeMapping {#objecttype-dataconnectortypemapping}

This defines the mapping of the fields of an object type to the corresponding columns of an object type in a data connector.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `dataConnectorName` | [DataConnectorName](#objecttype-dataconnectorname) | true |  |
| `dataConnectorObjectType` | [DataConnectorObjectType](#objecttype-dataconnectorobjecttype) | true |  |
| `fieldMapping` | [FieldMappings](#objecttype-fieldmappings) | false |  |



#### FieldMappings {#objecttype-fieldmappings}

Mapping of object fields to their source columns in the data connector.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` | [FieldMapping](#objecttype-fieldmapping) | false |  |



#### FieldMapping {#objecttype-fieldmapping}

Source field directly maps to some column in the data connector.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `column` | [ColumnFieldMapping](#objecttype-columnfieldmapping) | true | The target column in a data connector object that a source field maps to. |



#### ColumnFieldMapping {#objecttype-columnfieldmapping}

The target column in a data connector object that a source field maps to.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [DataConnectorColumnName](#objecttype-dataconnectorcolumnname) | true | The name of the target column |
| `argumentMapping` | [ArgumentMapping](#objecttype-argumentmapping) / null | false | Arguments to the column field |



#### ArgumentMapping {#objecttype-argumentmapping}

Mapping of a comand or model argument name to the corresponding argument name used in the data connector. The key of this object is the argument name used in the command or model and the value is the argument name used in the data connector.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` | [DataConnectorArgumentName](#objecttype-dataconnectorargumentname) | false |  |



#### DataConnectorArgumentName {#objecttype-dataconnectorargumentname}

The name of an argument as defined by a data connector.


**Value:** string


#### DataConnectorColumnName {#objecttype-dataconnectorcolumnname}

The name of a column in a data connector.


**Value:** string


#### DataConnectorObjectType {#objecttype-dataconnectorobjecttype}

The name of an object type in a data connector.


**Value:** string


#### DataConnectorName {#objecttype-dataconnectorname}

The name of a data connector.


**Value:** string


#### ObjectTypeGraphQLConfiguration {#objecttype-objecttypegraphqlconfiguration}

GraphQL configuration of an Open DD object type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `typeName` | [GraphQlTypeName](#objecttype-graphqltypename) / null | false | The name to use for the GraphQL type representation of this object type when used in an output context. |
| `inputTypeName` | [GraphQlTypeName](#objecttype-graphqltypename) / null | false | The name to use for the GraphQL type representation of this object type when used in an input context. |
| `apolloFederation` | [ObjectApolloFederationConfig](#objecttype-objectapollofederationconfig) / null | false | Configuration for exposing apollo federation related types and directives. |



#### ObjectApolloFederationConfig {#objecttype-objectapollofederationconfig}

Configuration for apollo federation related types and directives.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `keys` | [[ApolloFederationObjectKey](#objecttype-apollofederationobjectkey)] | true |  |



#### ApolloFederationObjectKey {#objecttype-apollofederationobjectkey}

The definition of a key for an apollo federation object.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fields` | [[FieldName](#objecttype-fieldname)] | true |  |



#### GraphQlTypeName {#objecttype-graphqltypename}

The name of a GraphQL type.


**Value:** string


#### ObjectFieldDefinition {#objecttype-objectfielddefinition}

The definition of a field in a user-defined object type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [FieldName](#objecttype-fieldname) | true | The name of the field. This name is used both when referring to the field elsewhere in the metadata and when creating the corresponding GraphQl type. |
| `type` | [TypeReference](#objecttype-typereference) | true | The type of this field. This uses the GraphQL syntax to represent field types and must refer to one of the inbuilt OpenDd types or another user-defined type. |
| `description` | string / null | false | The description of this field. Gets added to the description of the field's definition in the graphql schema. |
| `deprecated` | [Deprecated](#objecttype-deprecated) / null | false | Whether this field is deprecated. If set, the deprecation status is added to the field's graphql schema. |
| `arguments` | [[FieldArgumentDefinition](#objecttype-fieldargumentdefinition)] | false | The arguments for the field |



#### FieldArgumentDefinition {#objecttype-fieldargumentdefinition}

The definition of an argument for a field in a user-defined object type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [ArgumentName](#objecttype-argumentname) | true |  |
| `argumentType` | [TypeReference](#objecttype-typereference) | true |  |
| `description` | string / null | false |  |



#### ArgumentName {#objecttype-argumentname}

The name of an argument.


**Value:** string


#### Deprecated {#objecttype-deprecated}

OpenDd configuration to indicate whether an object type field, relationship, model root field or command root field is deprecated.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `reason` | string / null | false | The reason for deprecation. |



#### TypeReference {#objecttype-typereference}

A reference to an Open DD type including nullable values and arrays.
Suffix '!' to indicate a non-nullable reference, and wrap in '[]' to indicate an array.
Eg: '[String!]!' is a non-nullable array of non-nullable strings.


**Value:** string


#### FieldName {#objecttype-fieldname}

The name of a field in a user-defined object type.


**Value:** string


#### CustomTypeName {#objecttype-customtypename}

The name of a user-defined type.


**Value:** string



==============================



# models.mdx

URL: https://hasura.io/docs/promptql/reference/metadata-reference/models

# Models

## Introduction

In Hasura DDN, a **model** represents a collection of data objects within a data domain. Models act as the foundational
elements (or "nouns") that define the data structure and behavior in your API. They can be backed by various data
sources such as database tables, custom SQL queries, materialized views, or even external REST or GraphQL APIs.

Models bridge the gap between [data connectors](/data-sources/overview.mdx) and the PromptQL.

## How models work

### Lifecycle

You can quickly add models to your metadata [using the CLI](/reference/cli/commands/ddn_model_add.mdx).

Models themselves are backed by [types](/reference/metadata-reference/types.mdx), which are derived using the schema of
the data source they represent via a [DataConnectorLink object](/reference/metadata-reference/data-connector-links.mdx).
This means that when you add a model, Hasura DDN automatically generates the necessary types for you.

Once a model is declared, it becomes a central reference point within your metadata. Models are often associated with
[`Relationship`](/reference/metadata-reference/relationships.mdx) objects, allowing them to interact with other models,
and with [`Permissions`](/reference/metadata-reference/permissions.mdx) objects, which control access to the data they
represent. Like all other metadata objects, models are defined in an HML file.

You should [update your models](/reference/cli/commands/ddn_model_update.mdx) whenever you make changes to your data
sources and, in turn, your DataConnectorLink objects. This ensures that your API remains in sync with your data.

To make a new model available in your supergraph, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

### Examples

```yaml title="The following is an example of a model definition for a Users model:"
---
kind: Model
version: v1
definition:
  name: Users
  objectType: Users
  source:
    dataConnectorName: my_pg
    collection: users
  filterExpressionType: UsersBoolExp
  orderableFields:
    - fieldName: id
      orderByDirections:
        enableAll: true
    - fieldName: name
      orderByDirections:
        enableAll: true
    - fieldName: email
      orderByDirections:
        enableAll: true
    - fieldName: createdAt
      orderByDirections:
        enableAll: true
  graphql:
    selectMany:
      queryRootField: users
    selectUniques:
      - queryRootField: usersById
        uniqueIdentifier:
          - id
    orderByExpressionType: UsersOrderBy
```

| **Field**                                        | **Description**                                                                                                                                                                                | **Reference**                                                         |
| ------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------- |
| **kind**                                         | Specifies the type of object being defined. In this case, it’s a model.                                                                                                                        | [Model](#model-model)                                                 |
| **version**                                      | Indicates the version of the model's structure.                                                                                                                                                | [ModelV1](#model-modelv1)                                             |
| **definition.name**                              | The name of the model, representing the collection of data objects within this model.                                                                                                          | [ModelName](#model-modelname)                                         |
| **definition.objectType**                        | Defines the type of objects contained within this model.                                                                                                                                       | [CustomTypeName](#model-customtypename)                               |
| **definition.source.dataConnectorName**          | The name of the data connector that backs this model, linking it to the actual data source.                                                                                                    | [DataConnectorName](#model-dataconnectorname)                         |
| **definition.source.collection**                 | The specific collection within the data connector that this model maps to.                                                                                                                     | [CollectionName](#model-collectionname)                               |
| **definition.filterExpressionType**              | Specifies the type used for filtering the model's data within GraphQL queries.                                                                                                                 | [CustomTypeName](#model-customtypename)                               |
| **definition.orderableFields**                   | A list of fields (in this example: `id`, `name`, `email`, `createdAt`) that can be used to sort the data in this model.                                                                        | [OrderableField](#model-orderablefield)                               |
| **definition.graphql.selectMany.queryRootField** | The root field in the GraphQL API for querying multiple objects from this model. Removing this will disable the ability to query and return an array of this model.                            | [SelectManyGraphQlDefinition](#model-selectmanygraphqldefinition)     |
| **definition.graphql.selectUniques**             | Defines unique query root fields (e.g., usersById) and identifiers used to retrieve unique objects in GraphQL. Removing this will disable the abilty to query a single instance of this model. | [SelectUniqueGraphQlDefinition](#model-selectuniquegraphqldefinition) |
| **definition.graphql.orderByExpressionType**     | The type name used for specifying how to order the data when querying this model in GraphQL.                                                                                                   | [GraphQlTypeName](#model-graphqltypename)                             |

```graphql title="The earlier model definition enables the following query in the API:"
query UsersQuery {
  users(where: { name: { _eq: "Bob" } }, order_by: { createdAt: Asc }, limit: 10) {
    id
    name
    email
    createdAt
  }
}
```

The above example works because the earlier model definition includes the necessary configuration for filtering,
sorting, and pagination. Alternatively, if we'd set `enableAll` to `false` for the `createdAt` field in the
[`orderableFields` section](#model-orderablefield), the `createdAt` field would not be available for sorting in the API.

<details>
<summary>Check out Global IDs for relay:</summary>

A Global ID is a unique identifier for an object across the entire application, not just within a specific table or
type. Think of it as an ID which you can use to fetch any object directly, regardless of what kind of object it is. This
is different from typical database IDs, which are often guaranteed unique only within a particular table.

[//]: # "TODO: As long as it has already been fetched?"

Hasura's Global ID implementation can be used to provide options for GraphQL clients to elegantly handle caching and
data re-fetching in a predictable and standardized way.

The Global ID generated by Hasura DDN follows the
[Relay Global ID spec](https://relay.dev/graphql/objectidentification.htm).

As the example below shows, the `user` object type has a field `user_id` that uniquely identifies a user. The Global ID
for the `user` object type will be generated using the `user_id` field:

For the following request on a model which has enabled Global ID:

```graphql
{
  user_by_id(user_id: 1) {
    id // Global ID
    user_id
    name
  }
}
```

The response obtained should look like the following:

```json
{
  "data": {
    "user_by_id": {
      "id": "eyJ2ZXJzaW9uIjoxLCJ0eXBlbmFtZSI6IkFydGljbGUiLCJpZCI6eyJhcnRpY2xlX2lkIjoyfX0=",
      "user_id": 1,
      "name": "Bob"
    }
  }
}
```

Now, with the Global ID received above, the `User` object corresponding to `user_id: 1` can be retrieved, as shown
below.

```graphql
{
  node(id: "eyJ2ZXJzaW9uIjoxLCJ0eXBlbmFtZSI6IkFydGljbGUiLCJpZCI6eyJhcnRpY2xlX2lkIjoyfX0=") {
    id
    __typename
    ... on User {
      name
    }
  }
}
```

The response to the above request should identify the `User` with `user_id: 1`.

```json
{
  "node": {
    "id": "eyJ2ZXJzaW9uIjoxLCJ0eXBlbmFtZSI6IkFydGljbGUiLCJpZCI6eyJhcnRpY2xlX2lkIjoyfX0=",
    "__typename": "User",
    "name": "Bob"
  }
}
```

</details>

---

## Metadata structure

#### Model {#model-model}

The definition of a data model. A data model is a collection of objects of a particular type. Models can support one or
more CRUD operations.

**One of the following values:**

| Value                    | Description |
| ------------------------ | ----------- |
| [ModelV1](#model-model1) |             |
| [ModelV2](#model-model2) |             |

**Example:**

```yaml
kind: Model
version: v1
definition:
  name: Articles
  objectType: article
  globalIdSource: true
  arguments: []
  source:
    dataConnectorName: data_connector
    collection: articles
    argumentMapping: {}
  filterExpressionType: Article_bool_exp
  orderableFields:
    - fieldName: article_id
      orderByDirections:
        enableAll: true
    - fieldName: title
      orderByDirections:
        enableAll: true
    - fieldName: author_id
      orderByDirections:
        enableAll: true
  graphql:
    selectUniques:
      - queryRootField: ArticleByID
        uniqueIdentifier:
          - article_id
        description: Description for the select unique ArticleByID
    selectMany:
      queryRootField: ArticleMany
      description: Description for the select many ArticleMany
    orderByExpressionType: Article_Order_By
    apolloFederation:
      entitySource: true
  description: Description for the model Articles
```

#### ModelV2 {#model-modelv2}

| Key          | Value                   | Required | Description                                                                                                                                                                                                                        |
| ------------ | ----------------------- | -------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `kind`       | `Model`                 | true     |                                                                                                                                                                                                                                    |
| `version`    | `v2`                    | true     |                                                                                                                                                                                                                                    |
| `definition` | [Model2](#model-model2) | true     | The definition of a data model. A data model is a collection of objects of a particular type. Models can support one or more CRUD operations. ModelV2 implements the changes described in rfcs/open-dd-expression-type-changes.md. |

#### Model2 {#model-model2}

The definition of a data model. A data model is a collection of objects of a particular type. Models can support one or
more CRUD operations. ModelV2 implements the changes described in rfcs/open-dd-expression-type-changes.md.

| Key                    | Value                                                              | Required | Description                                                                                     |
| ---------------------- | ------------------------------------------------------------------ | -------- | ----------------------------------------------------------------------------------------------- |
| `name`                 | [ModelName](#model-modelname)                                      | true     | The name of the data model.                                                                     |
| `objectType`           | [CustomTypeName](#model-customtypename)                            | true     | The type of the objects of which this model is a collection.                                    |
| `globalIdSource`       | boolean                                                            | false    | Whether this model should be used as the global ID source for all objects of its type.          |
| `arguments`            | [[ArgumentDefinition](#model-argumentdefinition)]                  | false    | A list of arguments accepted by this model. Defaults to no arguments.                           |
| `source`               | [ModelSource](#model-modelsource) / null                           | false    | The source configuration for this model.                                                        |
| `filterExpressionType` | [CustomTypeName](#model-customtypename) / null                     | false    | The boolean expression type that should be used to perform filtering on this model.             |
| `orderByExpression`    | [OrderByExpressionName](#model-orderbyexpressionname) / null       | false    | The order by expression to use for this model.                                                  |
| `aggregateExpression`  | [AggregateExpressionName](#model-aggregateexpressionname) / null   | false    | The name of the AggregateExpression that defines how to aggregate over this model               |
| `graphql`              | [ModelGraphQlDefinitionV2](#model-modelgraphqldefinitionv2) / null | false    | Configuration for how this model should appear in the GraphQL schema.                           |
| `description`          | string / null                                                      | false    | The description of the model. Gets added to the description of the model in the graphql schema. |

#### ModelGraphQlDefinitionV2 {#model-modelgraphqldefinitionv2}

The definition of how a model appears in the GraphQL API. Note: ModelGraphQlDefinitionV2 removed the
`order_by_expression_type` property. See rfcs/open-dd-expression-type-changes.md.

| Key                   | Value                                                                                  | Required | Description                                                                                                                                    |
| --------------------- | -------------------------------------------------------------------------------------- | -------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| `selectUniques`       | [[SelectUniqueGraphQlDefinition](#model-selectuniquegraphqldefinition)]                | true     | For each select unique defined here, a query root field is added to the GraphQL API that can be used to select a unique object from the model. |
| `selectMany`          | [SelectManyGraphQlDefinition](#model-selectmanygraphqldefinition) / null               | false    | Select many configuration for a model adds a query root field to the GraphQl API that can be used to retrieve multiple objects from the model. |
| `argumentsInputType`  | [GraphQlTypeName](#model-graphqltypename) / null                                       | false    | The type name of the input type used to hold the arguments of the model.                                                                       |
| `apolloFederation`    | [ModelApolloFederationConfiguration](#model-modelapollofederationconfiguration) / null | false    | Apollo Federation configuration                                                                                                                |
| `filterInputTypeName` | [GraphQlTypeName](#model-graphqltypename) / null                                       | false    | The type name of the input type used to hold the filtering settings used by aggregates (etc) to filter their input before processing           |
| `aggregate`           | [ModelAggregateGraphQlDefinition](#model-modelaggregategraphqldefinition) / null       | false    | Configures the query root field added to the GraphQL API that can be used to aggregate over the model                                          |

**Example:**

```yaml
selectUniques:
  - queryRootField: ArticleByID
    uniqueIdentifier:
      - article_id
    description: Description for the select unique ArticleByID
selectMany:
  queryRootField: ArticleMany
  description: Description for the select many ArticleMany
aggregate:
  queryRootField: ArticleAggregate
  description: Aggregate over Articles
```

#### OrderByExpressionName {#model-orderbyexpressionname}

The name of an order by expression.

**Value:** string

#### ModelV1 {#model-modelv1}

| Key          | Value                   | Required | Description                                                                                                                                   |
| ------------ | ----------------------- | -------- | --------------------------------------------------------------------------------------------------------------------------------------------- |
| `kind`       | `Model`                 | true     |                                                                                                                                               |
| `version`    | `v1`                    | true     |                                                                                                                                               |
| `definition` | [Model1](#model-model1) | true     | The definition of a data model. A data model is a collection of objects of a particular type. Models can support one or more CRUD operations. |

#### Model1 {#model-model1}

The definition of a data model. A data model is a collection of objects of a particular type. Models can support one or
more CRUD operations.

| Key                    | Value                                                            | Required | Description                                                                                     |
| ---------------------- | ---------------------------------------------------------------- | -------- | ----------------------------------------------------------------------------------------------- |
| `name`                 | [ModelName](#model-modelname)                                    | true     | The name of the data model.                                                                     |
| `objectType`           | [CustomTypeName](#model-customtypename)                          | true     | The type of the objects of which this model is a collection.                                    |
| `globalIdSource`       | boolean                                                          | false    | Whether this model should be used as the global ID source for all objects of its type.          |
| `arguments`            | [[ArgumentDefinition](#model-argumentdefinition)]                | false    | A list of arguments accepted by this model. Defaults to no arguments.                           |
| `source`               | [ModelSource](#model-modelsource) / null                         | false    | The source configuration for this model.                                                        |
| `filterExpressionType` | [CustomTypeName](#model-customtypename) / null                   | false    | The boolean expression type that should be used to perform filtering on this model.             |
| `orderableFields`      | [[OrderableField](#model-orderablefield)]                        | true     | A list of fields that can be used to order the objects in this model.                           |
| `aggregateExpression`  | [AggregateExpressionName](#model-aggregateexpressionname) / null | false    | The name of the AggregateExpression that defines how to aggregate over this model               |
| `graphql`              | [ModelGraphQlDefinition](#model-modelgraphqldefinition) / null   | false    | Configuration for how this model should appear in the GraphQL schema.                           |
| `description`          | string / null                                                    | false    | The description of the model. Gets added to the description of the model in the graphql schema. |

#### ModelGraphQlDefinition {#model-modelgraphqldefinition}

The definition of how a model appears in the GraphQL API.

| Key                     | Value                                                                                  | Required | Description                                                                                                                                    |
| ----------------------- | -------------------------------------------------------------------------------------- | -------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| `selectUniques`         | [[SelectUniqueGraphQlDefinition](#model-selectuniquegraphqldefinition)]                | true     | For each select unique defined here, a query root field is added to the GraphQL API that can be used to select a unique object from the model. |
| `selectMany`            | [SelectManyGraphQlDefinition](#model-selectmanygraphqldefinition) / null               | false    | Select many configuration for a model adds a query root field to the GraphQl API that can be used to retrieve multiple objects from the model. |
| `argumentsInputType`    | [GraphQlTypeName](#model-graphqltypename) / null                                       | false    | The type name of the input type used to hold the arguments of the model.                                                                       |
| `orderByExpressionType` | [GraphQlTypeName](#model-graphqltypename) / null                                       | false    | The type name of the order by expression input type.                                                                                           |
| `apolloFederation`      | [ModelApolloFederationConfiguration](#model-modelapollofederationconfiguration) / null | false    | Apollo Federation configuration                                                                                                                |
| `filterInputTypeName`   | [GraphQlTypeName](#model-graphqltypename) / null                                       | false    | The type name of the input type used to hold the filtering settings used by aggregates (etc) to filter their input before processing           |
| `aggregate`             | [ModelAggregateGraphQlDefinition](#model-modelaggregategraphqldefinition) / null       | false    | Configures the query root field added to the GraphQL API that can be used to aggregate over the model                                          |

**Example:**

```yaml
selectUniques:
  - queryRootField: ArticleByID
    uniqueIdentifier:
      - article_id
    description: Description for the select unique ArticleByID
selectMany:
  queryRootField: ArticleMany
  description: Description for the select many ArticleMany
orderByExpressionType: Article_Order_By
aggregate:
  queryRootField: ArticleAggregate
  description: Aggregate over Articles
```

#### ModelAggregateGraphQlDefinition {#model-modelaggregategraphqldefinition}

The definition of the GraphQL API for aggregating over a model.

| Key              | Value                                                                        | Required | Description                                                                                                                                                     |
| ---------------- | ---------------------------------------------------------------------------- | -------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `queryRootField` | [GraphQlFieldName](#model-graphqlfieldname)                                  | true     | The name of the query root field for this API.                                                                                                                  |
| `description`    | string / null                                                                | false    | The description of the aggregate graphql definition of the model. Gets added to the description of the aggregate root field of the model in the graphql schema. |
| `deprecated`     | [Deprecated](#model-deprecated) / null                                       | false    | Whether this aggregate query field is deprecated. If set, the deprecation status is added to the aggregate root field's graphql schema.                         |
| `subscription`   | [SubscriptionGraphQlDefinition](#model-subscriptiongraphqldefinition) / null | false    | Enable subscription on this aggregate root field.                                                                                                               |

#### ModelApolloFederationConfiguration {#model-modelapollofederationconfiguration}

Apollo Federation configuration for a model.

| Key            | Value   | Required | Description                                                                                   |
| -------------- | ------- | -------- | --------------------------------------------------------------------------------------------- |
| `entitySource` | boolean | true     | Whether this model should be used as the source for fetching \_entity for object of its type. |

#### GraphQlTypeName {#model-graphqltypename}

The name of a GraphQL type.

**Value:** string

#### SelectManyGraphQlDefinition {#model-selectmanygraphqldefinition}

The definition of the GraphQL API for selecting rows from a model.

| Key              | Value                                                                        | Required | Description                                                                                                                                                         |
| ---------------- | ---------------------------------------------------------------------------- | -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `queryRootField` | [GraphQlFieldName](#model-graphqlfieldname)                                  | true     | The name of the query root field for this API.                                                                                                                      |
| `description`    | string / null                                                                | false    | The description of the select many graphql definition of the model. Gets added to the description of the select many root field of the model in the graphql schema. |
| `deprecated`     | [Deprecated](#model-deprecated) / null                                       | false    | Whether this select many query field is deprecated. If set, the deprecation status is added to the select many root field's graphql schema.                         |
| `subscription`   | [SubscriptionGraphQlDefinition](#model-subscriptiongraphqldefinition) / null | false    | Enable subscription on this select many root field.                                                                                                                 |

#### SelectUniqueGraphQlDefinition {#model-selectuniquegraphqldefinition}

The definition of the GraphQL API for selecting a unique row/object from a model.

| Key                | Value                                                                        | Required | Description                                                                                                                                                             |
| ------------------ | ---------------------------------------------------------------------------- | -------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `queryRootField`   | [GraphQlFieldName](#model-graphqlfieldname)                                  | true     | The name of the query root field for this API.                                                                                                                          |
| `uniqueIdentifier` | [[FieldName](#model-fieldname)]                                              | true     | A set of fields which can uniquely identify a row/object in the model.                                                                                                  |
| `description`      | string / null                                                                | false    | The description of the select unique graphql definition of the model. Gets added to the description of the select unique root field of the model in the graphql schema. |
| `deprecated`       | [Deprecated](#model-deprecated) / null                                       | false    | Whether this select unique query field is deprecated. If set, the deprecation status is added to the select unique root field's graphql schema.                         |
| `subscription`     | [SubscriptionGraphQlDefinition](#model-subscriptiongraphqldefinition) / null | false    | Enable subscription on this select unique root field.                                                                                                                   |

#### SubscriptionGraphQlDefinition {#model-subscriptiongraphqldefinition}

The definition of the GraphQL API for enabling subscription on query root fields.

| Key                 | Value                                       | Required | Description                                                                                                                                  |
| ------------------- | ------------------------------------------- | -------- | -------------------------------------------------------------------------------------------------------------------------------------------- |
| `rootField`         | [GraphQlFieldName](#model-graphqlfieldname) | true     | The name of the subscription root field.                                                                                                     |
| `description`       | string / null                               | false    | The description of the subscription graphql definition. Gets added to the description of the subscription root field in the graphql schema.  |
| `deprecated`        | [Deprecated](#model-deprecated) / null      | false    | Whether this subscription root field is deprecated. If set, the deprecation status is added to the subscription root field's graphql schema. |
| `pollingIntervalMs` | integer                                     | false    | Polling interval in milliseconds for the subscription.                                                                                       |

#### Deprecated {#model-deprecated}

OpenDd configuration to indicate whether an object type field, relationship, model root field or command root field is
deprecated.

| Key      | Value         | Required | Description                 |
| -------- | ------------- | -------- | --------------------------- |
| `reason` | string / null | false    | The reason for deprecation. |

#### GraphQlFieldName {#model-graphqlfieldname}

The name of a GraphQL object field.

**Value:** string

#### AggregateExpressionName {#model-aggregateexpressionname}

The name of an aggregate expression.

**Value:** string

#### OrderableField {#model-orderablefield}

A field that can be used to order the objects in a model.

| Key                 | Value                                             | Required | Description                    |
| ------------------- | ------------------------------------------------- | -------- | ------------------------------ |
| `fieldName`         | [FieldName](#model-fieldname)                     | true     |                                |
| `orderByDirections` | [EnableAllOrSpecific](#model-enableallorspecific) | true     | Enable all or specific values. |

#### EnableAllOrSpecific {#model-enableallorspecific}

Enable all or specific values.

**Must have exactly one of the following fields:**

| Key              | Value                                         | Required | Description |
| ---------------- | --------------------------------------------- | -------- | ----------- |
| `enableAll`      | boolean                                       | false    |             |
| `enableSpecific` | [[OrderByDirection](#model-orderbydirection)] | false    |             |

#### OrderByDirection {#model-orderbydirection}

**Value:** `Asc` / `Desc`

#### FieldName {#model-fieldname}

The name of a field in a user-defined object type.

**Value:** string

#### ModelSource {#model-modelsource}

Description of how a model maps to a particular data connector

| Key                 | Value                                         | Required | Description                                                                    |
| ------------------- | --------------------------------------------- | -------- | ------------------------------------------------------------------------------ |
| `dataConnectorName` | [DataConnectorName](#model-dataconnectorname) | true     | The name of the data connector backing this model.                             |
| `collection`        | [CollectionName](#model-collectionname)       | true     | The collection in the data connector that backs this model.                    |
| `argumentMapping`   | [ArgumentMapping](#model-argumentmapping)     | false    | Mapping from model argument names to data connector collection argument names. |

**Example:**

```yaml
dataConnectorName: data_connector
collection: articles
```

#### ArgumentMapping {#model-argumentmapping}

Mapping of a comand or model argument name to the corresponding argument name used in the data connector. The key of
this object is the argument name used in the command or model and the value is the argument name used in the data
connector.

| Key           | Value                                                         | Required | Description |
| ------------- | ------------------------------------------------------------- | -------- | ----------- |
| `<customKey>` | [DataConnectorArgumentName](#model-dataconnectorargumentname) | false    |             |

#### DataConnectorArgumentName {#model-dataconnectorargumentname}

The name of an argument as defined by a data connector.

**Value:** string

#### CollectionName {#model-collectionname}

The name of a collection in a data connector.

**Value:** string

#### DataConnectorName {#model-dataconnectorname}

The name of a data connector.

**Value:** string

#### ArgumentDefinition {#model-argumentdefinition}

The definition of an argument for a field, command, or model.

| Key           | Value                                 | Required | Description              |
| ------------- | ------------------------------------- | -------- | ------------------------ |
| `name`        | [ArgumentName](#model-argumentname)   | true     | The name of an argument. |
| `type`        | [TypeReference](#model-typereference) | true     |                          |
| `description` | string / null                         | false    |                          |

#### TypeReference {#model-typereference}

A reference to an Open DD type including nullable values and arrays. Suffix '!' to indicate a non-nullable reference,
and wrap in '[]' to indicate an array. Eg: '[String!]!' is a non-nullable array of non-nullable strings.

**Value:** string

#### ArgumentName {#model-argumentname}

The name of an argument.

**Value:** string

#### CustomTypeName {#model-customtypename}

The name of a user-defined type.

**Value:** string

#### ModelName {#model-modelname}

The name of data model.

**Value:** string



==============================



# commands.mdx

URL: https://hasura.io/docs/promptql/reference/metadata-reference/commands

# Commands

## Introduction

In Hasura DDN, a **command** represents an action that can be executed within your data domain. Commands act as the
verbs or operations that modify or interact with the data managed by your
[models](/reference/metadata-reference/models.mdx). They can be triggered via GraphQL queries or mutations.

Commands have a fixed definition and are backed by [functions](#command-functionname) (exposed as queries) or
[procedures](#command-procedurename) (exposed as mutations) allowing you to add top-level methods to your API, or, add a
custom field to an existing model.

Commands also allow you to execute [custom business logic](/business-logic/overview.mdx) directly from your GraphQL API
and are useful in validating, processing or enriching data, calling another API, or, for example, logging a user in.

## How commands work

### Lifecycle

Commands can be added to your metadata [using the CLI](/reference/cli/commands/ddn_command_add.mdx). The CLI will use a
connector's [DataConnectorLink object](/reference/metadata-reference/data-connector-links.mdx) to determine which
functions or procedures are available to be added as commands.

You should [update your commands](/reference/cli/commands/ddn_command_update.mdx) whenever you make changes to your data
sources and, in turn, your DataConnectorLink objects. This ensures that your API remains in sync with your data.

As an example, if you're usings the [TypesScript connector](/business-logic/overview.mdx) for business logic and add a
new function, you'll need to update your DataConnectorLink to ensure that the new function is present in the connector's
configuration. Then, add the new command to your metadata.

To make a new command available in your supergraph, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

### Examples

```yaml title="The following is an example of a command for a function:"
---
kind: Command
version: v1
definition:
  name: Hello
  outputType: String!
  arguments:
    - name: name
      type: String
  source:
    dataConnectorName: my_data_connector
    dataConnectorCommand:
      function: hello
  graphql:
    rootFieldName: hello
    rootFieldKind: Query
```

| **Field**                                  | **Description**                                                                                                        | **Reference**                                         |
| ------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------- |
| **kind**                                   | Specifies the type of object being defined, which is a command in this context.                                        | [Command](#command-command)                           |
| **version**                                | Indicates the version of the command's structure.                                                                      | [CommandV1](#command-commandv1)                       |
| **definition.name**                        | The name of the command, representing the action to be executed. You can configure this to whatever you wish.          | [CommandName](#command-commandname)                   |
| **definition.outputType**                  | Defines the return type of the command, specifying what kind of data is returned after execution.                      | [TypeReference](#command-typereference)               |
| **definition.arguments**                   | Lists the arguments the command can take, allowing customization of the command's execution based on input parameters. | [ArgumentDefinition](#command-argumentdefinition)     |
| **definition.source.dataConnectorName**    | The name of the data connector that backs this command, linking it to the data source or function.                     | [DataConnectorName](#command-dataconnectorname)       |
| **definition.source.dataConnectorCommand** | Specifies the function or procedure in the data connector's configuration file(s) that implements the command's logic. | [DataConnectorCommand](#command-dataconnectorcommand) |
| **definition.graphql.rootFieldName**       | The name of the root field in the GraphQL API for invoking this command.                                               | [GraphQlFieldName](#command-graphqlfieldname)         |
| **definition.graphql.rootFieldKind**       | Determines whether the command should be part of the Query or Mutation root in the GraphQL API.                        | [GraphQlRootFieldKind](#command-graphqlrootfieldkind) |

```graphql title="The HML example above results in this query:"
query {
  hello(name: "Hasura")
}
```

---

## Metadata structure

### Command {#command-command}

The definition of a command. A command is a user-defined operation which can take arguments and returns an output. The
semantics of a command are opaque to the Open DD specification.

| Key          | Value                           | Required | Description                                                                                                                                                  |
| ------------ | ------------------------------- | -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `kind`       | `Command`                       | true     |                                                                                                                                                              |
| `version`    | `v1`                            | true     |                                                                                                                                                              |
| `definition` | [CommandV1](#command-commandv1) | true     | Definition of an OpenDD Command, which is a custom operation that can take arguments and returns an output. The semantics of a command are opaque to OpenDD. |

**Example:**

```yaml
kind: Command
version: v1
definition:
  name: get_latest_article
  outputType: commandArticle
  arguments: []
  source:
    dataConnectorName: data_connector
    dataConnectorCommand:
      function: latest_article
    argumentMapping: {}
  graphql:
    rootFieldName: getLatestArticle
    rootFieldKind: Query
  description: Get the latest article
```

#### CommandV1 {#command-commandv1}

Definition of an OpenDD Command, which is a custom operation that can take arguments and returns an output. The
semantics of a command are opaque to OpenDD.

| Key           | Value                                                                | Required | Description                                                                                                      |
| ------------- | -------------------------------------------------------------------- | -------- | ---------------------------------------------------------------------------------------------------------------- |
| `name`        | [CommandName](#command-commandname)                                  | true     | The name of the command.                                                                                         |
| `outputType`  | [TypeReference](#command-typereference)                              | true     | The return type of the command.                                                                                  |
| `arguments`   | [[ArgumentDefinition](#command-argumentdefinition)]                  | false    | The list of arguments accepted by this command. Defaults to no arguments.                                        |
| `source`      | [CommandSource](#command-commandsource) / null                       | false    | The source configuration for this command.                                                                       |
| `graphql`     | [CommandGraphQlDefinition](#command-commandgraphqldefinition) / null | false    | Configuration for how this command should appear in the GraphQL schema.                                          |
| `description` | string / null                                                        | false    | The description of the command. Gets added to the description of the command's root field in the GraphQL schema. |

#### CommandGraphQlDefinition {#command-commandgraphqldefinition}

The definition of how a command should appear in the GraphQL API.

| Key             | Value                                                 | Required | Description                                                                                                            |
| --------------- | ----------------------------------------------------- | -------- | ---------------------------------------------------------------------------------------------------------------------- |
| `rootFieldName` | [GraphQlFieldName](#command-graphqlfieldname)         | true     | The name of the graphql root field to use for this command.                                                            |
| `rootFieldKind` | [GraphQlRootFieldKind](#command-graphqlrootfieldkind) | true     | Whether to put this command in the Query or Mutation root of the GraphQL API.                                          |
| `deprecated`    | [Deprecated](#command-deprecated) / null              | false    | Whether this command root field is deprecated. If set, this will be added to the graphql schema as a deprecated field. |

**Example:**

```yaml
rootFieldName: getLatestArticle
rootFieldKind: Query
```

#### Deprecated {#command-deprecated}

OpenDd configuration to indicate whether an object type field, relationship, model root field or command root field is
deprecated.

| Key      | Value         | Required | Description                 |
| -------- | ------------- | -------- | --------------------------- |
| `reason` | string / null | false    | The reason for deprecation. |

#### GraphQlRootFieldKind {#command-graphqlrootfieldkind}

Whether to put this command in the Query or Mutation root of the GraphQL API.

**Value:** `Query` / `Mutation`

#### GraphQlFieldName {#command-graphqlfieldname}

The name of a GraphQL object field.

**Value:** string

#### CommandSource {#command-commandsource}

Description of how a command maps to a particular data connector

| Key                    | Value                                                 | Required | Description                                                                                 |
| ---------------------- | ----------------------------------------------------- | -------- | ------------------------------------------------------------------------------------------- |
| `dataConnectorName`    | [DataConnectorName](#command-dataconnectorname)       | true     | The name of the data connector backing this command.                                        |
| `dataConnectorCommand` | [DataConnectorCommand](#command-dataconnectorcommand) | true     | The function/procedure in the data connector that backs this command.                       |
| `argumentMapping`      | [ArgumentMapping](#command-argumentmapping)           | false    | Mapping from command argument names to data connector function or procedure argument names. |

**Example:**

```yaml
dataConnectorName: data_connector
dataConnectorCommand:
  function: latest_article
argumentMapping: {}
```

#### ArgumentMapping {#command-argumentmapping}

Mapping of a comand or model argument name to the corresponding argument name used in the data connector. The key of
this object is the argument name used in the command or model and the value is the argument name used in the data
connector.

| Key           | Value                                                           | Required | Description |
| ------------- | --------------------------------------------------------------- | -------- | ----------- |
| `<customKey>` | [DataConnectorArgumentName](#command-dataconnectorargumentname) | false    |             |

#### DataConnectorArgumentName {#command-dataconnectorargumentname}

The name of an argument as defined by a data connector.

**Value:** string

#### DataConnectorCommand {#command-dataconnectorcommand}

The function/procedure in the data connector that backs this command.

**Must have exactly one of the following fields:**

| Key         | Value                                   | Required | Description                                  |
| ----------- | --------------------------------------- | -------- | -------------------------------------------- |
| `function`  | [FunctionName](#command-functionname)   | false    | The name of a function backing the command.  |
| `procedure` | [ProcedureName](#command-procedurename) | false    | The name of a procedure backing the command. |

#### ProcedureName {#command-procedurename}

The name of a procedure backing the command.

**Value:** string

#### FunctionName {#command-functionname}

The name of a function backing the command.

**Value:** string

#### DataConnectorName {#command-dataconnectorname}

The name of a data connector.

**Value:** string

#### ArgumentDefinition {#command-argumentdefinition}

The definition of an argument for a field, command, or model.

| Key           | Value                                   | Required | Description              |
| ------------- | --------------------------------------- | -------- | ------------------------ |
| `name`        | [ArgumentName](#command-argumentname)   | true     | The name of an argument. |
| `type`        | [TypeReference](#command-typereference) | true     |                          |
| `description` | string / null                           | false    |                          |

#### ArgumentName {#command-argumentname}

The name of an argument.

**Value:** string

#### TypeReference {#command-typereference}

A reference to an Open DD type including nullable values and arrays. Suffix '!' to indicate a non-nullable reference,
and wrap in '[]' to indicate an array. Eg: '[String!]!' is a non-nullable array of non-nullable strings.

**Value:** string

#### CommandName {#command-commandname}

The name of a command.

**Value:** string



==============================



# boolean-expressions.mdx

URL: https://hasura.io/docs/promptql/reference/metadata-reference/boolean-expressions

# Boolean Expressions

## Introduction

Hasura provides powerful tools to control filtering and selecting data. Boolean expression types let you control which
filters are available for a [model](/reference/metadata-reference/models.mdx) or
[command](/reference/metadata-reference/commands.mdx). They can be used to configure filters on models, or as the types
of arguments to commands or models.

## How Boolean expressions work

### Lifecycle

There are two types of boolean expressions:

| Type              | Description                                               |
| ----------------- | --------------------------------------------------------- |
| [Scalar](#scalar) | This specifies how a user able to compare a scalar field. |
| [Object](#object) | This specifies how fields of a type can be filtered.      |

Regardless of the type, boolean expressions are used to define how a user is able to filter data and are defined in your
metadata.

### Examples

#### Scalar

This specifies how a user is able to compare a scalar field. For instance, you might want to say that a user can only
check if a `String` type is equals to another, or whether it is null or not. You can do that with the following
metadata:

```yaml
kind: BooleanExpressionType
version: v1
definition:
  name: String_comparison_exp_with_eq_and_is_null
  operand:
    scalar:
      type: String
      comparisonOperators:
        - name: equals
          argumentType: String!
      dataConnectorOperatorMapping:
        - dataConnectorName: postgres
          dataConnectorScalarType: varchar
          operatorMapping:
            equals: _eq
  logicalOperators:
    enable: true
  isNull:
    enable: true
  graphql:
    typeName: String_comparison_exp_with_eq_and_is_null
```

Note the `dataConnectorOperatorMapping`. This allows us to define what these operators mean in zero or more data
connectors. Here, we want our `equals` operator to use Postgres's `_eq` operator.

This would allow us to write filters like:

```json
{ "first_name": { "_is_null": true } }
```

```json
{ "last_name": { "equals": "Bruce" } }
```

#### Object

An object `BooleanExpressionType` is used to define how fields of a type can be filtered. Note that nothing here talks
about specific data connectors - instead you can specify which `BooleanExpressionType` is used to filter each field or
relationship, and then defer the mappings of individual scalar types to those `BooleanExpressionType`s.

```yaml
kind: BooleanExpressionType
version: v2
definition:
  name: Album_bool_exp
  operand:
    object:
      type: Album
      comparableFields:
        - fieldName: AlbumId
          booleanExpressionType: Int_comparison_exp
        - fieldName: ArtistId
          booleanExpressionType: Int_comparison_exp_with_is_null
        - fieldName: Address
          booleanExpressionType: Address_bool_exp
      comparableRelationships:
        - relationshipName: artist
          booleanExpressionType: Artist_bool_exp
  logicalOperators:
    enable: true
  isNull:
    enable: true
  graphql:
    typeName: Album_bool_exp
```

Note here that we can specify different comparison operators for `AlbumId` and `ArtistId` by using different
`BooleanExpressionType`s for them. We are also able to define filtering on nested objects such as `Address`.

The above would let us write filters on `Album` types like:

```json
{ "album": { "AlbumId": { "equals": 100 } } }
```

```json
{ "album": { "Address": { "postcode": { "like": "N1" } } } }
```

```json
{ "album": { "artist": { "name": { "equals": "Madonna" } } } }
```

---

## Metadata structure


### BooleanExpressionType {#booleanexpressiontype-booleanexpressiontype}

Definition of a type representing a boolean expression on an OpenDD type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `BooleanExpressionType` | true |  |
| `version` | `v1` | true |  |
| `definition` | [BooleanExpressionTypeV1](#booleanexpressiontype-booleanexpressiontypev1) | true | Definition of a type representing a boolean expression on an OpenDD object type. |

 **Example:**

```yaml
kind: BooleanExpressionType
version: v1
definition:
  name: Album_bool_exp
  operand:
    object:
      type: Album
      comparableFields:
        - fieldName: AlbumId
          booleanExpressionType: pg_Int_Comparison_exp
        - fieldName: ArtistId
          booleanExpressionType: pg_Int_Comparison_exp_with_is_null
        - fieldName: Address
          booleanExpressionType: Address_bool_exp
      comparableRelationships:
        - relationshipName: artist
          booleanExpressionType: Artist_bool_exp
  logicalOperators:
    enable: true
  isNull:
    enable: true
  graphql:
    typeName: App_Album_bool_exp
```


#### BooleanExpressionTypeV1 {#booleanexpressiontype-booleanexpressiontypev1}

Definition of a type representing a boolean expression on an OpenDD object type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [CustomTypeName](#booleanexpressiontype-customtypename) | true | The name to give this boolean expression type, used to refer to it elsewhere in the metadata. Must be unique across all types defined in this subgraph. |
| `operand` | [BooleanExpressionOperand](#booleanexpressiontype-booleanexpressionoperand) | true | The type that this boolean expression applies to. |
| `logicalOperators` | [BooleanExpressionLogicalOperators](#booleanexpressiontype-booleanexpressionlogicaloperators) | true | Whether to enable _and / _or / _not |
| `isNull` | [BooleanExpressionIsNull](#booleanexpressiontype-booleanexpressionisnull) | true | Whether to enable _is_null |
| `graphql` | [BooleanExpressionTypeGraphQlConfiguration](#booleanexpressiontype-booleanexpressiontypegraphqlconfiguration) / null | false | Configuration for how this object type should appear in the GraphQL schema. |



#### BooleanExpressionTypeGraphQlConfiguration {#booleanexpressiontype-booleanexpressiontypegraphqlconfiguration}

GraphQL configuration of an OpenDD boolean expression type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `typeName` | [GraphQlTypeName](#booleanexpressiontype-graphqltypename) | true | The name to use for the GraphQL type representation of this boolean expression type. |



#### GraphQlTypeName {#booleanexpressiontype-graphqltypename}

The name of a GraphQL type.


**Value:** string


#### BooleanExpressionIsNull {#booleanexpressiontype-booleanexpressionisnull}

Configuration for is_null in boolean expressions

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `enable` | boolean | true |  |



#### BooleanExpressionLogicalOperators {#booleanexpressiontype-booleanexpressionlogicaloperators}

Configuration for logical operators in boolean expressions

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `enable` | boolean | true |  |



#### BooleanExpressionOperand {#booleanexpressiontype-booleanexpressionoperand}

Configuration for object or scalar boolean expression


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `object` | [BooleanExpressionObjectOperand](#booleanexpressiontype-booleanexpressionobjectoperand) | false | Definition of an object type representing a boolean expression on an OpenDD object type. |
| `scalar` | [BooleanExpressionScalarOperand](#booleanexpressiontype-booleanexpressionscalaroperand) | false | Definition of a scalar type representing a boolean expression on an OpenDD scalar type. |



#### BooleanExpressionScalarOperand {#booleanexpressiontype-booleanexpressionscalaroperand}

Definition of a scalar type representing a boolean expression on an OpenDD scalar type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `type` | [TypeName](#booleanexpressiontype-typename) | true | The OpenDD type name of the scalar type that this boolean expression applies to. |
| `comparisonOperators` | [[ComparisonOperator](#booleanexpressiontype-comparisonoperator)] | true | The list of comparison operators that can used on this scalar type |
| `dataConnectorOperatorMapping` | [[DataConnectorOperatorMapping](#booleanexpressiontype-dataconnectoroperatormapping)] | true | The list of mappings between OpenDD operator names and the names used in the data connector schema |



#### DataConnectorOperatorMapping {#booleanexpressiontype-dataconnectoroperatormapping}

Mapping between OpenDD operator names and the names used in the data connector schema

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `dataConnectorName` | [DataConnectorName](#booleanexpressiontype-dataconnectorname) | true | Name of the data connector this mapping applies to |
| `dataConnectorScalarType` | [DataConnectorScalarType](#booleanexpressiontype-dataconnectorscalartype) | true | Name of the scalar type according to the data connector's schema |
| `operatorMapping` | [operator_mapping](#booleanexpressiontype-operator_mapping) | true | Mapping between OpenDD operator names and the data connector's operator names Defaults to the same operator name (e.g. "_eq: _eq") if no explicit mapping is present. |



#### operator_mapping {#booleanexpressiontype-operator_mapping}

Mapping between OpenDD operator names and the data connector's operator names Defaults to the same operator name (e.g. "_eq: _eq") if no explicit mapping is present.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` | [DataConnectorOperatorName](#booleanexpressiontype-dataconnectoroperatorname) | false | The name of an operator in a data connector. |



#### DataConnectorOperatorName {#booleanexpressiontype-dataconnectoroperatorname}

The name of an operator in a data connector.


**Value:** string


#### DataConnectorScalarType {#booleanexpressiontype-dataconnectorscalartype}

The name of a scalar type in a data connector.


**Value:** string


#### DataConnectorName {#booleanexpressiontype-dataconnectorname}

The name of a data connector.


**Value:** string


#### ComparisonOperator {#booleanexpressiontype-comparisonoperator}

Definition of a comparison operator for a scalar type

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [OperatorName](#booleanexpressiontype-operatorname) | true | Name you want to give the operator in OpenDD / GraphQL |
| `argumentType` | [TypeReference](#booleanexpressiontype-typereference) | true | An OpenDD type |



#### TypeReference {#booleanexpressiontype-typereference}

A reference to an Open DD type including nullable values and arrays.
Suffix '!' to indicate a non-nullable reference, and wrap in '[]' to indicate an array.
Eg: '[String!]!' is a non-nullable array of non-nullable strings.


**Value:** string


#### OperatorName {#booleanexpressiontype-operatorname}

The name of an operator


**Value:** string


#### TypeName {#booleanexpressiontype-typename}

The OpenDD type name of the scalar type that this boolean expression applies to.


**One of the following values:**

| Value | Description |
|-----|-----|
| [InbuiltType](#booleanexpressiontype-inbuilttype) | An inbuilt primitive OpenDD type. |
| [CustomTypeName](#booleanexpressiontype-customtypename) |  |



#### InbuiltType {#booleanexpressiontype-inbuilttype}

An inbuilt primitive OpenDD type.


**Value:** `ID` / `Int` / `Float` / `Boolean` / `String`


#### BooleanExpressionObjectOperand {#booleanexpressiontype-booleanexpressionobjectoperand}

Definition of an object type representing a boolean expression on an OpenDD object type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `type` | [CustomTypeName](#booleanexpressiontype-customtypename) | true | The name of the object type that this boolean expression applies to. |
| `comparableFields` | [[BooleanExpressionComparableField](#booleanexpressiontype-booleanexpressioncomparablefield)] | true | The list of fields of the object type that can be used for comparison when evaluating this boolean expression. |
| `comparableRelationships` | [[BooleanExpressionComparableRelationship](#booleanexpressiontype-booleanexpressioncomparablerelationship)] | true | The list of relationships of the object type that can be used for comparison when evaluating this boolean expression. |



#### BooleanExpressionComparableRelationship {#booleanexpressiontype-booleanexpressioncomparablerelationship}

Definition of a relationship that can be used for a comparison

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `relationshipName` | [RelationshipName](#booleanexpressiontype-relationshipname) | true | The name of the relationship to use for comparison |
| `booleanExpressionType` | [CustomTypeName](#booleanexpressiontype-customtypename) / null | false | The boolean expression type to use for comparison. This is optional for relationships to models, and defaults to the filterExpressionType of the model |



#### RelationshipName {#booleanexpressiontype-relationshipname}

The name of the GraphQL relationship field.


**Value:** string


#### BooleanExpressionComparableField {#booleanexpressiontype-booleanexpressioncomparablefield}

Comparison configuration definition for a field that can be used for a comparison

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldName` | [FieldName](#booleanexpressiontype-fieldname) | true | The name of the field that can be compared. |
| `booleanExpressionType` | [CustomTypeName](#booleanexpressiontype-customtypename) | true | The boolean expression type that can be used for comparison against the type of the field. |



#### FieldName {#booleanexpressiontype-fieldname}

The name of a field in a user-defined object type.


**Value:** string


#### CustomTypeName {#booleanexpressiontype-customtypename}

The name of a user-defined type.


**Value:** string
### ObjectBooleanExpressionType {#objectbooleanexpressiontype-objectbooleanexpressiontype}

Definition of a type representing a boolean expression on an Open DD object type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `ObjectBooleanExpressionType` | true |  |
| `version` | `v1` | true |  |
| `definition` | [ObjectBooleanExpressionTypeV1](#objectbooleanexpressiontype-objectbooleanexpressiontypev1) | true | Definition of a type representing a boolean expression on an Open DD object type. Deprecated in favour of `BooleanExpressionType`. |

 **Example:**

```yaml
kind: ObjectBooleanExpressionType
version: v1
definition:
  name: AuthorBoolExp
  objectType: Author
  dataConnectorName: my_db
  dataConnectorObjectType: author
  comparableFields:
    - fieldName: article_id
      operators:
        enableAll: true
    - fieldName: title
      operators:
        enableAll: true
    - fieldName: author_id
      operators:
        enableAll: true
  graphql:
    typeName: Author_bool_exp
```


#### ObjectBooleanExpressionTypeV1 {#objectbooleanexpressiontype-objectbooleanexpressiontypev1}

Definition of a type representing a boolean expression on an Open DD object type. Deprecated in favour of `BooleanExpressionType`.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [CustomTypeName](#objectbooleanexpressiontype-customtypename) | true | The name to give this object boolean expression type, used to refer to it elsewhere in the metadata. Must be unique across all types defined in this subgraph. |
| `objectType` | [CustomTypeName](#objectbooleanexpressiontype-customtypename) | true | The name of the object type that this boolean expression applies to. |
| `dataConnectorName` | [DataConnectorName](#objectbooleanexpressiontype-dataconnectorname) | true | The data connector this boolean expression type is based on. |
| `dataConnectorObjectType` | [DataConnectorObjectType](#objectbooleanexpressiontype-dataconnectorobjecttype) | true | The object type in the data connector's schema this boolean expression type is based on. |
| `comparableFields` | [[ComparableField](#objectbooleanexpressiontype-comparablefield)] | true | The list of fields of the object type that can be used for comparison when evaluating this boolean expression. |
| `graphql` | [ObjectBooleanExpressionTypeGraphQlConfiguration](#objectbooleanexpressiontype-objectbooleanexpressiontypegraphqlconfiguration) / null | false | Configuration for how this object type should appear in the GraphQL schema. |



#### ObjectBooleanExpressionTypeGraphQlConfiguration {#objectbooleanexpressiontype-objectbooleanexpressiontypegraphqlconfiguration}

GraphQL configuration of an Open DD boolean expression type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `typeName` | [GraphQlTypeName](#objectbooleanexpressiontype-graphqltypename) | true | The name to use for the GraphQL type representation of this boolean expression type. |



#### GraphQlTypeName {#objectbooleanexpressiontype-graphqltypename}

The name of a GraphQL type.


**Value:** string


#### ComparableField {#objectbooleanexpressiontype-comparablefield}

A field of an object type that can be used for comparison when evaluating a boolean expression.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldName` | [FieldName](#objectbooleanexpressiontype-fieldname) | true |  |
| `operators` | [EnableAllOrSpecific](#objectbooleanexpressiontype-enableallorspecific) | true | Enable all or specific values. |



#### EnableAllOrSpecific {#objectbooleanexpressiontype-enableallorspecific}

Enable all or specific values.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `enableAll` | boolean | false |  |
| `enableSpecific` | [[OperatorName](#objectbooleanexpressiontype-operatorname)] | false |  |



#### OperatorName {#objectbooleanexpressiontype-operatorname}

The name of an operator


**Value:** string


#### FieldName {#objectbooleanexpressiontype-fieldname}

The name of a field in a user-defined object type.


**Value:** string


#### DataConnectorObjectType {#objectbooleanexpressiontype-dataconnectorobjecttype}

The name of an object type in a data connector.


**Value:** string


#### DataConnectorName {#objectbooleanexpressiontype-dataconnectorname}

The name of a data connector.


**Value:** string


#### CustomTypeName {#objectbooleanexpressiontype-customtypename}

The name of a user-defined type.


**Value:** string



==============================



# aggregate-expressions.mdx

URL: https://hasura.io/docs/promptql/reference/metadata-reference/aggregate-expressions

# Aggregate Expressions

## Introduction

Aggregate expression types allow you to summarize and calculate collective properties of your data. These expressions
enable you to define how to aggregate over different data types, facilitating efficient data manipulation and retrieval.

Aggregate expressions can be used to configure how data is summarized in your models, or as the types of arguments to
commands or models.

## How AggregateExpressions work

### Lifecycle

:::tip Examples below

Below this description of the lifecycle are examples for each step and how to define AggregateExpressions in your
metadata files.

:::

You'll need to ensure you've generated [ScalarTypes](/reference/metadata-reference/types.mdx#scalartype-scalartype) and
[DataConnectorScalarRepresentations](/reference/metadata-reference/data-connector-links.mdx#dataconnectorscalarrepresentation-dataconnectorscalarrepresentation)
for the data types you want to aggregate over. We recommend saving these in your connector's type definition file.

Once you have these, you can create AggregateExpression objects for the types you want to aggregate over. We recommend
these be saved in a model's metadata file.

Finally, you'll need to update your `graphql-config.hml` in the `globals` subgraph to
[include an `aggregate` field in the `definition`](/reference/metadata-reference/graphql-config.mdx#graphqlconfig-aggregategraphqlconfig).

Once these are included, you'll need to [create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx)
using the CLI.

### Examples

In the following example, we'll show the snippets necessary to implement the example explained above: an
AggregateExpression for the `Reviews` model of our sample application.

```yaml title="metadata/my_connector-types.hml"
# Existing definitions above
---
kind: ScalarType
version: v1
definition:
  name: Numeric
  graphql:
    typeName: Numeric

---
kind: DataConnectorScalarRepresentation
version: v1
definition:
  dataConnectorName: my_connector
  dataConnectorScalarType: numeric
  representation: Numeric
  graphql:
    comparisonExpressionTypeName: NumericComparisonExp
```

```yaml title="metadata/Reviews.hml"
# Existing objects above
---
kind: AggregateExpression
version: v1
definition:
  name: Int4_aggregate_exp
  operand:
    scalar:
      aggregatedType: Int4
      aggregationFunctions:
        - name: avg
          returnType: Numeric
      dataConnectorAggregationFunctionMapping:
        - dataConnectorName: my_connector
          dataConnectorScalarType: int4
          functionMapping:
            avg:
              name: avg
  graphql:
    selectTypeName: Int4_aggregate_fields

---
kind: AggregateExpression
version: v1
definition:
  name: Reviews_aggregate_exp
  operand:
    object:
      aggregatedType: Reviews
      aggregatableFields:
        - fieldName: rating
          aggregateExpression: Int4_aggregate_exp
  graphql:
    selectTypeName: Reviews_aggregate_fields
  description: Aggregate over Reviews
```

```yaml title="globals/graphql-config.hml"
kind: GraphqlConfig
version: v1
definition:
  query:
    rootOperationTypeName: Query
    argumentsInput:
      fieldName: args
    limitInput:
      fieldName: limit
    offsetInput:
      fieldName: offset
    filterInput:
      fieldName: where
      operatorNames:
        and: _and
        or: _or
        not: _not
        isNull: _is_null
    orderByInput:
      fieldName: order_by
      enumDirectionValues:
        asc: Asc
        desc: Desc
      enumTypeNames:
        - directions:
            - Asc
            - Desc
          typeName: OrderBy
    #highlight-start
    aggregate:
      filterInputFieldName: filter_input
      countFieldName: _count
      countDistinctFieldName: _count_distinct
    #highlight-end
  mutation:
    rootOperationTypeName: Mutation
```

---

## Metadata structure


### AggregateExpression {#aggregateexpression-aggregateexpression}

Definition of an aggregate expression on an OpenDD type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `AggregateExpression` | true |  |
| `version` | `v1` | true |  |
| `definition` | [AggregateExpressionV1](#aggregateexpression-aggregateexpressionv1) | true | Definition of how to aggregate over a particular operand type |

 **Example:**

```yaml
kind: AggregateExpression
version: v1
definition:
  name: Invoice_aggregate_exp
  operand:
    object:
      aggregatedType: Invoice
      aggregatableFields:
        - fieldName: Total
          aggregateExpression: Float_aggregate_exp
  graphql:
    selectTypeName: Invoice_aggregate_fields
  description: Aggregate over Invoices
```


#### AggregateExpressionV1 {#aggregateexpression-aggregateexpressionv1}

Definition of how to aggregate over a particular operand type

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [AggregateExpressionName](#aggregateexpression-aggregateexpressionname) | true | The name of the aggregate expression. |
| `operand` | [AggregateOperand](#aggregateexpression-aggregateoperand) | true | The type this aggregation expression aggregates over, and its associated configuration |
| `count` | [AggregateCountDefinition](#aggregateexpression-aggregatecountdefinition) / null | false | Configuration for the count aggregate function used over the operand |
| `countDistinct` | [AggregateCountDefinition](#aggregateexpression-aggregatecountdefinition) / null | false | Configuration for the count distinct aggregate function used over the operand |
| `graphql` | [AggregateExpressionGraphQlDefinition](#aggregateexpression-aggregateexpressiongraphqldefinition) / null | false | Configuration for how this command should appear in the GraphQL schema. |
| `description` | string / null | false | The description of the aggregate expression. Gets added to the description of the command's root field in the GraphQL schema. |



#### AggregateExpressionGraphQlDefinition {#aggregateexpression-aggregateexpressiongraphqldefinition}

The definition of how an aggregate expression should appear in the GraphQL API.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `selectTypeName` | [GraphQlTypeName](#aggregateexpression-graphqltypename) | true | The type name to use for the aggregate selection type |
| `deprecated` | [Deprecated](#aggregateexpression-deprecated) / null | false | Whether this command root field is deprecated. If set, this will be added to the graphql schema as a deprecated field. |

 **Example:**

```yaml
selectTypeName: Invoice_aggregate_fields
```


#### Deprecated {#aggregateexpression-deprecated}

OpenDd configuration to indicate whether an object type field, relationship, model root field or command root field is deprecated.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `reason` | string / null | false | The reason for deprecation. |



#### GraphQlTypeName {#aggregateexpression-graphqltypename}

The name of a GraphQL type.


**Value:** string


#### AggregateCountDefinition {#aggregateexpression-aggregatecountdefinition}

Definition of a count aggregation function

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `enable` | boolean | true | Whether or not the aggregate function is available for use or not |
| `description` | string / null | false | A description of the aggregation function. Gets added to the description of the field in the GraphQL schema. |
| `returnType` | [TypeName](#aggregateexpression-typename) / null | false | The scalar type that the count aggregation function returns. Must be an integer type. If omitted, Int is used as the default. |



#### AggregateOperand {#aggregateexpression-aggregateoperand}

Definition of an aggregate expression's operand


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `object` | [ObjectAggregateOperand](#aggregateexpression-objectaggregateoperand) | false | Definition of an aggregate over an object-typed operand |
| `scalar` | [ScalarAggregateOperand](#aggregateexpression-scalaraggregateoperand) | false | Definition of an aggregate over a scalar-typed operand |



#### ScalarAggregateOperand {#aggregateexpression-scalaraggregateoperand}

Definition of an aggregate over a scalar-typed operand

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `aggregatedType` | [TypeName](#aggregateexpression-typename) | true | The name of the scalar type the aggregate expression is aggregating |
| `aggregationFunctions` | [[AggregationFunctionDefinition](#aggregateexpression-aggregationfunctiondefinition)] | true | The aggregation functions that operate over the scalar type |
| `dataConnectorAggregationFunctionMapping` | [[DataConnectorAggregationFunctionMapping](#aggregateexpression-dataconnectoraggregationfunctionmapping)] | true | Mapping of aggregation functions to corresponding aggregation functions in various data connectors |



#### DataConnectorAggregationFunctionMapping {#aggregateexpression-dataconnectoraggregationfunctionmapping}

Definition of how to map an aggregate expression's aggregation functions to data connector aggregation functions.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `dataConnectorName` | [DataConnectorName](#aggregateexpression-dataconnectorname) | true | The data connector being mapped to |
| `dataConnectorScalarType` | [DataConnectorScalarType](#aggregateexpression-dataconnectorscalartype) | true | The matching scalar type in the data connector for the operand scalar type |
| `functionMapping` | [AggregationFunctionMappings](#aggregateexpression-aggregationfunctionmappings) | true | Mapping from Open DD aggregation function to data connector aggregation function |



#### AggregationFunctionMappings {#aggregateexpression-aggregationfunctionmappings}

Mapping of aggregation functions to their matching aggregation functions in the data connector.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` | [AggregateFunctionMapping](#aggregateexpression-aggregatefunctionmapping) | false | Definition of how to map the aggregation function to a function in the data connector |



#### AggregateFunctionMapping {#aggregateexpression-aggregatefunctionmapping}

Definition of how to map the aggregation function to a function in the data connector

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [DataConnectorAggregationFunctionName](#aggregateexpression-dataconnectoraggregationfunctionname) | true | The name of the aggregation function in the data connector |



#### DataConnectorAggregationFunctionName {#aggregateexpression-dataconnectoraggregationfunctionname}

The name of an aggregation function in a data connector


**Value:** string


#### DataConnectorScalarType {#aggregateexpression-dataconnectorscalartype}

The name of a scalar type in a data connector.


**Value:** string


#### DataConnectorName {#aggregateexpression-dataconnectorname}

The name of a data connector.


**Value:** string


#### AggregationFunctionDefinition {#aggregateexpression-aggregationfunctiondefinition}

Definition of an aggregation function

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [AggregationFunctionName](#aggregateexpression-aggregationfunctionname) | true | The name of the aggregation function |
| `description` | string / null | false | A description of the aggregation function. Gets added to the description of the field in the GraphQL schema. |
| `returnType` | [TypeReference](#aggregateexpression-typereference) | true |  |



#### TypeReference {#aggregateexpression-typereference}

A reference to an Open DD type including nullable values and arrays.
Suffix '!' to indicate a non-nullable reference, and wrap in '[]' to indicate an array.
Eg: '[String!]!' is a non-nullable array of non-nullable strings.


**Value:** string


#### AggregationFunctionName {#aggregateexpression-aggregationfunctionname}

The name of an aggregation function.


**Value:** string


#### TypeName {#aggregateexpression-typename}

The name of the scalar type the aggregate expression is aggregating


**One of the following values:**

| Value | Description |
|-----|-----|
| [InbuiltType](#aggregateexpression-inbuilttype) | An inbuilt primitive OpenDD type. |
| [CustomTypeName](#aggregateexpression-customtypename) |  |



#### InbuiltType {#aggregateexpression-inbuilttype}

An inbuilt primitive OpenDD type.


**Value:** `ID` / `Int` / `Float` / `Boolean` / `String`


#### ObjectAggregateOperand {#aggregateexpression-objectaggregateoperand}

Definition of an aggregate over an object-typed operand

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `aggregatedType` | [CustomTypeName](#aggregateexpression-customtypename) | true | The name of the object type the aggregate expression is aggregating |
| `aggregatableFields` | [[AggregatableFieldDefinition](#aggregateexpression-aggregatablefielddefinition)] | true | The fields on the object that are aggregatable |



#### AggregatableFieldDefinition {#aggregateexpression-aggregatablefielddefinition}

Definition of an aggregatable field on an object type

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldName` | [FieldName](#aggregateexpression-fieldname) | true | The name of the field on the operand aggregated type that is aggregatable |
| `description` | string / null | false | A description of the aggregatable field. Gets added to the description of the field in the GraphQL schema. |
| `aggregateExpression` | [AggregateExpressionName](#aggregateexpression-aggregateexpressionname) | true | The aggregate expression used to aggregate the type of the field |



#### FieldName {#aggregateexpression-fieldname}

The name of a field in a user-defined object type.


**Value:** string


#### CustomTypeName {#aggregateexpression-customtypename}

The name of a user-defined type.


**Value:** string


#### AggregateExpressionName {#aggregateexpression-aggregateexpressionname}

The name of an aggregate expression.


**Value:** string



==============================



# orderby-expressions.mdx

URL: https://hasura.io/docs/promptql/reference/metadata-reference/orderby-expressions

# OrderBy Expressions

## Introduction

Hasura provides powerful tools to control sorting and ordering data. OrderByExpression types let you define customizable
sorting rules for a [type](/reference/metadata-reference/types.mdx).

## How OrderByExpressions work

There are two types of OrderByExpressions:

| Type              | Description                                                |
| ----------------- | ---------------------------------------------------------- |
| [Scalar](#scalar) | This specifies how a user is able to order a scalar field. |
| [Object](#object) | This specifies how fields of a type can be ordered.        |

### Examples

#### Scalar

This specifies how a user is able to order a scalar field. For instance, you might want to say that a user can only sort
a `String` type in ascending order instead of both ascending and descending order. You can do that with the following
metadata:

```yaml
kind: OrderByExpression
version: v1
definition:
  name: String_orderby_exp_asc
  operand:
    scalar:
      orderedType: String
      enableOrderByDirections:
        enableSpecific:
          - Asc
  graphql:
    expressionTypeName: StringOrderByExp
```

#### Object

An object `OrderByExpression` is used to define how fields of a type can be ordered. You can specify which
`OrderByExpression` is used to order each field or relationship, and then defer the mappings of individual scalar types
to those `OrderByExpression`s.

```yaml
kind: OrderByExpression
version: v1
definition:
  name: Album_orderby_exp
  operand:
    object:
      orderedType: Album
      orderableFields:
        - fieldName: AlbumId
          orderByExpression: Int_orderby_exp
        - fieldName: ArtistId
          orderByExpression: Int_orderby_exp_asc
        - fieldName: Address
          orderByExpression: Address_orderby_exp
      orderableRelationships:
        - relationshipName: artist
          orderByExpression: Artist_orderby_exp
        - relationshipName: genre
  graphql:
    expressionTypeName: Album_orderby_Exp
```

Note here that we can specify different orderBy operators for `AlbumId` and `ArtistId` by using different
`OrderByExpression`s for them. We are also able to define ordering on nested objects such as `Address`.

---

## Metadata structure


### OrderByExpression {#orderbyexpression-orderbyexpression}

Definition of an order by expression on an OpenDD type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `OrderByExpression` | true |  |
| `version` | `v1` | true |  |
| `definition` | [OrderByExpressionV1](#orderbyexpression-orderbyexpressionv1) | true | Definition of a type representing an order by expression on an OpenDD type. |

 **Examples:**

```yaml
kind: OrderByExpression
version: v1
definition:
  name: Album_order_by_exp
  operand:
    object:
      orderedType: Album
      orderableFields:
        - fieldName: AlbumId
          orderByExpression: Int_order_by_exp
        - fieldName: ArtistId
          orderByExpression: Int_order_by_exp
        - fieldName: Address
          orderByExpression: Address_order_by_default_exp
      orderableRelationships:
        - relationshipName: artist
          orderByExpression: Artist_order_by_default_exp
  graphql:
    expressionTypeName: App_Album_order_by_exp
  description: Order by expression for Albums
```



```yaml
kind: OrderByExpression
version: v1
definition:
  name: Int_order_by_exp
  operand:
    scalar:
      orderedType: Int
      enableOrderByDirections:
        enableAll: true
  graphql:
    expressionTypeName: App_Int_order_by_exp
  description: Order by expression for Int
```


#### OrderByExpressionV1 {#orderbyexpression-orderbyexpressionv1}

Definition of a type representing an order by expression on an OpenDD type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [OrderByExpressionName](#orderbyexpression-orderbyexpressionname) | true | The name used to refer to this expression. This name is unique only in the context of the `orderedType` |
| `operand` | [OrderByExpressionOperand](#orderbyexpression-orderbyexpressionoperand) | true | The type that this expression applies to. |
| `graphql` | [OrderByExpressionGraphQlConfiguration](#orderbyexpression-orderbyexpressiongraphqlconfiguration) / null | false | Configuration for how this order by expression should appear in the GraphQL schema. |
| `description` | string / null | false | The description of the order by expression. |



#### OrderByExpressionGraphQlConfiguration {#orderbyexpression-orderbyexpressiongraphqlconfiguration}

GraphQL configuration settings for a type representing an order by expression on an OpenDD type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `expressionTypeName` | [GraphQlTypeName](#orderbyexpression-graphqltypename) | true | The name of a GraphQL type. |



#### GraphQlTypeName {#orderbyexpression-graphqltypename}

The name of a GraphQL type.


**Value:** string


#### OrderByExpressionOperand {#orderbyexpression-orderbyexpressionoperand}

Configuration for object or scalar order by expression


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `object` | [OrderByExpressionObjectOperand](#orderbyexpression-orderbyexpressionobjectoperand) | false | Definition of an type representing an order by expression on an OpenDD object type. |
| `scalar` | [OrderByExpressionScalarOperand](#orderbyexpression-orderbyexpressionscalaroperand) | false | Definition of a type representing an order by expression on an OpenDD scalar type. |



#### OrderByExpressionScalarOperand {#orderbyexpression-orderbyexpressionscalaroperand}

Definition of a type representing an order by expression on an OpenDD scalar type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `orderedType` | [TypeName](#orderbyexpression-typename) | true | The type that this expression applies to. |
| `enableOrderByDirections` | [EnableAllOrSpecific](#orderbyexpression-enableallorspecific) | true | Order by directions supported by this scalar type. |



#### EnableAllOrSpecific {#orderbyexpression-enableallorspecific}

Enable all or specific values.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `enableAll` | boolean | false |  |
| `enableSpecific` | [[OrderByDirection](#orderbyexpression-orderbydirection)] | false |  |



#### OrderByDirection {#orderbyexpression-orderbydirection}


**Value:** `Asc` / `Desc`


#### TypeName {#orderbyexpression-typename}

The type that this expression applies to.


**One of the following values:**

| Value | Description |
|-----|-----|
| [InbuiltType](#orderbyexpression-inbuilttype) | An inbuilt primitive OpenDD type. |
| [CustomTypeName](#orderbyexpression-customtypename) |  |



#### InbuiltType {#orderbyexpression-inbuilttype}

An inbuilt primitive OpenDD type.


**Value:** `ID` / `Int` / `Float` / `Boolean` / `String`


#### OrderByExpressionObjectOperand {#orderbyexpression-orderbyexpressionobjectoperand}

Definition of an type representing an order by expression on an OpenDD object type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `orderedType` | [CustomTypeName](#orderbyexpression-customtypename) | true | The type that this expression applies to. |
| `orderableFields` | [[OrderByExpressionOrderableField](#orderbyexpression-orderbyexpressionorderablefield)] | true | Orderable fields of the `orderedType` |
| `orderableRelationships` | [[OrderByExpressionOrderableRelationship](#orderbyexpression-orderbyexpressionorderablerelationship)] | true | Orderable relationships |



#### OrderByExpressionOrderableRelationship {#orderbyexpression-orderbyexpressionorderablerelationship}

Definition of a relationship in a type representing an order by expression on an OpenDD type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `relationshipName` | [RelationshipName](#orderbyexpression-relationshipname) | true | The name of the relationship. |
| `orderByExpression` | [OrderByExpressionName](#orderbyexpression-orderbyexpressionname) / null | false | The OrderByExpression to use for this relationship. This is optional for model relationships. If not specified we use the model's OrderByExpression configuration. For local command relationships this is required. |



#### RelationshipName {#orderbyexpression-relationshipname}

The name of the GraphQL relationship field.


**Value:** string


#### OrderByExpressionOrderableField {#orderbyexpression-orderbyexpressionorderablefield}

Definition of a field in a type representing an order by expression on an OpenDD type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldName` | [FieldName](#orderbyexpression-fieldname) | true |  |
| `orderByExpression` | [OrderByExpressionName](#orderbyexpression-orderbyexpressionname) | true | OrderByExpression to use for this field. |



#### FieldName {#orderbyexpression-fieldname}

The name of a field in a user-defined object type.


**Value:** string


#### CustomTypeName {#orderbyexpression-customtypename}

The name of a user-defined type.


**Value:** string


#### OrderByExpressionName {#orderbyexpression-orderbyexpressionname}

The name of an order by expression.


**Value:** string



==============================



# relationships.mdx

URL: https://hasura.io/docs/promptql/reference/metadata-reference/relationships

# Relationships

## Introduction

Defining a relationship allows you to make queries across linked information.

Relationships are defined in metadata **from an [object type](./types.mdx#objecttype-objecttype), to a
[model](./models.mdx) or [command](./commands.mdx)**.

This enables you to create complex queries, where you can fetch related data in a single query.

By defining a `Relationship` object, **all [models](./models.mdx) or [commands](./commands.mdx) whose output type is the
source object type defined in the relationship will now have a connection to the target model or command.**

So from this relationship definition, all models or commands in the supergraph whose output type is the `orders` object
type will now have a connection to the `customers` model via the `customer` relationship field on the `orders` type.

So, as in example 1 below, you can now fetch the customer when you query orders. Note that also, if you had, for
instance an arbitrary command such as `getCustomerLastOrder` which also returned an order from a customer id, you can
now also return the customer's details for that returned order in the same single query from this relationship.

## How relationships work

### Lifecycle

You can automatically add foreign-key relationships to your metadata
[using the CLI](/reference/cli/commands/ddn_relationship_add.mdx).

Additionally, you can manually define relationships from an object type to a model or command in your metadata using the
VS Code extension. The extension knows about your data sources and can help you write relationships more efficiently.

Simply start by typing a delimiter (`---`) at the end of a model's metadata file and start typing `Relationship`. The
extension will provide you with auto-completion, syntax highlighting, and error checking as you write your
relationships. At any point in the process, you can type `Ctrl + Space` to see the available options and tab through the
fields to fill in the required information.

To make a relationship available in your supergraph, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

### Examples

```yaml
---
kind: Relationship
version: v1
definition:
  name: customer
  sourceType: orders
  target:
    model:
      name: customers
      subgraph: customers
      relationshipType: Object
  mapping:
    - source:
        fieldPath:
          - fieldName: customerId
      target:
        modelField:
          - fieldName: customerId
```

| **Field**                                    | **Description**                                                                                                        | **Reference**                                                    |
| -------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- |
| **kind**                                     | Indicates that this object is defining a relationship between two data types or models.                                | [Relationship](#relationship-relationship)                       |
| **version**                                  | Specifies the version of the relationship's structure.                                                                 | [RelationshipV1](#relationship-relationshipv1)                   |
| **definition.name**                          | The name of the relationship, representing the link between the source and target.                                     | [RelationshipName](#relationship-relationshipname)               |
| **definition.sourceType**                    | Defines the source object type that the relationship starts from.                                                      | [CustomTypeName](#relationship-customtypename)                   |
| **definition.target.model.name**             | The name of the target model to which the source is related.                                                           | [ModelName](#relationship-modelname)                             |
| **definition.target.model.subgraph**         | Specifies the subgraph in which the target model resides.                                                              | [ModelRelationshipTarget](#relationship-modelrelationshiptarget) |
| **definition.target.model.relationshipType** | Indicates the type of relationship (Object or Array), determining whether one or many related items can be fetched.    | [RelationshipType](#relationship-relationshiptype)               |
| **definition.mapping**                       | Defines how fields from the source map to fields or arguments in the target, establishing the connection between them. | [RelationshipMapping](#relationship-relationshipmapping)         |

#### Object Type to a Model

As mentioned above, in an e-commerce context, you will likely have customers and orders, and you want to relate them so
that when you query orders you can easily fetch the customer on that order.

To do this in DDN metadata for this example, you might have an `orders` **model** which returns an `orders` **object
type** and you want to relate that to a `customers` **model** and whatever object type it returns.

**Example:** Fetch a list of orders along with the customer details for each order:

```graphql
query OrdersAndCustomers {
  orders {
    orderId
    orderDate
    customer {
      customerId
      name
      email
    }
  }
}
```

```json
{
  "data": {
    "orders": [
      {
        "orderId": "ORD001",
        "orderDate": "2024-05-10",
        "customer": {
          "customerId": "CUST001",
          "name": "John Doe",
          "email": "john.doe@example.com"
        }
      },
      {
        "orderId": "ORD002",
        "orderDate": "2024-05-11",
        "customer": {
          "customerId": "CUST002",
          "name": "Jane Smith",
          "email": "jane.smith@example.com"
        }
      }
    ]
  }
}
```

Here is the corresponding relationship configuration which enables this query:

```yaml
kind: Relationship
version: v1
definition:
  name: customer
  sourceType: orders
  target:
    model:
      name: customers
      subgraph: customers
      relationshipType: Object
  mapping:
    - source:
        fieldPath:
          - fieldName: customerId
      target:
        modelField:
          - fieldName: customerId
  description: The customer details for an order
```

Now, you can also retrieve the customer details for any model or command in your schema that returns the `orders` object
type.

For convenience, this relationship configuration would best be located with your `orders` object type and model.

#### Object Type to a Command

Let's say you have a `user` object type in your graph, and also a command which responds with the current logged-in user
information (say `getLoggedInUserInfo`). Now you can link these two, by defining a relationship from the `user` object
type to `getLoggedInUserInfo`.

Let's say we name the relationship `currentSession`. Now you can make a single query from your client to get a user's
data and their current session information.

**Example:** fetch a list of users and the current session information of each user:

```graphql
query UsersAndCurrentSession {
  users {
    id
    username
    currentSession {
      activeSince
    }
  }
}
```

```json
{
  "data": {
    "users": [
      {
        "id": 1,
        "username": "sit_amet",
        "currentSession": {
          "activeSince": "2024-04-01T07:08:22+0000"
        }
      },
      {
        "id": 2,
        "username": "fancy_nibh",
        "currentSession": {
          "activeSince": "2024-04-01T07:08:22+0000"
        }
      },
      {
        "id": 3,
        "username": "just_joe",
        "currentSession": {
          "activeSince": "2024-04-01T07:08:22+0000"
        }
      }
    ]
  }
}
```

Here is the corresponding relationship configuration which enables this query:

```yaml
kind: Relationship
version: v1
definition:
  name: currentSession
  sourceType: user
  target:
    command:
      name: getLoggedInUserInfo
      subgraph: users
  mapping:
    - source:
        fieldPath:
          - fieldName: id
      target:
        argument:
          argumentName: user_id
  description: The current session information for the user
```

#### Command to Command

Hasura DDN also allows you to link commands together from a source type to query related data.

**Example:** fetch the result of one command and use it as input for another command:

```graphql
query TrackOrder {
  trackOrder(orderId: "ORD12345") {
    trackingNumber
    shippingDetails {
      carrier
      estimatedDeliveryDate
      currentStatus
    }
  }
}
```

```json
{
  "data": {
    "trackOrder": {
      "trackingNumber": "1Z9999999999999999",
      "shippingDetails": {
        "carrier": "UPS",
        "estimatedDeliveryDate": "2024-05-25",
        "currentStatus": "In Transit"
      }
    }
  }
}
```

And the corresponding relationship configuration which enables this query:

```yaml
kind: Relationship
version: v1
definition:
  name: shippingDetails
  sourceType: trackOrder
  target:
    command:
      name: getShippingDetails
      subgraph: orders
  mapping:
    - source:
        fieldPath:
          - fieldName: trackingNumber
      target:
        argument:
          argumentName: trackingNumber
  description: The shipping details for an order based on its tracking number
```

:::info VS Code extension assisted authoring

Relationships are written in your various HML files. If you're using a compatible editor (such as VS Code with the
[Hasura VS Code extension](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura)), assisted authoring
will help you create relationships more efficiently. It will provide you with auto-completion, syntax highlighting, and
error checking as you write your relationships.

The CLI also works to automatically track your relationships for you whenever you add or update a data connector.

:::

---

## Metadata structure


### Relationship {#relationship-relationship}

Definition of a relationship on an OpenDD type which allows it to be extended with related models or commands.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `Relationship` | true |  |
| `version` | `v1` | true |  |
| `definition` | [RelationshipV1](#relationship-relationshipv1) | true | Definition of a relationship on an OpenDD type which allows it to be extended with related models or commands. |

 **Example:**

```yaml
kind: Relationship
version: v1
definition:
  name: Articles
  sourceType: author
  target:
    model:
      name: Articles
      subgraph: null
      relationshipType: Array
  mapping:
    - source:
        fieldPath:
          - fieldName: author_id
      target:
        modelField:
          - fieldName: author_id
  description: Articles written by an author
```


#### RelationshipV1 {#relationship-relationshipv1}

Definition of a relationship on an OpenDD type which allows it to be extended with related models or commands.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [RelationshipName](#relationship-relationshipname) | true | The name of the relationship. |
| `sourceType` | [CustomTypeName](#relationship-customtypename) | true | The source type of the relationship. |
| `target` | [RelationshipTarget](#relationship-relationshiptarget) | true | The target of the relationship. |
| `mapping` | [[RelationshipMapping](#relationship-relationshipmapping)] | true | The mapping configuration of source to target for the relationship. |
| `description` | string / null | false | The description of the relationship. Gets added to the description of the relationship in the graphql schema. |
| `deprecated` | [Deprecated](#relationship-deprecated) / null | false | Whether this relationship is deprecated. If set, the deprecation status is added to the relationship field's graphql schema. |
| `graphql` | [RelationshipGraphQlDefinition](#relationship-relationshipgraphqldefinition) / null | false | Configuration for how this relationship should appear in the GraphQL schema. |



#### RelationshipGraphQlDefinition {#relationship-relationshipgraphqldefinition}

The definition of how a relationship appears in the GraphQL API

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `aggregateFieldName` | [FieldName](#relationship-fieldname) / null | false | The field name to use for the field that represents an aggregate over the relationship |



#### Deprecated {#relationship-deprecated}

OpenDd configuration to indicate whether an object type field, relationship, model root field or command root field is deprecated.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `reason` | string / null | false | The reason for deprecation. |



#### RelationshipMapping {#relationship-relationshipmapping}

Definition of a how a particular field in the source maps to a target field or argument.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `source` | [RelationshipMappingSource](#relationship-relationshipmappingsource) | true | The source configuration for this relationship mapping. |
| `target` | [RelationshipMappingTarget](#relationship-relationshipmappingtarget) | true | The target configuration for this relationship mapping. |

 **Example:**

```yaml
source:
  fieldPath:
    - fieldName: author_id
target:
  modelField:
    - fieldName: author_id
```


#### RelationshipMappingTarget {#relationship-relationshipmappingtarget}

The target configuration for a relationship mapping.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `argument` | [ArgumentMappingTarget](#relationship-argumentmappingtarget) | false | An argument target for a relationship mapping. |
| `modelField` | [[RelationshipSourceFieldAccess](#relationship-relationshipsourcefieldaccess)] | false |  |



#### ArgumentMappingTarget {#relationship-argumentmappingtarget}

An argument target for a relationship mapping.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `argumentName` | [ArgumentName](#relationship-argumentname) | true |  |



#### ArgumentName {#relationship-argumentname}

The name of an argument.


**Value:** string


#### RelationshipMappingSource {#relationship-relationshipmappingsource}

The source configuration for a relationship mapping.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `value` | [ValueExpression](#relationship-valueexpression) | false | An expression which evaluates to a value that can be used in permissions and various presets. |
| `fieldPath` | [[RelationshipSourceFieldAccess](#relationship-relationshipsourcefieldaccess)] | false |  |



#### RelationshipSourceFieldAccess {#relationship-relationshipsourcefieldaccess}

A field access in a relationship mapping.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldName` | [FieldName](#relationship-fieldname) | true |  |



#### FieldName {#relationship-fieldname}

The name of a field in a user-defined object type.


**Value:** string


#### ValueExpression {#relationship-valueexpression}

An expression which evaluates to a value that can be used in permissions and various presets.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `literal` |  | false |  |
| `sessionVariable` | [OpenDdSessionVariable](#relationship-openddsessionvariable) | false |  |
| `valueFromEnv` | string | false |  |



#### OpenDdSessionVariable {#relationship-openddsessionvariable}

Used to represent the name of a session variable, like "x-hasura-role".


**Value:** string


#### RelationshipTarget {#relationship-relationshiptarget}

The target for a relationship.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `model` | [ModelRelationshipTarget](#relationship-modelrelationshiptarget) | false | The target model for a relationship. |
| `command` | [CommandRelationshipTarget](#relationship-commandrelationshiptarget) | false | The target command for a relationship. |

 **Example:**

```yaml
model:
  name: Articles
  subgraph: null
  relationshipType: Array
```


#### CommandRelationshipTarget {#relationship-commandrelationshiptarget}

The target command for a relationship.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [CommandName](#relationship-commandname) | true | The name of the command. |
| `subgraph` | string / null | false | The subgraph of the target command. Defaults to the current subgraph. |



#### CommandName {#relationship-commandname}

The name of a command.


**Value:** string


#### ModelRelationshipTarget {#relationship-modelrelationshiptarget}

The target model for a relationship.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [ModelName](#relationship-modelname) | true | The name of the data model. |
| `subgraph` | string / null | false | The subgraph of the target model. Defaults to the current subgraph. |
| `relationshipType` | [RelationshipType](#relationship-relationshiptype) | true | Type of the relationship - object or array. |
| `aggregate` | [ModelRelationshipTargetAggregate](#relationship-modelrelationshiptargetaggregate) / null | false | How to aggregate over the relationship. Only valid for array relationships |



#### ModelRelationshipTargetAggregate {#relationship-modelrelationshiptargetaggregate}

Which aggregate expression to use to aggregate the array relationship.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `aggregateExpression` | [AggregateExpressionName](#relationship-aggregateexpressionname) | true | The name of the aggregate expression that defines how to aggregate across the relationship. |
| `description` | string / null | false | The description of the relationship aggregate. Gets added to the description of the aggregate field in the GraphQL schema |



#### AggregateExpressionName {#relationship-aggregateexpressionname}

The name of an aggregate expression.


**Value:** string


#### RelationshipType {#relationship-relationshiptype}

Type of the relationship.


**One of the following values:**

| Value | Description |
|-----|-----|
| `Object` | Select one related object from the target. |
| `Array` | Select multiple related objects from the target. |



#### ModelName {#relationship-modelname}

The name of data model.


**Value:** string


#### CustomTypeName {#relationship-customtypename}

The name of a user-defined type.


**Value:** string


#### RelationshipName {#relationship-relationshipname}

The name of the GraphQL relationship field.


**Value:** string



==============================



# permissions.mdx

URL: https://hasura.io/docs/promptql/reference/metadata-reference/permissions


# Permissions

## Introduction

Access control rules are essential for securing your data and ensuring that only authorized users can access it. In
Hasura, these are referred to as **permissions**. You can use permissions to control access to certain rows or columns
in your database, or to restrict access to certain operations or fields in your GraphQL API.

The following types of permissions can be defined:

| Type                                                         | Description                                                                       |
| ------------------------------------------------------------ | --------------------------------------------------------------------------------- |
| [TypePermissions](#typepermissions)                          | Define which fields are allowed to be accessed by a role on an output type.       |
| [ModelPermissions](#modelpermissions)                        | Define which objects or rows within a model are allowed to be accessed by a role. |
| [CommandPermissions](#commandpermissions-commandpermissions) | Define whether the command is executable by a role.                               |

## How TypePermissions work {#typepermissions}

### Lifecycle

By default, whenever a new type is created in your supergraph, each field is only accessible to the `admin` role. You
can think of these as permissions on columns in a database table.

You can quickly generate new roles by adding a new item to the `permissions` array in the TypePermissions object. Each
item in the array should have a `role` field and an `output` field. The `output` field should contain an `allowedFields`
array, which lists the fields that are accessible to the role when the type is used in an output context.

To make a new TypePermission or role available in your supergraph, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

### Examples

```yaml
---
kind: TypePermissions
version: v1
definition:
  typeName: article
  permissions:
    - role: admin
      output:
        allowedFields:
          - article_id
          - author_id
          - title
    - role: user
      output:
        allowedFields:
          - article_id
          - author_id
```

| **Field**                                         | **Description**                                                                                   | **Reference**                                                 |
| ------------------------------------------------- | ------------------------------------------------------------------------------------------------- | ------------------------------------------------------------- |
| **kind**                                          | Indicates that this object is defining permissions for a specific type.                           | [TypePermissions](#typepermissions-typepermissions)           |
| **version**                                       | Specifies the version of the type permissions structure.                                          | [TypePermissionsV1](#typepermissions-typepermissionsv1)       |
| **definition.typeName**                           | The name of the type for which permissions are being defined.                                     | [CustomTypeName](#typepermissions-customtypename)             |
| **definition.permissions**                        | A list of permissions specifying which fields are accessible for different roles.                 | [TypePermission](#typepermissions-typepermission)             |
| **definition.permissions[].role**                 | The role for which permissions are being defined.                                                 | [Role](#typepermissions-role)                                 |
| **definition.permissions[].output.allowedFields** | The fields of the type that are accessible for a role when the type is used in an output context. | [TypeOutputPermission](#typepermissions-typeoutputpermission) |

## How ModelPermissions work {#modelpermissions}

### Lifecycle

By default, whenever a new model is created in your supergraph, all records are only accessible to the `admin` role. You
can think of these as permissions on rows in a database table.

You can restrict access to certain rows by adding a new item to the `permissions` array in the `ModelPermissions`
object. Each item in the array should have a `role` field and a `select` field. The `select` field should contain a
`filter` expression that determines which rows are accessible to the role when selecting from the model.

Most commonly, you'll use session variables — provided via your configured
[authentication mechanism](/auth/overview.mdx) — to restrict access to rows based on the user's identity or other
criteria. You can also use argument presets to enforce row-level security. You can see an example of this below.

To make a new ModelPermission or role available in your supergraph, after updating your metadata, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

### Examples

```yaml
---
kind: ModelPermissions
version: v1
definition:
  modelName: Articles
  permissions:
    - role: admin
      select:
        filter: null
    - role: user
      select:
        filter:
          fieldComparison:
            field: author_id
            operator: _eq
            value:
              sessionVariable: x-hasura-user-id
```

| **Field**                                  | **Description**                                                                                            | **Reference**                                              |
| ------------------------------------------ | ---------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- |
| **kind**                                   | Indicates that this object is defining permissions for a specific model.                                   | [ModelPermissions](#modelpermissions-modelpermissions)     |
| **version**                                | Specifies the version of the model permissions structure.                                                  | [ModelPermissionsV1](#modelpermissions-modelpermissionsv1) |
| **definition.modelName**                   | The name of the model for which permissions are being defined.                                             | [ModelName](#modelpermissions-modelname)                   |
| **definition.permissions**                 | A list of permissions specifying which rows or objects are accessible for different roles.                 | [ModelPermission](#modelpermissions-modelpermission)       |
| **definition.permissions[].role**          | The role for which permissions are being defined.                                                          | [Role](#modelpermissions-role)                             |
| **definition.permissions[].select.filter** | A filter expression that determines which rows are accessible for this role when selecting from the model. | [ModelPredicate](#modelpermissions-modelpredicate)         |

## How CommandPermissions work {#commandpermissions}

### Lifecycle

By default, whenever a new command is created in your supergraph, it is only executable by the `admin` role.

You can enable or restrict access to commands by adding a new item to the `permissions` array in the
`CommandPermissions` object. Each item in the array should have a `role` field and an `allowExecution` field. The
`allowExecution` field should be set to `true` if the command is executable by the role.

Further, you can use argument presets to pass actual logical expressions to your data sources to control how they do
things.

For example, a data connector might expose a `Command` called `delete_user_by_id` with two arguments - `user_id` and
`pre_check`. `user_id` is the primary key of the user you'd like to remove, and `pre_check` lets you provide a custom
boolean expression.

```yaml
kind: CommandPermissions
version: v1
definition:
  commandName: delete_user_by_id
  permissions:
    - role: admin
      allowExecution: true
    - role: user
      allowExecution: true
      argumentPresets:
        - argument: pre_check
          value:
            booleanExpression:
              fieldComparison:
                field: is_invincible
                operator: _eq
                value:
                  literal: false
```

Now, when `admin` runs this command, once again, they can do what they want, and provide their own `pre_check` if they
want. The `user` however, is able to pass a `user_id` argument, but the `pre_check` expression is passed to the data
connector which will only let them delete the row if the row's `is_invincible` value is set to `false`.

To make a execution of a command available to a role in your supergraph, after updating your metadata, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

### Examples

```yaml
---
kind: CommandPermissions
version: v1
definition:
  commandName: get_article_by_id
  permissions:
    - role: admin
      allowExecution: true
    - role: user
      allowExecution: true
      argumentPresets:
        - argument: id
          value:
            literal: 100
```

| **Field**                                    | **Description**                                                                         | **Reference**                                                    |
| -------------------------------------------- | --------------------------------------------------------------------------------------- | ---------------------------------------------------------------- |
| **kind**                                     | Indicates that this object is defining permissions for a specific command.              | [CommandPermissions](#commandpermissions-commandpermissions)     |
| **version**                                  | Specifies the version of the command permissions structure.                             | [CommandPermissionsV1](#commandpermissions-commandpermissionsv1) |
| **definition.commandName**                   | The name of the command for which permissions are being defined.                        | [CommandName](#commandpermissions-commandname)                   |
| **definition.permissions**                   | A list of permissions specifying whether the command is executable by different roles.  | [CommandPermission](#commandpermissions-commandpermission)       |
| **definition.permissions[].role**            | The role for which permissions are being defined.                                       | [Role](#commandpermissions-role)                                 |
| **definition.permissions[].allowExecution**  | Indicates whether the command is executable by the role.                                | [CommandPermission](#commandpermissions-commandpermission)       |
| **definition.permissions[].argumentPresets** | Preset values for arguments that are enforced when the command is executed by the role. | [ArgumentPreset](#commandpermissions-argumentpreset)             |

---

## Metadata structure


### TypePermissions {#typepermissions-typepermissions}

Definition of permissions for an OpenDD type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `TypePermissions` | true |  |
| `version` | `v1` | true |  |
| `definition` | [TypePermissionsV1](#typepermissions-typepermissionsv1) | true | Definition of permissions for an OpenDD type. |

 **Example:**

```yaml
kind: TypePermissions
version: v1
definition:
  typeName: article
  permissions:
    - role: admin
      output:
        allowedFields:
          - article_id
          - author_id
          - title
    - role: user
      output:
        allowedFields:
          - article_id
          - author_id
```


#### TypePermissionsV1 {#typepermissions-typepermissionsv1}

Definition of permissions for an OpenDD type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `typeName` | [CustomTypeName](#typepermissions-customtypename) | true | The name of the type for which permissions are being defined. Must be an object type. |
| `permissions` | [[TypePermission](#typepermissions-typepermission)] | true | A list of type permissions, one for each role. |



#### TypePermission {#typepermissions-typepermission}

Defines permissions for a particular role for a type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `role` | [Role](#typepermissions-role) | true | The role for which permissions are being defined. |
| `output` | [TypeOutputPermission](#typepermissions-typeoutputpermission) / null | false | Permissions for this role when this type is used in an output context. If null, this type is inaccessible for this role in an output context. |
| `input` | [TypeInputPermission](#typepermissions-typeinputpermission) / null | false | Permissions for this role when this type is used in an input context. If null, this type is accessible for this role in an input context. |

 **Example:**

```yaml
role: user
output:
  allowedFields:
    - article_id
    - author_id
input:
  fieldPresets:
    - field: author_id
      value:
        sessionVariable: x-hasura-user-id
```


#### TypeInputPermission {#typepermissions-typeinputpermission}

Permissions for a type for a particular role when used in an input context.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldPresets` | [[FieldPreset](#typepermissions-fieldpreset)] | false | Preset values for fields of the type |



#### FieldPreset {#typepermissions-fieldpreset}

Preset value for a field

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `field` | [FieldName](#typepermissions-fieldname) | true | Field name for preset |
| `value` | [ValueExpression](#typepermissions-valueexpression) | true | Value for preset |



#### ValueExpression {#typepermissions-valueexpression}

An expression which evaluates to a value that can be used in permissions and various presets.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `literal` |  | false |  |
| `sessionVariable` | [OpenDdSessionVariable](#typepermissions-openddsessionvariable) | false |  |
| `valueFromEnv` | string | false |  |



#### OpenDdSessionVariable {#typepermissions-openddsessionvariable}

Used to represent the name of a session variable, like "x-hasura-role".


**Value:** string


#### TypeOutputPermission {#typepermissions-typeoutputpermission}

Permissions for a type for a particular role when used in an output context.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `allowedFields` | [[FieldName](#typepermissions-fieldname)] | true | Fields of the type that are accessible for a role |



#### FieldName {#typepermissions-fieldname}

The name of a field in a user-defined object type.


**Value:** string


#### Role {#typepermissions-role}

The role for which permissions are being defined.


**Value:** string


#### CustomTypeName {#typepermissions-customtypename}

The name of a user-defined type.


**Value:** string
### ModelPermissions {#modelpermissions-modelpermissions}

Definition of permissions for an OpenDD model.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `ModelPermissions` | true |  |
| `version` | `v1` | true |  |
| `definition` | [ModelPermissionsV1](#modelpermissions-modelpermissionsv1) | true | Definition of permissions for an OpenDD model. |

 **Examples:**

```yaml
kind: ModelPermissions
version: v1
definition:
  modelName: Articles
  permissions:
    - role: admin
      select:
        filter: null
    - role: user
      select:
        filter:
          fieldComparison:
            field: author_id
            operator: _eq
            value:
              sessionVariable: x-hasura-user-id
```



```yaml
kind: ModelPermissions
version: v1
definition:
  modelName: Articles
  permissions:
    - role: admin
      select:
        filter: null
    - role: user
      select:
        filter:
          relationship:
            name: author
            predicate:
              fieldComparison:
                field: id
                operator: _eq
                value:
                  sessionVariable: x-hasura-user-id
```


#### ModelPermissionsV1 {#modelpermissions-modelpermissionsv1}

Definition of permissions for an OpenDD model.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `modelName` | [ModelName](#modelpermissions-modelname) | true | The name of the model for which permissions are being defined. |
| `permissions` | [[ModelPermission](#modelpermissions-modelpermission)] | true | A list of model permissions, one for each role. |



#### ModelPermission {#modelpermissions-modelpermission}

Defines the permissions for an OpenDD model.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `role` | [Role](#modelpermissions-role) | true | The role for which permissions are being defined. |
| `select` | [SelectPermission](#modelpermissions-selectpermission) / null | false | The permissions for selecting from this model for this role. If this is null, the role is not allowed to query the model. |

 **Example:**

```yaml
role: user
select:
  filter:
    fieldComparison:
      field: author_id
      operator: _eq
      value:
        sessionVariable: x-hasura-user-id
  argument_presets:
    - field: likes_dogs
      value:
        literal: true
```


#### SelectPermission {#modelpermissions-selectpermission}

Defines the permissions for selecting a model for a role.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `filter` | null / [ModelPredicate](#modelpermissions-modelpredicate) | true | Filter expression when selecting rows for this model. Null filter implies all rows are selectable. |
| `argumentPresets` | [[ArgumentPreset](#modelpermissions-argumentpreset)] | false | Preset values for arguments for this role |
| `allowSubscriptions` | boolean | false | Whether the role is allowed to subscribe to the root fields of this model. |



#### ArgumentPreset {#modelpermissions-argumentpreset}

Preset value for an argument

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `argument` | [ArgumentName](#modelpermissions-argumentname) | true | Argument name for preset |
| `value` | [ValueExpressionOrPredicate](#modelpermissions-valueexpressionorpredicate) | true | Value for preset |



#### ValueExpressionOrPredicate {#modelpermissions-valueexpressionorpredicate}

An expression which evaluates to a value that can be used in permissions and various presets.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `literal` |  | false |  |
| `sessionVariable` | [OpenDdSessionVariable](#modelpermissions-openddsessionvariable) | false | Used to represent the name of a session variable, like "x-hasura-role". |
| `booleanExpression` | [ModelPredicate](#modelpermissions-modelpredicate) | false |  |
| `valueFromEnv` | string | false |  |



#### ArgumentName {#modelpermissions-argumentname}

The name of an argument.


**Value:** string


#### ModelPredicate {#modelpermissions-modelpredicate}

A predicate that can be used to restrict the objects returned when querying a model.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldComparison` | [FieldComparisonPredicate](#modelpermissions-fieldcomparisonpredicate) | false | Field comparison predicate filters objects based on a field value. |
| `fieldIsNull` | [FieldIsNullPredicate](#modelpermissions-fieldisnullpredicate) | false | Predicate to check if the given field is null. |
| `nestedField` | [NestedFieldPredicate](#modelpermissions-nestedfieldpredicate) | false | Nested field predicate filters objects of a source model based on a predicate on the nested field. |
| `relationship` | [RelationshipPredicate](#modelpermissions-relationshippredicate) | false | Relationship predicate filters objects of a source model based on a predicate on the related model. |
| `and` | [[ModelPredicate](#modelpermissions-modelpredicate)] | false |  |
| `or` | [[ModelPredicate](#modelpermissions-modelpredicate)] | false |  |
| `not` | [ModelPredicate](#modelpermissions-modelpredicate) | false |  |

 **Examples:**

```yaml
fieldComparison:
  field: author_id
  operator: _eq
  value:
    sessionVariable: x-hasura-user-id
```



```yaml
relationship:
  name: author
  predicate:
    fieldComparison:
      field: id
      operator: _eq
      value:
        sessionVariable: x-hasura-user-id
```



```yaml
and:
  - fieldComparison:
      field: author_id
      operator: _eq
      value:
        sessionVariable: x-hasura-user-id
  - fieldComparison:
      field: title
      operator: _eq
      value:
        literal: Hello World
```



```yaml
not:
  fieldComparison:
    field: author_id
    operator: _eq
    value:
      sessionVariable: x-hasura-user-id
```


#### RelationshipPredicate {#modelpermissions-relationshippredicate}

Relationship predicate filters objects of a source model based on a predicate on the related model.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [RelationshipName](#modelpermissions-relationshipname) | true | The name of the relationship of the object type of the model to follow. |
| `predicate` | [ModelPredicate](#modelpermissions-modelpredicate) / null | false | The predicate to apply on the related objects. If this is null, then the predicate evaluates to true as long as there is at least one related object present. |



#### RelationshipName {#modelpermissions-relationshipname}

The name of the GraphQL relationship field.


**Value:** string


#### NestedFieldPredicate {#modelpermissions-nestedfieldpredicate}

Nested field predicate filters objects of a source model based on a predicate on the nested field.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldName` | [FieldName](#modelpermissions-fieldname) | true | The name of the field in the object type of the model to follow. |
| `predicate` | [ModelPredicate](#modelpermissions-modelpredicate) | true | The predicate to apply on the related objects. |



#### FieldIsNullPredicate {#modelpermissions-fieldisnullpredicate}

Predicate to check if the given field is null.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `field` | [FieldName](#modelpermissions-fieldname) | true | The name of the field that should be checked for a null value. |



#### FieldComparisonPredicate {#modelpermissions-fieldcomparisonpredicate}

Field comparison predicate filters objects based on a field value.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `field` | [FieldName](#modelpermissions-fieldname) | true | The field name of the object type of the model to compare. |
| `operator` | [OperatorName](#modelpermissions-operatorname) | true | The name of the operator to use for comparison. |
| `value` | [ValueExpression](#modelpermissions-valueexpression) | true | The value expression to compare against. |



#### ValueExpression {#modelpermissions-valueexpression}

An expression which evaluates to a value that can be used in permissions and various presets.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `literal` |  | false |  |
| `sessionVariable` | [OpenDdSessionVariable](#modelpermissions-openddsessionvariable) | false |  |
| `valueFromEnv` | string | false |  |



#### OpenDdSessionVariable {#modelpermissions-openddsessionvariable}

Used to represent the name of a session variable, like "x-hasura-role".


**Value:** string


#### OperatorName {#modelpermissions-operatorname}

The name of an operator


**Value:** string


#### FieldName {#modelpermissions-fieldname}

The name of a field in a user-defined object type.


**Value:** string


#### Role {#modelpermissions-role}

The role for which permissions are being defined.


**Value:** string


#### ModelName {#modelpermissions-modelname}

The name of data model.


**Value:** string
### CommandPermissions {#commandpermissions-commandpermissions}

Definition of permissions for an OpenDD command.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `CommandPermissions` | true |  |
| `version` | `v1` | true |  |
| `definition` | [CommandPermissionsV1](#commandpermissions-commandpermissionsv1) | true | Definition of permissions for an OpenDD command. |

 **Example:**

```yaml
kind: CommandPermissions
version: v1
definition:
  commandName: get_article_by_id
  permissions:
    - role: admin
      allowExecution: true
    - role: user
      allowExecution: true
```


#### CommandPermissionsV1 {#commandpermissions-commandpermissionsv1}

Definition of permissions for an OpenDD command.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `commandName` | [CommandName](#commandpermissions-commandname) | true | The name of the command for which permissions are being defined. |
| `permissions` | [[CommandPermission](#commandpermissions-commandpermission)] | true | A list of command permissions, one for each role. |



#### CommandPermission {#commandpermissions-commandpermission}

Defines the permissions for a role for a command.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `role` | [Role](#commandpermissions-role) | true | The role for which permissions are being defined. |
| `allowExecution` | boolean | true | Whether the command is executable by the role. |
| `argumentPresets` | [[ArgumentPreset](#commandpermissions-argumentpreset)] | false | Preset values for arguments for this role |

 **Example:**

```yaml
role: user
allowExecution: true
argumentPresets:
  - argument: user_id
    value:
      session_variable: x-hasura-user_id
```


#### ArgumentPreset {#commandpermissions-argumentpreset}

Preset value for an argument

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `argument` | [ArgumentName](#commandpermissions-argumentname) | true | Argument name for preset |
| `value` | [ValueExpressionOrPredicate](#commandpermissions-valueexpressionorpredicate) | true | Value for preset |



#### ValueExpressionOrPredicate {#commandpermissions-valueexpressionorpredicate}

An expression which evaluates to a value that can be used in permissions and various presets.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `literal` |  | false |  |
| `sessionVariable` | [OpenDdSessionVariable](#commandpermissions-openddsessionvariable) | false | Used to represent the name of a session variable, like "x-hasura-role". |
| `booleanExpression` | [ModelPredicate](#commandpermissions-modelpredicate) | false |  |
| `valueFromEnv` | string | false |  |



#### ModelPredicate {#commandpermissions-modelpredicate}

A predicate that can be used to restrict the objects returned when querying a model.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldComparison` | [FieldComparisonPredicate](#commandpermissions-fieldcomparisonpredicate) | false | Field comparison predicate filters objects based on a field value. |
| `fieldIsNull` | [FieldIsNullPredicate](#commandpermissions-fieldisnullpredicate) | false | Predicate to check if the given field is null. |
| `nestedField` | [NestedFieldPredicate](#commandpermissions-nestedfieldpredicate) | false | Nested field predicate filters objects of a source model based on a predicate on the nested field. |
| `relationship` | [RelationshipPredicate](#commandpermissions-relationshippredicate) | false | Relationship predicate filters objects of a source model based on a predicate on the related model. |
| `and` | [[ModelPredicate](#commandpermissions-modelpredicate)] | false |  |
| `or` | [[ModelPredicate](#commandpermissions-modelpredicate)] | false |  |
| `not` | [ModelPredicate](#commandpermissions-modelpredicate) | false |  |

 **Examples:**

```yaml
fieldComparison:
  field: author_id
  operator: _eq
  value:
    sessionVariable: x-hasura-user-id
```



```yaml
relationship:
  name: author
  predicate:
    fieldComparison:
      field: id
      operator: _eq
      value:
        sessionVariable: x-hasura-user-id
```



```yaml
and:
  - fieldComparison:
      field: author_id
      operator: _eq
      value:
        sessionVariable: x-hasura-user-id
  - fieldComparison:
      field: title
      operator: _eq
      value:
        literal: Hello World
```



```yaml
not:
  fieldComparison:
    field: author_id
    operator: _eq
    value:
      sessionVariable: x-hasura-user-id
```


#### RelationshipPredicate {#commandpermissions-relationshippredicate}

Relationship predicate filters objects of a source model based on a predicate on the related model.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [RelationshipName](#commandpermissions-relationshipname) | true | The name of the relationship of the object type of the model to follow. |
| `predicate` | [ModelPredicate](#commandpermissions-modelpredicate) / null | false | The predicate to apply on the related objects. If this is null, then the predicate evaluates to true as long as there is at least one related object present. |



#### RelationshipName {#commandpermissions-relationshipname}

The name of the GraphQL relationship field.


**Value:** string


#### NestedFieldPredicate {#commandpermissions-nestedfieldpredicate}

Nested field predicate filters objects of a source model based on a predicate on the nested field.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldName` | [FieldName](#commandpermissions-fieldname) | true | The name of the field in the object type of the model to follow. |
| `predicate` | [ModelPredicate](#commandpermissions-modelpredicate) | true | The predicate to apply on the related objects. |



#### FieldIsNullPredicate {#commandpermissions-fieldisnullpredicate}

Predicate to check if the given field is null.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `field` | [FieldName](#commandpermissions-fieldname) | true | The name of the field that should be checked for a null value. |



#### FieldComparisonPredicate {#commandpermissions-fieldcomparisonpredicate}

Field comparison predicate filters objects based on a field value.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `field` | [FieldName](#commandpermissions-fieldname) | true | The field name of the object type of the model to compare. |
| `operator` | [OperatorName](#commandpermissions-operatorname) | true | The name of the operator to use for comparison. |
| `value` | [ValueExpression](#commandpermissions-valueexpression) | true | The value expression to compare against. |



#### ValueExpression {#commandpermissions-valueexpression}

An expression which evaluates to a value that can be used in permissions and various presets.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `literal` |  | false |  |
| `sessionVariable` | [OpenDdSessionVariable](#commandpermissions-openddsessionvariable) | false |  |
| `valueFromEnv` | string | false |  |



#### OperatorName {#commandpermissions-operatorname}

The name of an operator


**Value:** string


#### FieldName {#commandpermissions-fieldname}

The name of a field in a user-defined object type.


**Value:** string


#### OpenDdSessionVariable {#commandpermissions-openddsessionvariable}

Used to represent the name of a session variable, like "x-hasura-role".


**Value:** string


#### ArgumentName {#commandpermissions-argumentname}

The name of an argument.


**Value:** string


#### Role {#commandpermissions-role}

The role for which permissions are being defined.


**Value:** string


#### CommandName {#commandpermissions-commandname}

The name of a command.


**Value:** string



==============================



# graphql-config.mdx

URL: https://hasura.io/docs/promptql/reference/metadata-reference/graphql-config

# Configuration for GraphQL API

## Introduction

The `GraphqlConfig` object allows you to configure the GraphQL schema generated by Hasura. This object helps you
configure the common GraphQL schema applicable to all subgraphs within a project.

**The GraphqlConfig object belongs to the supergraph** and can be defined **once** across your supergraph **in any
subgraph** of your choice.

## How GraphqlConfig works

### Lifecycle

By default, all projects are created with a default `GraphqlConfig` object in the **globals** subgraph.

Specifically, it allows you to configure:

1. The root type names for query, mutation and subscription operations.
2. The name for [model](/reference/metadata-reference/models.mdx) selection input field names (`where`, `limit`,
   `offset`, `args`, `order_by`)
3. The enum values for `order_by` directions and the `order_by` enum type name.

An `GraphqlConfig` object is required to be defined in the supergraph metadata. If not defined, any attempted builds
will not be successful.

### Examples

```yaml
kind: GraphqlConfig
version: v1
definition:
  query:
    rootOperationTypeName: Query
    argumentsInput:
      fieldName: args
    limitInput:
      fieldName: limit
    offsetInput:
      fieldName: offset
    filterInput:
      fieldName: where
      operatorNames:
        and: _and
        or: _or
        not: _not
        isNull: _is_null
    orderByInput:
      fieldName: order_by
      enumDirectionValues:
        asc: Asc
        desc: Desc
      enumTypeNames:
        - directions:
            - Asc
            - Desc
          typeName: OrderBy
  mutation:
    rootOperationTypeName: Mutation
  apolloFederation:
    enableRootFields: false
```

| **Field**                                             | **Description**                                                                                | **Reference**                                                                 |
| ----------------------------------------------------- | ---------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------- |
| `kind`                                                | The type of configuration you're defining, in this case, it's for GraphQL API settings.        | [GraphqlConfig](#graphqlconfig-graphqlconfig)                                 |
| `version`                                             | The version of this configuration object.                                                      | [GraphqlConfigV1](#graphqlconfig-graphqlconfigv1)                             |
| `definition.query.rootOperationTypeName`              | The name used for the root query type in your GraphQL schema, usually `Query`.                 | [QueryGraphqlConfig](#graphqlconfig-querygraphqlconfig)                       |
| `definition.query.argumentsInput.fieldName`           | The field name used for passing arguments in queries, usually `args`.                          | [ArgumentsInputGraphqlConfig](#graphqlconfig-argumentsinputgraphqlconfig)     |
| `definition.query.limitInput.fieldName`               | The field name used to limit the number of results, typically `limit`.                         | [LimitInputGraphqlConfig](#graphqlconfig-limitinputgraphqlconfig)             |
| `definition.query.offsetInput.fieldName`              | The field name used to set the starting point for results, typically `offset`.                 | [OffsetInputGraphqlConfig](#graphqlconfig-offsetinputgraphqlconfig)           |
| `definition.query.filterInput.fieldName`              | The field name used for filtering results, usually `where`.                                    | [FilterInputGraphqlConfig](#graphqlconfig-filterinputgraphqlconfig)           |
| `definition.query.filterInput.operatorNames[]`        | The names of the built-in filter operators.                                                    | [FilterInputOperatorNames](#graphqlconfig-filterinputoperatornames)           |
| `definition.query.orderByInput.fieldName`             | The field name used for sorting results, usually `order_by`.                                   | [OrderByInputGraphqlConfig](#graphqlconfig-orderbyinputgraphqlconfig)         |
| `definition.query.orderByInput.enumDirectionValues[]` | The names of the direction parameters, like `asc` and `desc`.                                  | [OrderByDirectionValues](#graphqlconfig-orderbydirectionvalues)               |
| `definition.query.orderByInput.enumTypeNames`         | The name of the enum type used for sorting, like `OrderBy`.                                    | [OrderByEnumTypeName](#graphqlconfig-orderbyenumtypename)                     |
| `definition.mutation.rootOperationTypeName`           | The name used for the root mutation type in your GraphQL schema, usually `Mutation`.           | [MutationGraphqlConfig](#graphqlconfig-mutationgraphqlconfig)                 |
| `definition.apolloFederation.enableRootFields`        | A setting to enable or disable fields needed for Apollo Federation (`_entities`, `_services`). | [GraphqlApolloFederationConfig](#graphqlconfig-graphqlapollofederationconfig) |

---

## Metadata structure


### GraphqlConfig {#graphqlconfig-graphqlconfig}

GraphqlConfig object tells us two things:

1. How the Graphql schema should look like for the features (`where`, `order_by` etc) Hasura provides 2. What features should be enabled/disabled across the subgraphs

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `GraphqlConfig` | true |  |
| `version` | `v1` | true |  |
| `definition` | [GraphqlConfigV1](#graphqlconfig-graphqlconfigv1) | true | GraphqlConfig object tells us two things:  1. How the Graphql schema should look like for the features (`where`, `order_by` etc) Hasura provides 2. What features should be enabled/disabled across the subgraphs |



#### GraphqlConfigV1 {#graphqlconfig-graphqlconfigv1}

GraphqlConfig object tells us two things:

1. How the Graphql schema should look like for the features (`where`, `order_by` etc) Hasura provides 2. What features should be enabled/disabled across the subgraphs

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `query` | [QueryGraphqlConfig](#graphqlconfig-querygraphqlconfig) | true | Configuration for the GraphQL schema of Hasura features for queries. `None` means disable the feature. |
| `mutation` | [MutationGraphqlConfig](#graphqlconfig-mutationgraphqlconfig) | true | Configuration for the GraphQL schema of Hasura features for mutations. |
| `subscription` | [SubscriptionGraphqlConfig](#graphqlconfig-subscriptiongraphqlconfig) / null | false |  |
| `apolloFederation` | [GraphqlApolloFederationConfig](#graphqlconfig-graphqlapollofederationconfig) / null | false |  |



#### GraphqlApolloFederationConfig {#graphqlconfig-graphqlapollofederationconfig}

Configuration for the GraphQL schema of Hasura features for Apollo Federation.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `enableRootFields` | boolean | true | Adds the `_entities` and `_services` root fields required for Apollo Federation. |



#### SubscriptionGraphqlConfig {#graphqlconfig-subscriptiongraphqlconfig}

Configuration for the GraphQL schema of Hasura features for subscriptions.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `rootOperationTypeName` | [GraphQlTypeName](#graphqlconfig-graphqltypename) | true | The name of the root operation type name for subscriptions. Usually `subscription`. |



#### MutationGraphqlConfig {#graphqlconfig-mutationgraphqlconfig}

Configuration for the GraphQL schema of Hasura features for mutations.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `rootOperationTypeName` | [GraphQlTypeName](#graphqlconfig-graphqltypename) | true | The name of the root operation type name for mutations. Usually `mutation`. |



#### QueryGraphqlConfig {#graphqlconfig-querygraphqlconfig}

Configuration for the GraphQL schema of Hasura features for queries. `None` means disable the feature.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `rootOperationTypeName` | [GraphQlTypeName](#graphqlconfig-graphqltypename) | true | The name of the root operation type name for queries. Usually `query`. |
| `argumentsInput` | [ArgumentsInputGraphqlConfig](#graphqlconfig-argumentsinputgraphqlconfig) / null | false | Configuration for the arguments input. |
| `limitInput` | [LimitInputGraphqlConfig](#graphqlconfig-limitinputgraphqlconfig) / null | false | Configuration for the limit operation. |
| `offsetInput` | [OffsetInputGraphqlConfig](#graphqlconfig-offsetinputgraphqlconfig) / null | false | Configuration for the offset operation. |
| `filterInput` | [FilterInputGraphqlConfig](#graphqlconfig-filterinputgraphqlconfig) / null | false | Configuration for the filter operation. |
| `orderByInput` | [OrderByInputGraphqlConfig](#graphqlconfig-orderbyinputgraphqlconfig) / null | false | Configuration for the sort operation. |
| `aggregate` | [AggregateGraphqlConfig](#graphqlconfig-aggregategraphqlconfig) / null | false | Configuration for aggregates |



#### AggregateGraphqlConfig {#graphqlconfig-aggregategraphqlconfig}

Configuration for the GraphQL schema for aggregates.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `filterInputFieldName` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the filter input parameter of aggregate fields and field name in predicates |
| `countFieldName` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the _count field used for the count aggregate function |
| `countDistinctFieldName` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the _count_distinct field used for the count distinct aggregate function |



#### OrderByInputGraphqlConfig {#graphqlconfig-orderbyinputgraphqlconfig}

Configuration for the sort operation.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldName` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the filter operation field. Usually `order_by`. |
| `enumDirectionValues` | [OrderByDirectionValues](#graphqlconfig-orderbydirectionvalues) | true | The names of the direction parameters. |
| `enumTypeNames` | [[OrderByEnumTypeName](#graphqlconfig-orderbyenumtypename)] | true |  |



#### OrderByEnumTypeName {#graphqlconfig-orderbyenumtypename}

Type name for a sort directions enum, with the given set of possible directions.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `directions` | [[OrderByDirection](#graphqlconfig-orderbydirection)] | true |  |
| `typeName` | [GraphQlTypeName](#graphqlconfig-graphqltypename) | true |  |



#### OrderByDirection {#graphqlconfig-orderbydirection}

Sort direction.


**One of the following values:**

| Value | Description |
|-----|-----|
| `Asc` | Ascending. |
| `Desc` | Descending. |



#### OrderByDirectionValues {#graphqlconfig-orderbydirectionvalues}

The names of the direction parameters.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `asc` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the ascending parameter. Usually `Asc`. |
| `desc` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the descending parameter. Usually `Desc`. |



#### FilterInputGraphqlConfig {#graphqlconfig-filterinputgraphqlconfig}

Configuration for the filter operation.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldName` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the filter operation field. Usually `where`. |
| `operatorNames` | [FilterInputOperatorNames](#graphqlconfig-filterinputoperatornames) | true | The names of built-in filter operators. |



#### FilterInputOperatorNames {#graphqlconfig-filterinputoperatornames}

The names of built-in filter operators.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `and` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the `and` operator. Usually `_and`. |
| `or` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the `or` operator. Usually `_or`. |
| `not` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the `not` operator. Usually `_not`. |
| `isNull` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the `is null` operator. Usually `_is_null`. |



#### OffsetInputGraphqlConfig {#graphqlconfig-offsetinputgraphqlconfig}

Configuration for the offset operation.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldName` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the offset operation field. Usually `offset`. |



#### LimitInputGraphqlConfig {#graphqlconfig-limitinputgraphqlconfig}

Configuration for the limit operation.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldName` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the limit operation field. Usually `limit`. |



#### ArgumentsInputGraphqlConfig {#graphqlconfig-argumentsinputgraphqlconfig}

Configuration for the arguments input.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldName` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of arguments passing field. Usually `args`. |



#### GraphQlFieldName {#graphqlconfig-graphqlfieldname}

The name of a GraphQL object field.


**Value:** string


#### GraphQlTypeName {#graphqlconfig-graphqltypename}

The name of a GraphQL type.


**Value:** string



==============================



# auth-config.mdx

URL: https://hasura.io/docs/promptql/reference/metadata-reference/auth-config

# AuthConfig

## Introduction

The `AuthConfig` object is used to define the authentication configuration used by the API. With Hasura, you can utilize
either webhooks or JWTs for authentication. **The AuthConfig object belongs to the supergraph** and can be defined
**once** across your supergraph in **any subgraph** of your choice using one of the following modes:

| Mode                | Description                                                                                                                            |
| ------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |
| [noAuth](#noAuth)   | This is enabled by default and allows you to connect to your API without any authentication. **It is not recommended for production.** |
| [JWT](#jwt)         | This allows you to use JWTs for authentication.                                                                                        |
| [Webhook](#webhook) | This allows you to use a custom webhook for authentication.                                                                            |

You can learn more about Hasura's approach to using existing authentication systems in the
[auth section](/auth/overview.mdx).

## How AuthConfig works

### Lifecycle

By default, all projects are created with an `AuthConfig` object in the **globals** subgraph in a `noAuth` mode that
doesn't use any auth service and doesn't allow setting session variables with API requests. Meaning that your API is, by
default, fully public and exposed if deployed live.

However, you would want update the `AuthConfig` to use a custom webhook or JWT service for authentication to restrict
access to your API and make use of Hasura's powerful [authorization](/auth/overview.mdx) features. The metadata examples
below can help you configure your `AuthConfig` object to use your own custom webhook or JWT service.

An AuthConfig object is required to be defined in the supergraph metadata. If not defined, any attempted builds will not
be successful.

To make an update AuthConfig available in your supergraph, after updating your metadata, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

### Examples

Each example below offers numerous customizations. However, we've provided a basic example for each mode to get you
started. Check the reference table for more information on each field.

#### noAuth {#noAuth}

```yaml title="An AuthConfig for noAuth mode:"
kind: AuthConfig
version: v3
definition:
  mode:
    noAuth:
      role: admin
      sessionVariables: {}
```

| **Field**          | **Description**                                                                                                                                                                                                                  | **Reference**                                    |
| ------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------ |
| `role`             | The role to be assumed while running the engine in `noAuth` mode.                                                                                                                                                                | [Role](#authconfig-role)                         |
| `sessionVariables` | Static session variables that will be used while running the engine without authentication. This is helpful when you want to test requests using particular session variables, such as `x-hasura-user-id` with a non-admin role. | [SessionVariables](#authconfig-sessionvariables) |

#### JWT

```yaml title="An AuthConfig for JWT mode:"
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        fixed:
          algorithm: HS256
          key:
            valueFromEnv: AUTH_SECRET
```

| **Field**       | **Description**                                                                                                                                                                                                                                                                     | **Reference**                                    |
| --------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------ |
| `claimsConfig`  | Configuration to describe how and where the engine should look for the claims within the decoded token. You can vary the format and location of the claims.                                                                                                                         | [JWTClaimsConfig](#authconfig-jwtclaimsconfig)   |
| `tokenLocation` | Specifies the source of the JWT authentication token and how it should be parsed.                                                                                                                                                                                                   | [JWTTokenLocation](#authconfig-jwttokenlocation) |
| `key`           | Configuration for the JWT key, specifying how the incoming JWT will be verified and decoded. In this example, we've used a fixed key that's stored as an environment variable in our root-level `.env` file that is also mapped to the `subgraph.yaml` in our `/globals` directory. | [JWTKey](#authconfig-jwtkey)                     |

#### Webhook

```yaml title="An AuthConfig for Webhook mode:"
kind: AuthConfig
version: v3
definition:
  mode:
    webhook:
      url:
        value: http://auth_hook:3050/validate-request
      method: POST
      customHeadersConfig:
        body:
          headers:
            forward:
              - authorization
              - content-type
        headers:
          additional:
            user-agent: "Hasura DDN"

```

| **Field** | **Description**                                                                                                                                                                       | **Reference**                                                          |
| --------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| `url`     | The URL of the authentication webhook that will be called to validate authentication. This can be a static URL or an environment variable.                                            | [EnvironmentValue](#authconfig-environmentvalue)                       |
| `method`  | The HTTP method (`GET` or `POST`) that will be used to make the request to the auth hook.                                                                                             | [AuthHookConfigV3](#authconfig-authhookconfigv3)                       |
| `customHeadersConfig` | Configuration for the headers and body to be sent to the auth hook. This can be used to forward headers from the client request or add additional headers to the request. | [AuthHookConfigV3POSTHeaders](#authconfig-authhookconfigv3postheaders) |

---

## Metadata structure


#### AuthConfig {#authconfig-authconfig}

Definition of the authentication configuration used by the API server.


**One of the following values:**

| Value | Description |
|-----|-----|
| [AuthConfig1](#authconfig-authconfig1) | Definition of the authentication configuration v1, used by the API server. |
| [AuthConfig2](#authconfig-authconfig2) | Definition of the authentication configuration v2, used by the API server. |
| [AuthConfig3](#authconfig-authconfig3) |  |



#### AuthConfig3 {#authconfig-authconfig3}

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `AuthConfig` | true |  |
| `version` | `v3` | true |  |
| `definition` | [AuthConfigV3](#authconfig-authconfigv3) | true | Definition of the authentication configuration v3, used by the API server. |



#### AuthConfigV3 {#authconfig-authconfigv3}

Definition of the authentication configuration v3, used by the API server.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `mode` | [AuthModeConfigV3](#authconfig-authmodeconfigv3) | true | The configuration for the authentication mode to use - webhook, JWT or NoAuth. |



#### AuthModeConfigV3 {#authconfig-authmodeconfigv3}

The configuration for the authentication mode to use - webhook, JWT or NoAuth.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `webhook` | [AuthHookConfigV3](#authconfig-authhookconfigv3) | false | The configuration of the authentication webhook. |
| `jwt` | [JWTConfig](#authconfig-jwtconfig) | false |  |
| `noAuth` | [NoAuthConfig](#authconfig-noauthconfig) | false |  |



#### AuthHookConfigV3 {#authconfig-authhookconfigv3}

The configuration of the authentication webhook.


**One of the following values:**

| Value | Description |
|-----|-----|
| [AuthHookConfigV3GET](#authconfig-authhookconfigv3get) | The configuration of the GET authentication webhook. |
| [AuthHookConfigV3POST](#authconfig-authhookconfigv3post) | The configuration of the POST authentication webhook. |

 **Example:**

```yaml
method: GET
url:
  value: http://auth_hook:3050/validate-request
customHeadersConfig:
  headers:
    forward:
      - Authorization
    additional:
      user-agent: hasura-ddn
```


#### AuthHookConfigV3POST {#authconfig-authhookconfigv3post}

The configuration of the POST authentication webhook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `method` | `POST` | true |  |
| `url` | [EnvironmentValue](#authconfig-environmentvalue) | true | The URL of the POST authentication webhook. |
| `customHeadersConfig` | [AuthHookConfigV3POSTHeaders](#authconfig-authhookconfigv3postheaders) / null | false | The configuration for the headers to be sent to the POST auth hook. |

 **Example:**

```yaml
url:
  value: http://auth_hook:3050/validate-request
customHeadersConfig:
  headers:
    forward: '*'
    additional:
      user-agent: hasura-ddn
  body:
    headers:
      forward:
        - Authorization
      additional: {}
```


#### AuthHookConfigV3POSTHeaders {#authconfig-authhookconfigv3postheaders}

The configuration for the headers and body to be sent to the POST auth hook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `headers` | [AuthHookConfigV3Headers](#authconfig-authhookconfigv3headers) / null | false | The configuration for the headers to be sent to the POST auth hook. |
| `body` | [AuthHookConfigV3Body](#authconfig-authhookconfigv3body) / null | false | The configuration for the body to be sent to the POST auth hook. |

 **Example:**

```yaml
headers:
  forward: '*'
  additional:
    user-agent: hasura-ddn
body:
  headers:
    forward:
      - Authorization
    additional: {}
```


#### AuthHookConfigV3Body {#authconfig-authhookconfigv3body}

The configuration for the body to be sent to the POST auth hook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `headers` | [AuthHookConfigV3Headers](#authconfig-authhookconfigv3headers) / null | false | The configuration for the headers to be sent as part of the body to the POST auth hook. |

 **Example:**

```yaml
headers:
  forward:
    - Authorization
  additional: {}
```


#### AuthHookConfigV3GET {#authconfig-authhookconfigv3get}

The configuration of the GET authentication webhook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `method` | `GET` | true |  |
| `url` | [EnvironmentValue](#authconfig-environmentvalue) | true | The URL of the GET authentication webhook. |
| `customHeadersConfig` | [AuthHookConfigV3GETHeaders](#authconfig-authhookconfigv3getheaders) / null | false | The configuration for the headers to be sent to the GET auth hook. |

 **Example:**

```yaml
url:
  value: http://auth_hook:3050/validate-request
customHeadersConfig:
  headers:
    forward:
      - Authorization
    additional:
      user-agent: hasura-ddn
```


#### AuthHookConfigV3GETHeaders {#authconfig-authhookconfigv3getheaders}

The configuration for the headers to be sent to the GET auth hook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `headers` | [AuthHookConfigV3Headers](#authconfig-authhookconfigv3headers) / null | false |  |

 **Example:**

```yaml
headers:
  forward:
    - Authorization
  additional:
    user-agent: hasura-ddn
```


#### AuthHookConfigV3Headers {#authconfig-authhookconfigv3headers}

The configuration for the headers to be sent to the auth hook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `forward` | [AllOrList](#authconfig-allorlist) | false | The headers to be forwarded from the client request. |
| `additional` | [AdditionalHeaders](#authconfig-additionalheaders) | false | The additional headers to be sent to the auth hook. |

 **Example:**

```yaml
forward:
  - Authorization
additional:
  user-agent: hasura-ddn
```


#### AdditionalHeaders {#authconfig-additionalheaders}

The additional headers to be sent to the auth hook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` | string | false |  |



#### AllOrList {#authconfig-allorlist}

A list of items or a wildcard.


**One of the following values:**

| Value | Description |
|-----|-----|
| [All](#authconfig-all) | Wildcard: match all items |
| [string] |  |

 **Example:**

```yaml
'*'
```


#### All {#authconfig-all}

Wildcard: match all items


**Value:** `*`


#### AuthConfig2 {#authconfig-authconfig2}

Definition of the authentication configuration v2, used by the API server.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `AuthConfig` | true |  |
| `version` | `v2` | true |  |
| `definition` | [AuthConfigV2](#authconfig-authconfigv2) | true | Definition of the authentication configuration v2, used by the API server. |



#### AuthConfigV2 {#authconfig-authconfigv2}

Definition of the authentication configuration v2, used by the API server.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `mode` | [AuthModeConfig](#authconfig-authmodeconfig) | true |  |



#### AuthConfig1 {#authconfig-authconfig1}

Definition of the authentication configuration v1, used by the API server.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `AuthConfig` | true |  |
| `version` | `v1` | true |  |
| `definition` | [AuthConfigV1](#authconfig-authconfigv1) | true | Definition of the authentication configuration v1, used by the API server. |



#### AuthConfigV1 {#authconfig-authconfigv1}

Definition of the authentication configuration v1, used by the API server.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `allowRoleEmulationBy` | [Role](#authconfig-role) / null | false |  |
| `mode` | [AuthModeConfig](#authconfig-authmodeconfig) | true | The configuration for the authentication mode to use - webhook, JWT or NoAuth. |



#### AuthModeConfig {#authconfig-authmodeconfig}

The configuration for the authentication mode to use - webhook, JWT or NoAuth.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `webhook` | [AuthHookConfig](#authconfig-authhookconfig) | false | The configuration of the authentication webhook. |
| `jwt` | [JWTConfig](#authconfig-jwtconfig) | false | JWT config according to which the incoming JWT will be verified and decoded to extract the session variable claims. |
| `noAuth` | [NoAuthConfig](#authconfig-noauthconfig) | false | Configuration used when running engine without authentication |



#### NoAuthConfig {#authconfig-noauthconfig}

Configuration used when running engine without authentication

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `role` | [Role](#authconfig-role) | true | role to assume whilst running the engine |
| `sessionVariables` | [SessionVariables](#authconfig-sessionvariables) | true | static session variables to use whilst running the engine |

 **Example:**

```yaml
role: admin
sessionVariables:
  x-hasura-user-id: '100'
```


#### SessionVariables {#authconfig-sessionvariables}

static session variables to use whilst running the engine

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` |  | false | JSON value of a session variable |



#### JWTConfig {#authconfig-jwtconfig}

JWT config according to which the incoming JWT will be verified and decoded to extract the session variable claims.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `audience` | array / null | false | Optional validation to check that the `aud` field is a member of the `audience` received, otherwise will throw error. |
| `issuer` | string / null | false | Optional validation to check that the `iss` field is a member of the `iss` received, otherwise will throw error. |
| `allowedSkew` | integer / null | false | Allowed leeway (in seconds) to the `exp` validation to account for clock skew. |
| `claimsConfig` | [JWTClaimsConfig](#authconfig-jwtclaimsconfig) | true | Claims config. Either specified via `claims_mappings` or `claims_namespace_path` |
| `tokenLocation` | [JWTTokenLocation](#authconfig-jwttokenlocation) | true | Source of the JWT authentication token. |
| `key` | [JWTKey](#authconfig-jwtkey) | true | Mode according to which the JWT auth is configured. |

 **Example:**

```yaml
audience: null
issuer: null
allowedSkew: null
claimsConfig:
  namespace:
    claimsFormat: Json
    location: /claims.jwt.hasura.io
tokenLocation:
  type: BearerAuthorization
key:
  fixed:
    algorithm: HS256
    key:
      value: token
```


#### JWTKey {#authconfig-jwtkey}

JWT key configuration according to which the incoming JWT will be decoded.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fixed` | [JWTKeyConfig](#authconfig-jwtkeyconfig) | false | JWT Secret config according to which the incoming JWT will be decoded. |
| `jwkFromUrl` | string | false |  |



#### JWTKeyConfig {#authconfig-jwtkeyconfig}

JWT Secret config according to which the incoming JWT will be decoded.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `algorithm` | [JWTAlgorithm](#authconfig-jwtalgorithm) | true | The algorithm used to decode the JWT. |
| `key` | [EnvironmentValue](#authconfig-environmentvalue) | true | The key to use for decoding the JWT. |



#### EnvironmentValue {#authconfig-environmentvalue}

Either a literal string or a reference to a Hasura secret


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `value` | string | false |  |
| `valueFromEnv` | string | false |  |



#### JWTAlgorithm {#authconfig-jwtalgorithm}

The algorithm used to decode the JWT.


**One of the following values:**

| Value | Description |
|-----|-----|
| `HS256` | HMAC using SHA-256 |
| `HS384` | HMAC using SHA-384 |
| `HS512` | HMAC using SHA-512 |
| `ES256` | ECDSA using SHA-256 |
| `ES384` | ECDSA using SHA-384 |
| `RS256` | RSASSA-PKCS1-v1_5 using SHA-256 |
| `RS384` | RSASSA-PKCS1-v1_5 using SHA-384 |
| `RS512` | RSASSA-PKCS1-v1_5 using SHA-512 |
| `PS256` | RSASSA-PSS using SHA-256 |
| `PS384` | RSASSA-PSS using SHA-384 |
| `PS512` | RSASSA-PSS using SHA-512 |
| `EdDSA` | Edwards-curve Digital Signature Algorithm (EdDSA) |



#### JWTTokenLocation {#authconfig-jwttokenlocation}

Source of the Authorization token


**One of the following values:**

| Value | Description |
|-----|-----|
| [JWTBearerAuthorizationLocation](#authconfig-jwtbearerauthorizationlocation) | Get the bearer token from the `Authorization` header. |
| [JWTCookieLocation](#authconfig-jwtcookielocation) | Get the token from the Cookie header under the specified cookie name. |
| [JWTHeaderLocation](#authconfig-jwtheaderlocation) | Custom header from where the header should be parsed from. |



#### JWTHeaderLocation {#authconfig-jwtheaderlocation}

Custom header from where the header should be parsed from.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `type` | `Header` | true |  |
| `name` | string | true |  |



#### JWTCookieLocation {#authconfig-jwtcookielocation}

Get the token from the Cookie header under the specified cookie name.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `type` | `Cookie` | true |  |
| `name` | string | true |  |



#### JWTBearerAuthorizationLocation {#authconfig-jwtbearerauthorizationlocation}

Get the bearer token from the `Authorization` header.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `type` | `BearerAuthorization` | true |  |



#### JWTClaimsConfig {#authconfig-jwtclaimsconfig}

Config to describe how/where the engine should look for the claims within the decoded token.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `locations` | [JWTClaimsMap](#authconfig-jwtclaimsmap) | false | Can be used when Hasura claims are not all present in the single object, but individual claims are provided a JSON pointer within the decoded JWT and optionally a default value. |
| `namespace` | [JWTClaimsNamespace](#authconfig-jwtclaimsnamespace) | false | Used when all of the Hasura claims are present in a single object within the decoded JWT. |



#### JWTClaimsNamespace {#authconfig-jwtclaimsnamespace}

Used when all of the Hasura claims are present in a single object within the decoded JWT.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `claimsFormat` | [JWTClaimsFormat](#authconfig-jwtclaimsformat) | true | Format in which the Hasura claims will be present. |
| `location` | string | true | Pointer to lookup the Hasura claims within the decoded claims. |



#### JWTClaimsFormat {#authconfig-jwtclaimsformat}

Format in which the Hasura claims will be present.


**One of the following values:**

| Value | Description |
|-----|-----|
| `Json` | Claims will be in the JSON format. |
| `StringifiedJson` | Claims will be in the Stringified JSON format. |



#### JWTClaimsMap {#authconfig-jwtclaimsmap}

Can be used when Hasura claims are not all present in the single object, but individual claims are provided a JSON pointer within the decoded JWT and optionally a default value.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `x-hasura-default-role` | [JWTClaimsMappingEntry](#authconfig-jwtclaimsmappingentry) | true | JSON pointer to lookup the default role within the decoded JWT. |
| `x-hasura-allowed-roles` | [JWTClaimsMappingEntry](#authconfig-jwtclaimsmappingentry) | true | JSON pointer to lookup the allowed roles within the decoded JWT. |
| `<customKey>` | [JWTClaimsMappingEntry](#authconfig-jwtclaimsmappingentry) | false |  |



#### JWTClaimsMappingEntry {#authconfig-jwtclaimsmappingentry}

JSON pointer to lookup the default role within the decoded JWT.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `literal` | [Role](#authconfig-role) | false |  |
| `path` | [JWTClaimsMappingPathEntry](#authconfig-jwtclaimsmappingpathentry) | false | Entry to lookup the Hasura claims at the specified JSON Pointer |



#### JWTClaimsMappingPathEntry {#authconfig-jwtclaimsmappingpathentry}

Entry to lookup the Hasura claims at the specified JSON Pointer

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `path` | string | true | JSON pointer to find the particular claim in the decoded JWT token. |
| `default` | [Role](#authconfig-role) / null | false | Default value to be used when no value is found when looking up the value using the `path`. |



#### AuthHookConfig {#authconfig-authhookconfig}

The configuration of the authentication webhook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `url` | string | true | The URL of the authentication webhook. |
| `method` | [AuthHookMethod](#authconfig-authhookmethod) | true | The HTTP method to be used to make the request to the auth hook. |

 **Example:**

```yaml
url: http://auth_hook:3050/validate-request
method: Post
```


#### AuthHookMethod {#authconfig-authhookmethod}

The HTTP method to be used to make the request to the auth hook.


**Value:** `Get` / `Post`


#### Role {#authconfig-role}


**Value:** string



==============================



# compatibility-config.mdx

URL: https://hasura.io/docs/promptql/reference/metadata-reference/compatibility-config

# Compatibility Config

## Introduction

The CompatibilityConfig object is a metadata object that defines the compatibility configuration of the Hasura metadata.

## How CompatibilityConfig works

### Lifecycle

By default, all projects are created with a default CompatibilityConfig object in the **globals** subgraph. It can be
defined in any subgraph of your choice, but only once across your supergraph.

The `date` field in the CompatibilityConfig object specifies the date after which any new backwards-incompatible changes
made to Hasura DDN will be disabled and will not impact the Hasura project. For example, if your project has its `date`
set to `2024-10-16`, then any new features added after that date that would result in breaking changes to your project
will be disabled. To enable these features, simply increase your `date` to a newer date.

### Compatibility dates and breaking changes

The following is a list of dates at which backwards-incompatible changes were added to Hasura DDN. Projects with
CompatibilityConfig `date`s prior to these dates will have these features disabled.

#### 2025-03-26

##### Validate scalar boolean expression operators

A build error is now raised if any of the following validations for operators in scalar boolean expressions fail.
- Non-list argument types are not allowed for the `_in` operator
- Argument type must match the scalar type for `_eq`, `_lt`, `_lte`, `_gt` and `_gte` operators
- Mapped operators must exist in the data connector
- Argument type must be compatible with the mapped operator's NDC argument type
- Operators such as `contains`, `icontains`, `starts_with`, `istarts_with`, `ends_with`, and `iends_with` are only
  applicable on string scalars, with arguments strictly of type string

#### 2025-03-20

##### Disallow duplicate model permissions roles

A build error is now raised if the same role is defined more than once in a ModelPermissions object.

#### 2025-03-12

##### Disallow procedure command relationships

A build error is now raised if a relationship is defined between an object type and a procedure command. Procedure
commands cannot be used in relationships. This prevents navigation of relationships from accidentally causing
side-effects, as procedures can modify data.

#### 2025-03-11

##### Validate object type field types in data connector type mappings

A build error is now raised when there is a mismatch between the field types defined in an object type and the
corresponding field types in the data connector type mapping. This ensures type safety and prevents runtime errors due
to incompatible type mappings.

For example, if a PostgreSQL data connector defines a column as `INTEGER`, mapping it to an object type field defined as
`String` would raise an error:

```json
{
  "kind": "ObjectType",
  "version": "v1",
  "definition": {
    "name": "User",
    "fields": [
      {
        "name": "age",
        "type": "String!" // Error: PostgreSQL column is INTEGER but object type defines it as String
      }
    ],
    "dataConnectorTypeMapping": [
      {
        "dataConnectorName": "postgres",
        "dataConnectorObjectType": "users",
        "fieldMapping": {
          "age": {
            "column": {
              "name": "age"
            }
          }
        }
      }
    ]
  }
}
```

##### Validate argument type mappings

A build error is now raised when argument mapping types are invalid or incompatible with their corresponding data
connector types. This validation ensures that arguments passed to commands and models are properly typed and match the
expected types in the data connector.

For example, if a PostgreSQL function expects an `INTEGER` argument, mapping it to an argument defined as `String` would
raise an error:

```json
{
  "kind": "Command",
  "version": "v1",
  "definition": {
    "name": "get_users_by_age",
    "arguments": [
      {
        "name": "age",
        "type": "String!" // Error: PostgreSQL function expects INTEGER but argument is defined as String
      }
    ],
    "source": {
      "dataConnectorName": "postgres",
      "dataConnectorCommand": {
        "function": "get_users_by_age"
      },
      "argumentMapping": {
        "age": "age"
      }
    }
  }
}
```

##### Disallow invalid headers in AuthConfig

When using webhook mode in AuthConfig, Hasura DDN now validates all specified headers. Any invalid headers will trigger
a build error, whereas previously they were silently discarded.

##### Require `aud` claim validation in JWTs

If a JWT contains an `aud` claim, Hasura DDN will validate it against the
[`audience` field in the AuthConfig](/auth/jwt/jwt-configuration.mdx#audience). Previously this was optional, but now if
the JWT contains an `aud` claim, `audience` is must be configured in the AuthConfig. If the JWT contains an `aud` claim
and `audience` is not configured, the JWT will be rejected and authentication will fail.

This protects customers from accidentally misconfiguring their `audience` and exposing their API to unauthorized users.

#### 2025-02-27

##### Require valid command output type

A build error is now raised when a command's output type references a type that doesn't exist in the metadata.
Previously, invalid output types might have been allowed but could lead to runtime errors. This ensures that all
commands have valid and well-defined output types that can be properly handled by the system.

#### 2025-02-20

##### Disallow unknown values in arguments

Previously if an argument preset was provided via a session variable, and contained unknown fields, the engine would
ignore those fields and pass them on to the data connector.

For an object type with `name` and `age` fields, an input like this will throw an error, saying that we don't know
anything about the `not_in_our_object_type` field.

```JSON
{
  "name": "Bruce",
  "age": 30,
  "not_in_our_object_type": "bad value"
}
```

##### Disallow recursive object types

A build error is now raised when object types contain infinite recursion through non-nullable fields. This prevents
creating object type structures that could lead to infinite loops during query execution. For example:

```json
{
  "kind": "ObjectType",
  "version": "v1",
  "definition": {
    "name": "Person",
    "fields": [
      {
        "name": "id",
        "type": "Int!"
      },
      {
        "name": "name",
        "type": "String!"
      },
      {
        "name": "bestFriend",
        "type": "Friend!"  // Non-nullable reference to Friend type
      }
    ]
  }
},
{
  "kind": "ObjectType",
  "version": "v1",
  "definition": {
    "name": "Friend",
    "fields": [
      {
        "name": "id",
        "type": "Int!"
      },
      {
        "name": "name",
        "type": "String!"
      },
      {
        "name": "bestFriend",
        "type": "Person!"  // Non-nullable reference back to Person type
      }
    ]
  }
}
```

This creates an infinite recursion because `Person` must have a `Friend`, who must have a `Person`, who must have a
`Friend`, and so on infinitely. This is now disallowed. To fix this, make at least one of the references nullable:

```json
{
  "kind": "ObjectType",
  "version": "v1",
  "definition": {
    "name": "Person",
    "fields": [
      {
        "name": "id",
        "type": "Int!"
      },
      {
        "name": "name",
        "type": "String!"
      },
      {
        "name": "bestFriend",
        "type": "Friend" // Now nullable, breaking the infinite recursion
      }
    ]
  }
}
```

This ensures that all object type relationships have a clear termination point.

#### 2025-02-08

##### Disallow unsupported orderable relationships

Build errors are now raised if unsupported orderable relationships are defined on OrderByExpressions. Specifically:

- If the relationship is a remote relationship
- If the data connector does not support performing relationships
- If the target Model or Command of the relationship does not have a source defined

##### Disallow relationships where the data connector does not support relationships or remote relationships

A build error is now raised if a Relationship is defined between an ObjectType and a Model or Command that are sourced
from the same data connector, but that data connector does not support performing relationships or support remote
relationships.

#### 2025-02-04

##### Command and argument preset typechecking

A build error is now raised if an argument preset defined in CommandPermissions or ModelPermissions or a field preset in
TypePermissions specifies a literal value that does not match the type of the argument or field it is supposed to be
presetting.

##### Disallow scalar type mismatch in object type fields

A build error is now raised if an ObjectType's field's scalar type does not match the scalar type of the underlying data
connector's object type field. The field's specified scalar type must have a DataConnectorScalarRepresentation defined
that specifies it as the representation for the data connector's scalar type.

##### Disallow unknown ObjectType field types

A build error is now raised if an ObjectType's field's type is an unknown type.

##### Disallow orderable fields that have field arguments

A build error is now raised if an orderable field on an OrderByExpression has field arguments. This scenario is not
supported.

#### 2025-01-25

##### Disallow duplicate scalar type aggregate functions

A build error is now raised if a data connector scalar type has multiple aggregate functions of the same type (for
example, two sum functions for the same scalar type). This ensures that there is no ambiguity when choosing an aggregate
function for a particular purpose (for example, performing a sum aggregate).

#### 2025-01-07

##### Disallow duplicate operator definitions for scalar type

A build error is now raised when a scalar type has multiple operator definitions with the same name. For example, if you
have a custom scalar type and define multiple operators with the same name in its boolean expression configuration, the
build will fail. This ensures that operator definitions for scalar types are unique and prevents ambiguity in boolean
expressions.

##### Disallow multidimensional arrays in boolean expressions

A build error is now raised when multidimensional arrays (arrays of arrays) are used in boolean expressions. This
restriction applies to both array comparison operators and array relationship fields within boolean expressions.
Previously, such configurations might have been allowed but could lead to runtime errors or undefined behavior. This
change ensures that only single-dimensional arrays can be used in boolean expressions, making the behavior more
predictable and preventing potential runtime issues.

##### Disallow duplicate names across types and expressions

A build error is now raised when:

- Duplicate names exist within a subgraph for the following definitions:

  - [`BooleanExpressionType`](/reference/metadata-reference/boolean-expressions.mdx#booleanexpressiontype-booleanexpressiontype)
  - [`OrderByExpression`](/reference/metadata-reference/orderby-expressions.mdx)

- Type name conflicts exist within a subgraph across the following type definitions:
  - [`ScalarType`](/reference/metadata-reference/types.mdx#scalartype)
  - [`ObjectType`](/reference/metadata-reference/types.mdx#objecttype)
  - [`BooleanExpressionType`](/reference/metadata-reference/boolean-expressions.mdx#booleanexpressiontype-booleanexpressiontype)

#### 2024-12-18

##### Disallow non-scalar fields in Model v1 `orderableFields`

`orderableFields` in `Model` v1 now only allows fields of scalar types. Previously this was allowed, but queries that
tried to order against the non-scalar fields would fail at runtime or produce unexpected results. To order over nested
object fields, upgrade to `Model` v2 and use `OrderByExpression`s instead.

##### Disallow nested fields and nested relationships in OrderByExpressions when the data connector does not support them

Previously `OrderByExpression`s that incorporated nested fields and nested relationships were allowed to be used with
`Model`s that sourced from data connectors that did not support ordering by nested fields and nested relationships. Such
a configuration will now result in a build error.

##### Prevent usage of array relationships in OrderByExpressions

Previously `OrderByExpression`s could specify array relationships in `orderableRelationships`. This was incorrect
behaviour, as only object relationships can be ordered over, and now results in a build error.

#### 2024-12-10

##### Disallow multiple fields on input objects in GraphQL in `order_by`

GraphQL queries with `order_by` arguments that contain multiple properties set on one input object now properly return
an error. For example `order_by: { location: { city: Asc, country: Asc } }` is no longer allowed. This is because the
order of input object fields in GraphQL is not defined, so it is unclear whether ordering should be first by city or
first by country. Instead, write this query like so:
`order_by: [{ location: { city: Asc } }, { location: { country: Asc } }]`.

Additionally, ordering by nested fields using a nested array is no longer allowed (for example:
`order_by: { location: [{ city: Asc }, { country: Asc }] }`). Instead, write this query like so:
`order_by: [{ location: { city: Asc } }, { location: { country: Asc } }]`.

#### 2024-12-05

##### Prevent conflicts between boolean expression fields in GraphQL

Conflicts between fields on boolean expression types that have the same name (such as conflicts between fields and
logical operators) are now detected and a build error is raised.

#### 2024-11-26

##### Logical operators in scalar boolean expression types

Previously, if logical operators (ie. `_and`, `_or`, `_not`) were enabled in scalar `BooleanExpressionType`s, they would
_not_ appear in the GraphQL API. Now, if they are enabled, they will correctly appear in the GraphQL API as configured.

#### 2024-11-18

##### Disallow object boolean expression type

A build error is now raised if `ObjectBooleanExpressionType` is used to define a boolean expression. To resolve this,
you can upgrade all object boolean expression types in your metadata by running the following DDN codemod command.

```ddn
ddn codemod upgrade-object-boolean-expression-types
```

Learn more about the [command here](/reference/cli/commands/ddn_codemod_upgrade-object-boolean-expression-types.mdx).

#### 2024-11-15

##### Require unique model GraphQL names

A build error is now raised if a root field defined in a `Model`'s GraphQL config conflicts with an existing root field.
Previously, the conflicting root field was silently excluded from the GraphQL schema.

##### Require a valid NDC version in data connector capabilities

A build error is now raised when there is an invalid version in the capabilities of a data connector's schema. To fix
the error, please consider upgrading to the latest version of the data connector and running `ddn connector introspect`
to correct the invalid version. Prior to this change, it was assumed that the data connector with the invalid version
was an NDC v0.1.x connector.

#### 2024-11-13

##### Allow resolving boolean expression fields without GraphQL config

Previously, comparable fields, relationships, and aggregate fields of a `BooleanExpressionType` were validated only if
the `graphql` field was configured in the metadata. These fields are now validated regardless of whether the `graphql`
configuration is specified.

#### 2024-10-31

##### Disallow comparisons against scalar array fields in predicates

A build error is now raised if a scalar array field is configured in a BooleanExpressionType as a comparable field.
Comparisons against scalar array fields are not supported as at this compatibility date.

#### 2024-10-16

##### Enable JSON session variable support

Session variables provided in JWT claims, webhook responses or in the NoAuth settings can now be any JSON value, instead
of just a string. This enable scenarios such as being able to put lists of values into a single session variable.
However, this now means that if you use the session variable in a comparison against a field, the field's type must
match the type of the session variable. For example, if you are comparing against an integer field, your session
variable must be set as a JSON number and not a string.

#### 2024-10-07

##### Require unique command GraphQL names

A build error is now raised when there is a conflict with a GraphQL root field or GraphQL type name that is already in
use. Before this, the command would be silently dropped from the GraphQL schema.

#### 2024-09-26

##### Propagate boolean expression deprecation status

The `deprecated` status for relationship fields is now propagated to boolean expressions that use them. The default
GraphQL introspection behavior is to hide such fields from the schema, so this behavior is considered a breaking change.

##### Disallow scalar types names conflicting with inbuilt types

A build error is now raised when a `ScalarType` metadata type is defined that conflicts with any of the built-in type
names (`ID`, `Int`, `String`, `Boolean`, `Float`).

#### 2024-09-18

##### Require nested array filtering capability

A build error is now raised when a nested array field is defined on an ObjectType used in a Model whose underlying data
connector does not have the `query.exists.nested_collections` capability defined.

#### 2024-09-03

##### Enable relationships in predicates to be used even if the data connector does not support it

Previously, only data connectors that supported the `relationships.relation_comparisons` capability supported
relationships in predicates. This change allows you to use relationships in boolean expressions even if the data
connector lacks support. When the capability is available, relationship predicates are resolved directly within the
native data connector for more efficient processing. If not, the predicates are handled at the API layer.

#### 2024-06-30

##### Require GraphQL Config

A build error is now raised if a [GraphQLConfig](reference/metadata-reference/graphql-config.mdx) is not provided in the
project metadata.

---

## Metadata structure


### v2_CompatibilityConfig {#v2_compatibilityconfig-v2_compatibilityconfig}

The compatibility configuration of the Hasura metadata.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `CompatibilityConfig` | true |  |
| `date` | [CompatibilityDate](#v2_compatibilityconfig-compatibilitydate) | true | Any backwards incompatible changes made to Hasura DDN after this date won't impact the metadata. |



#### CompatibilityDate {#v2_compatibilityconfig-compatibilitydate}

Any backwards incompatible changes made to Hasura DDN after this date won't impact the metadata


**One of the following values:**

| Value | Description |
|-----|-----|
| `2024-06-30` / `2024-09-03` / `2024-09-18` / `2024-09-26` / `2024-10-07` / `2024-10-16` / `2024-10-31` / `2024-11-13` / `2024-11-15` / `2024-11-18` / `2024-11-26` / `2024-12-05` / `2024-12-10` / `2024-12-18` / `2025-01-07` / `2025-01-25` / `2025-02-04` / `2025-02-08` / `2025-02-20` / `2025-02-27` / `2025-03-11` / `2025-03-12` / `2025-03-21` / `2025-03-26` / `2025-04-03` / `2025-04-24` | Known compatibility dates |
| string | Any date |



==============================



# engine-plugins.mdx

URL: https://hasura.io/docs/promptql/reference/metadata-reference/engine-plugins

# Engine Plugins

## Introduction

Engine plugins are a way to extend the functionality of the Hasura GraphQL Engine. Engine plugins are executed at
various stages of the request lifecycle. Engine plugins can be used to modify the request, response, or to perform
custom operations.

### How LifeCyclePluginHooks work

Lifecycle plugin hooks are not generated by default when a Hasura project is created. They need to be manually added to
the metadata.

When you create a LifeCyclePluginHook, you define the configuration for the plugin in your metadata by passing
information like the URL of the hosted plugin and its name. The plugin is then executed at the specified stage of the
request lifecycle.

After creating a new LifeCyclePluginHook in your metadata, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

### Examples

```yaml title="An example of a LifecyclePluginHook:"
kind: LifecyclePluginHook
version: v1
definition:
  name: cloudflare allowlist
  url:
    valueFromEnv: ALLOW_LIST_URL
  pre: parse
  config:
    request:
      headers:
        additional:
          hasura-m-auth:
            valueFromEnv: M_AUTH_KEY
      session: {}
      rawRequest:
        query: {}
        variables: {}
```

| Field                       | Description                                                               | Reference                                                                                                                                                    |
| --------------------------- | ------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `name`                      | The name of the lifecycle plugin hook.                                    | [LifecyclePluginHookPreParse](#lifecyclepluginhook-lifecyclepreparsepluginhook)                                                                              |
| `url`                       | The URL to access the plugin.                                             | [LifecyclePluginHookPreParse](#lifecyclepluginhook-lifecyclepreparsepluginhook)                                                                              |
| `pre`                       | The stage of the request lifecycle.                                       | [LifecyclePluginHookPreParse](#lifecyclepluginhook-lifecyclepreparsepluginhook)                                                                              |
| `config`                    | Configuration for the plugin.                                             | [PreParse](#lifecyclepluginhook-lifecyclepreparsepluginhookconfig) or [PreResponse](#lifecyclepluginhook-lifecyclepreresponsepluginhookconfig)               |
| `config.request`            | The shape of the request object.                                          | [PreParse](#lifecyclepluginhook-lifecyclepreparsepluginhookconfigrequest) or [PreResponse](#lifecyclepluginhook-lifecyclepreresponsepluginhookconfigrequest) |
| `config.request.headers`    | The headers that should be included for each request.                     | [LifecyclePluginHookHeadersConfig](#lifecyclepluginhook-lifecyclepluginhookheadersconfig)                                                                    |
| `config.request.rawRequest` | The configuration of the raw request, including any queries or variables. | [RawRequestConfig](#lifecyclepluginhook-rawrequestconfig)                                                                                                    |

## Metadata structure


### LifecyclePluginHook {#lifecyclepluginhook-lifecyclepluginhook}

Definition of a lifecycle plugin hook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `LifecyclePluginHook` | true |  |
| `version` | `v1` | true |  |
| `definition` | [LifecyclePluginHookV1](#lifecyclepluginhook-lifecyclepluginhookv1) | true | Definition of a lifecycle plugin hook - version 1. |

 **Example:**

```yaml
kind: LifecyclePluginHook
version: v1
definition:
  pre: parse
  name: test
  url:
    value: http://localhost:8080
  config:
    request:
      headers:
        additional:
          hasura-m-auth:
            value: zZkhKqFjqXR4g5MZCsJUZCnhCcoPyZ
      session: {}
      rawRequest:
        query: {}
        variables: {}
```


#### LifecyclePluginHookV1 {#lifecyclepluginhook-lifecyclepluginhookv1}

Definition of a lifecycle plugin hook - version 1.


**One of the following values:**

| Value | Description |
|-----|-----|
| [LifecyclePreParsePluginHook](#lifecyclepluginhook-lifecyclepreparsepluginhook) | Definition of a lifecycle plugin hook for the pre-parse stage. |
| [LifecyclePreResponsePluginHook](#lifecyclepluginhook-lifecyclepreresponsepluginhook) | Definition of a lifecycle plugin hook for the pre-response stage. |
| [LifecyclePreRoutePluginHook](#lifecyclepluginhook-lifecyclepreroutepluginhook) | Definition of a lifecycle plugin hook for the pre-route stage. |



#### LifecyclePreRoutePluginHook {#lifecyclepluginhook-lifecyclepreroutepluginhook}

Definition of a lifecycle plugin hook for the pre-route stage.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `pre` | `route` | true |  |
| `name` | string | true | The name of the lifecycle plugin hook. |
| `url` | [EnvironmentValue](#lifecyclepluginhook-environmentvalue) | true | The URL to access the lifecycle plugin hook. |
| `config` | [LifecyclePreRoutePluginHookConfig](#lifecyclepluginhook-lifecyclepreroutepluginhookconfig) | true | Configuration for the lifecycle plugin hook. |



#### LifecyclePreRoutePluginHookConfig {#lifecyclepluginhook-lifecyclepreroutepluginhookconfig}

Configuration for a lifecycle plugin hook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `matchPath` | string | true | Regex to match the request path |
| `matchMethods` | [[RequestMethod](#lifecyclepluginhook-requestmethod)] | true | Possible HTTP methods for the request |
| `request` | [LifecyclePreRoutePluginHookConfigRequest](#lifecyclepluginhook-lifecyclepreroutepluginhookconfigrequest) | true | Configuration for the request to the lifecycle plugin hook. |
| `response` | [LifecyclePreRoutePluginHookConfigResponse](#lifecyclepluginhook-lifecyclepreroutepluginhookconfigresponse) / null | false | Configuration for the response to the lifecycle plugin hook. |



#### LifecyclePreRoutePluginHookConfigResponse {#lifecyclepluginhook-lifecyclepreroutepluginhookconfigresponse}

Configuration for a lifecycle plugin hook response.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `headers` | [LifecyclePluginHookHeadersConfig](#lifecyclepluginhook-lifecyclepluginhookheadersconfig) / null | false | Configuration for the headers in the response from the engine. |



#### LifecyclePreRoutePluginHookConfigRequest {#lifecyclepluginhook-lifecyclepreroutepluginhookconfigrequest}

Configuration for a lifecycle plugin hook request.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `headers` | [LifecyclePluginHookHeadersConfig](#lifecyclepluginhook-lifecyclepluginhookheadersconfig) / null | false | Configuration for the headers in the pre-route plugin hook HTTP requests. |
| `method` | [LifecyclePreRoutePluginHookConfigRequestMethods](#lifecyclepluginhook-lifecyclepreroutepluginhookconfigrequestmethods) | true | Configuration for the HTTP method for the pre-route plugin hook HTTP requests. |
| `rawRequest` | [PreRouteRequestConfig](#lifecyclepluginhook-prerouterequestconfig) | true | Configuration for the raw request body for the pre-route plugin hook HTTP requests. |



#### PreRouteRequestConfig {#lifecyclepluginhook-prerouterequestconfig}

Configuration for the raw request body for the pre-route plugin hook HTTP requests.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `path` | [LeafConfig](#lifecyclepluginhook-leafconfig) / null | false | Configuration for adding/excluding the request path of the incoming request |
| `method` | [LeafConfig](#lifecyclepluginhook-leafconfig) / null | false | Configuration for adding/excluding the request method of the incoming request |
| `query` | [LeafConfig](#lifecyclepluginhook-leafconfig) / null | false | Configuration for adding/excluding the query params of the incoming request |
| `body` | [LeafConfig](#lifecyclepluginhook-leafconfig) / null | false | Configuration for adding/excluding the body of the incoming request |



#### LifecyclePreRoutePluginHookConfigRequestMethods {#lifecyclepluginhook-lifecyclepreroutepluginhookconfigrequestmethods}

Configuration for the method for the pre-route plugin hook HTTP requests.


**Value:** `GET` / `POST`


#### RequestMethod {#lifecyclepluginhook-requestmethod}

Possible HTTP Request Methods for the incoming requests handled by the pre-route plugin hook.


**Value:** `GET` / `POST` / `PUT` / `DELETE` / `PATCH`


#### LifecyclePreResponsePluginHook {#lifecyclepluginhook-lifecyclepreresponsepluginhook}

Definition of a lifecycle plugin hook for the pre-response stage.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `pre` | `response` | true |  |
| `name` | string | true | The name of the lifecycle plugin hook. |
| `url` | [EnvironmentValue](#lifecyclepluginhook-environmentvalue) | true | The URL to access the lifecycle plugin hook. |
| `config` | [LifecyclePreResponsePluginHookConfig](#lifecyclepluginhook-lifecyclepreresponsepluginhookconfig) | true | Configuration for the lifecycle plugin hook. |



#### LifecyclePreResponsePluginHookConfig {#lifecyclepluginhook-lifecyclepreresponsepluginhookconfig}

Configuration for a lifecycle plugin hook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `request` | [LifecyclePreResponsePluginHookConfigRequest](#lifecyclepluginhook-lifecyclepreresponsepluginhookconfigrequest) | true | Configuration for the request to the lifecycle plugin hook. |



#### LifecyclePreResponsePluginHookConfigRequest {#lifecyclepluginhook-lifecyclepreresponsepluginhookconfigrequest}

Configuration for a lifecycle plugin hook request.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `headers` | [LifecyclePluginHookHeadersConfig](#lifecyclepluginhook-lifecyclepluginhookheadersconfig) / null | false | Configuration for the headers. |
| `session` | [LeafConfig](#lifecyclepluginhook-leafconfig) / null | false | Configuration for the session (includes roles and session variables). |
| `rawRequest` | [RawRequestConfig](#lifecyclepluginhook-rawrequestconfig) | true | Configuration for the raw request. |
| `rawResponse` | [LeafConfig](#lifecyclepluginhook-leafconfig) / null | false | Configuration for the response. |



#### LifecyclePreParsePluginHook {#lifecyclepluginhook-lifecyclepreparsepluginhook}

Definition of a lifecycle plugin hook for the pre-parse stage.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `pre` | `parse` | true |  |
| `name` | string | true | The name of the lifecycle plugin hook. |
| `url` | [EnvironmentValue](#lifecyclepluginhook-environmentvalue) | true | The URL to access the lifecycle plugin hook. |
| `config` | [LifecyclePreParsePluginHookConfig](#lifecyclepluginhook-lifecyclepreparsepluginhookconfig) | true | Configuration for the lifecycle plugin hook. |



#### LifecyclePreParsePluginHookConfig {#lifecyclepluginhook-lifecyclepreparsepluginhookconfig}

Configuration for a lifecycle plugin hook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `request` | [LifecyclePreParsePluginHookConfigRequest](#lifecyclepluginhook-lifecyclepreparsepluginhookconfigrequest) | true | Configuration for the request to the lifecycle plugin hook. |



#### LifecyclePreParsePluginHookConfigRequest {#lifecyclepluginhook-lifecyclepreparsepluginhookconfigrequest}

Configuration for a lifecycle plugin hook request.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `headers` | [LifecyclePluginHookHeadersConfig](#lifecyclepluginhook-lifecyclepluginhookheadersconfig) / null | false | Configuration for the headers. |
| `session` | [LeafConfig](#lifecyclepluginhook-leafconfig) / null | false | Configuration for the session (includes roles and session variables). |
| `rawRequest` | [RawRequestConfig](#lifecyclepluginhook-rawrequestconfig) | true | Configuration for the raw request. |



#### RawRequestConfig {#lifecyclepluginhook-rawrequestconfig}

Configuration for the raw request.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `query` | [LeafConfig](#lifecyclepluginhook-leafconfig) / null | false | Configuration for the query. |
| `variables` | [LeafConfig](#lifecyclepluginhook-leafconfig) / null | false | Configuration for the variables. |



#### LeafConfig {#lifecyclepluginhook-leafconfig}

Leaf Configuration.

| Key | Value | Required | Description |
|-----|-----|-----|-----|



#### LifecyclePluginHookHeadersConfig {#lifecyclepluginhook-lifecyclepluginhookheadersconfig}

Configuration for a lifecycle plugin hook headers.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `additional` | [HttpHeaders](#lifecyclepluginhook-httpheaders) / null | false | Additional headers to be sent with the request. |
| `forward` | [string] | false | Headers to be forwarded from the incoming request. |



#### HttpHeaders {#lifecyclepluginhook-httpheaders}

Key value map of HTTP headers to be sent with an HTTP request. The key is the header name and the value is a potential reference to an environment variable.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` | [EnvironmentValue](#lifecyclepluginhook-environmentvalue) | false |  |



#### EnvironmentValue {#lifecyclepluginhook-environmentvalue}

Either a literal string or a reference to a Hasura secret


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `value` | string | false |  |
| `valueFromEnv` | string | false |  |



==============================



# promptql-config.mdx

URL: https://hasura.io/docs/promptql/reference/metadata-reference/promptql-config

# PromptQL Configuration

## Introduction

Your PromptQlConfig is a metadata object that defines the configuration of PromptQL for your project. It includes the
LLM to be used, the system instructions, and other settings.

```yaml title="Example PromptQlConfig"
kind: PromptQlConfig
version: v2
definition:
  llm:
    provider: openai
    model: o3-mini
  ai_primitives_llm:
    provider: openai
    model: gpt-4o
  system_instructions: |
    You are a helpful AI Assistant.
```

## Metadata structure

### PromptQlConfigV2 {#promptqlconfigv2-promptqlconfigv2}

Definition of the configuration of PromptQL, v2

| Key          | Value                                                  | Required | Description                                                 |
| ------------ | ------------------------------------------------------ | -------- | ----------------------------------------------------------- |
| `kind`       | `PromptQlConfig`                                       | true     |                                                             |
| `version`    | `v2`                                                   | true     |                                                             |
| `definition` | [PromptQlConfigV2](#promptqlconfigv2-promptqlconfigv2) | true     | Definition of the configuration of PromptQL for the project |

#### PromptQlConfigV2 {#promptqlconfigv2-promptqlconfigv2}

Definition of the configuration of PromptQL for the project

| Key                  | Value                                           | Required | Description                                                                                                                              |
| -------------------- | ----------------------------------------------- | -------- | ---------------------------------------------------------------------------------------------------------------------------------------- |
| `systemInstructions` | string / null                                   | false    | Custom system instructions provided to every PromptQL thread that allows tailoring of behavior to match to the project's specific needs. |
| `llm`                | [LlmConfig](#promptqlconfigv2-llmconfig)        | true     | Configuration of the LLM to be used for PromptQL                                                                                         |
| `aiPrimitivesLlm`    | [LlmConfig](#promptqlconfigv2-llmconfig) / null | false    | Configuration of the LLM to be used for AI primitives, such as classification, summarization etc                                         |
| `featureFlags`       | object / null                                   | false    | Feature flags to be used for PromptQL                                                                                                    |

#### LlmConfig {#promptqlconfigv2-llmconfig}

Configuration of the LLM to be used for PromptQL

**One of the following values:**

| Value                                                      | Description                                            |
| ---------------------------------------------------------- | ------------------------------------------------------ |
| [HasuraLlmConfig](#promptqlconfigv2-hasurallmconfig)       | Configuration settings for the Hasura-configured LLM   |
| [OpenAiLlmConfig](#promptqlconfigv2-openaillmconfig)       | Configuration settings for an OpenAI LLM               |
| [AnthropicLlmConfig](#promptqlconfigv2-anthropicllmconfig) | Configuration settings for an Anthropic LLM            |
| [AzureLlmConfig](#promptqlconfigv2-azurellmconfig)         | Configuration settings for an Azure-provided LLM       |
| [GeminiLlmConfig](#promptqlconfigv2-geminillmconfig)       | Configuration settings for a Gemini LLM                |
| [BedrockLlmConfig](#promptqlconfigv2-bedrockllmconfig)     | Configuration settings for an AWS Bedrock-provided LLM |

#### BedrockLlmConfig {#promptqlconfigv2-bedrockllmconfig}

Configuration settings for an AWS Bedrock-provided LLM

| Key                  | Value                                                  | Required | Description                                              |
| -------------------- | ------------------------------------------------------ | -------- | -------------------------------------------------------- |
| `provider`           | `bedrock`                                              | true     |                                                          |
| `modelId`            | string                                                 | true     | The specific AWS Bedrock model to use.                   |
| `regionName`         | string                                                 | true     | The specific AWS Bedrock region to use.                  |
| `awsAccessKeyId`     | [EnvironmentValue](#promptqlconfigv2-environmentvalue) | true     | The AWS access key ID to use for the AWS Bedrock API     |
| `awsSecretAccessKey` | [EnvironmentValue](#promptqlconfigv2-environmentvalue) | true     | The AWS secret access key to use for the AWS Bedrock API |

#### GeminiLlmConfig {#promptqlconfigv2-geminillmconfig}

Configuration settings for a Gemini LLM

| Key        | Value                                                  | Required | Description                                                                         |
| ---------- | ------------------------------------------------------ | -------- | ----------------------------------------------------------------------------------- |
| `provider` | `gemini`                                               | true     |                                                                                     |
| `model`    | string / null                                          | false    | The specific Gemini model to use. If not specified, the default model will be used. |
| `apiKey`   | [EnvironmentValue](#promptqlconfigv2-environmentvalue) | true     | The API key to use for the Gemini API                                               |

#### AzureLlmConfig {#promptqlconfigv2-azurellmconfig}

Configuration settings for an Azure-provided LLM

| Key          | Value                                                  | Required | Description                                                                                |
| ------------ | ------------------------------------------------------ | -------- | ------------------------------------------------------------------------------------------ |
| `provider`   | `azure`                                                | true     |                                                                                            |
| `apiVersion` | string / null                                          | false    | The specific Azure API version to use. If not specified, the default version will be used. |
| `model`      | string / null                                          | false    | The specific Azure model to use. If not specified, the default model will be used.         |
| `endpoint`   | string                                                 | true     | The endpoint to use for the Azure LLM API                                                  |
| `apiKey`     | [EnvironmentValue](#promptqlconfigv2-environmentvalue) | true     | The API key to use for the Azure API                                                       |

#### AnthropicLlmConfig {#promptqlconfigv2-anthropicllmconfig}

Configuration settings for an Anthropic LLM

| Key        | Value                                                  | Required | Description                                                                                |
| ---------- | ------------------------------------------------------ | -------- | ------------------------------------------------------------------------------------------ |
| `provider` | `anthropic`                                            | true     |                                                                                            |
| `model`    | string / null                                          | false    | The specific Anthropic model to use. If not specified, the default model will be used.     |
| `baseUrl`  | string / null                                          | false    | The base URL to use for the Anthropic API. If not specified, the default URL will be used. |
| `apiKey`   | [EnvironmentValue](#promptqlconfigv2-environmentvalue) | true     | The API key to use for the Anthropic API                                                   |

#### OpenAiLlmConfig {#promptqlconfigv2-openaillmconfig}

Configuration settings for an OpenAI LLM

| Key        | Value                                                  | Required | Description                                                                             |
| ---------- | ------------------------------------------------------ | -------- | --------------------------------------------------------------------------------------- |
| `provider` | `openai`                                               | true     |                                                                                         |
| `model`    | string / null                                          | false    | The specific OpenAI model to use. If not specified, the default model will be used.     |
| `baseUrl`  | string / null                                          | false    | The base URL to use for the OpenAI API. If not specified, the default URL will be used. |
| `apiKey`   | [EnvironmentValue](#promptqlconfigv2-environmentvalue) | true     | The API key to use for the OpenAI API                                                   |

#### EnvironmentValue {#promptqlconfigv2-environmentvalue}

Either a literal string or a reference to a Hasura secret

**Must have exactly one of the following fields:**

| Key            | Value  | Required | Description |
| -------------- | ------ | -------- | ----------- |
| `value`        | string | false    |             |
| `valueFromEnv` | string | false    |             |

#### HasuraLlmConfig {#promptqlconfigv2-hasurallmconfig}

Configuration settings for the Hasura-configured LLM

| Key        | Value    | Required | Description |
| ---------- | -------- | -------- | ----------- |
| `provider` | `hasura` | true     |             |



==============================



# index.mdx

URL: https://hasura.io/docs/promptql/reference/cli/

# Hasura CLI

## Introduction

Hasura introduces a new CLI, that gives you complete control over your data layer. You can create projects, apply
builds, and author metadata — all without leaving your favorite terminal.

## Find out more

- [Install the CLI](/reference/cli/installation.mdx)
- [Learn about the commands](/reference/cli/commands/index.mdx)



==============================



# installation.mdx

URL: https://hasura.io/docs/promptql/reference/cli/installation


# Installation {#install-instructions}

## Install the CLI


<Tabs groupId="os-preference" className="api-tabs">

<TabItem value="macOS-or-linux" label="macOS and Linux">

Simply run the installer script in your terminal:

<CodeBlock language="bash">
  {`curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/get.sh | bash`}
</CodeBlock>

<Admonition type="info" title="ARM-based Linux Machines">
  Currently, the CLI does not support installation on ARM-based Linux systems.
</Admonition>

</TabItem>

<TabItem value="windows" label="Windows">
- Download the latest <a href={`https://graphql-engine-cdn.hasura.io/ddn/cli/${props.revision || "v4"}/latest/DDN_CLI_Setup.exe`}>DDN CLI installer for Windows.</a> 
- Run the `DDN_CLI_Setup.exe` installer file and follow the instructions. This will only take a minute.
- By default, the DDN CLI is installed under `C:\Users\{Username}\AppData\Local\Programs\DDN_CLI`
- The DDN CLI is added to your `%PATH%` environment variable so that you can use the `ddn` command from your terminal.
<br />

</TabItem>
</Tabs>


## Verify the installation

Running `ddn version` should print the CLI version, For example:

```
DDN CLI Version: v2.0.1
```

## Set up auto-completion (optional)

The DDN CLI supports auto-completion for [zsh](#zsh), [Bash](#bash), [PowerShell](#powershell) and [fish](#fish).

### zsh

**Temporary set up**

```sh title="To load completions in the current session:"
source <(ddn completion zsh)
```

**Permanent set up**

```sh title="Ensure shell completion is enabled by adding the following to your ~/.zshrc file:"
echo "autoload -U compinit; compinit" >> ~/.zshrc
```

#### macOS

```sass
ddn completion zsh > $(brew --prefix)/share/zsh/site-functions/_ddn
```

#### Linux

```ddn title="Execute the following:"
ddn completion zsh > "${fpath[1]}/_ddn"
```

:::note

Restart your shell so the changes take effect

:::

Try using the DDN CLI with auto-completion by typing part of a command and pressing `TAB` to verify.

### Bash

Install the [bash-completion package](https://github.com/scop/bash-completion) using your OS's package manager if it's
not already installed.

**Temporary set up**

```sh title="To load completions in the current session:"
source <(ddn completion bash)
```

**Permanent set up**

#### macOS

```ddn title="Execute the following:"
ddn completion bash > $(brew --prefix)/etc/bash_completion.d/ddn
```

#### Linux

```ddn title="Execute the following:"
ddn completion bash > /etc/bash_completion.d/ddn
```

:::note

Restart your shell so the changes take effect

:::

Try using the DDN CLI with auto-completion by typing part of a command and pressing `TAB` to verify.

### PowerShell

#### Step 1. Generate the auto-completion script

Run the following command to generate the script for enabling auto-completion:

```ddn
ddn completion powershell | Out-String | Invoke-Expression
```

This temporarily enables auto-completion for the current session.

#### Step 2. Locate your profile

```sh title="Locate your PowerShell profile by running the following command:"
$PROFILE
```

Your PowerShell profile is a script that runs each time PowerShell starts.

Common paths for the PowerShell profile are:

- Windows PowerShell: `C:\Users\<YourUsername>\Documents\WindowsPowerShell\Microsoft.PowerShell_profile.ps1`
- PowerShell Core: `C:\Users\<YourUsername>\Documents\PowerShell\Microsoft.PowerShell_profile.ps1`

#### Step 3. Edit your PowerShell profile

```sh title="Open the profile in a text editor:"
notepad $PROFILE
```

```plaintext title="Add the following line to the end of the file:"
ddn completion powershell | Out-String | Invoke-Expression
```

Save the file and close the editor.

:::note

Restart your shell so the changes take effect

:::

Try using the DDN CLI with auto-completion by typing part of a command and pressing `TAB` to verify.

### fish

**Temporary set up**

```ddn title="Execute the following:"
ddn completion fish | source
```

**Permanent set up**

```ddn title="Execute the following:"
ddn completion fish > ~/.config/fish/completions/ddn.fish
```

:::note

Restart your shell so the changes take effect

:::

Try using the DDN CLI with auto-completion by typing part of a command and pressing `TAB` to verify.



==============================



# index.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/

# DDN CLI: Commands

## Overview

The DDN CLI utilizes a typical command / flag syntax enabling you to quickly manage your supergraph. A typical command
structure will follow this pattern:

```bash
ddn <command> <subcommand> <argument> --<flag> "<flag_value>"
```

:::info Note

A complete list of all CLI commands, including details and examples, can be found in the side navigation to the left
under the Commands heading.

:::

## Environment variables

You can configure additional behavior for the CLI using environment variables.

### `HASURA_DDN_PROJECT_DIRECTORY`

Set the `HASURA_DDN_PROJECT_DIRECTORY` environment variable to the path of your project directory, which contains the `hasura.yaml` file. This is helpful when scripting automations and ensures that any scripts or commands run from different locations will automatically reference the correct project configuration.

## Getting help

You can use the `--help` flag to get information on any command.

For example, running `ddn --help` will return information on all available operations and flags:

```

DDDDDDD\   DDDDDDD\   NN\   NN\
DD  __DD\  DD  __DD\  NNN\  NN |
DD |  DD | DD |  DD | NNNN\ NN |
DD |  DD | DD |  DD | NN NN\NN |
DD |  DD | DD |  DD | NN \NNNN |
DD |  DD | DD |  DD | NN |\NNN |
DDDDDDD  | DDDDDDD  | NN | \NN |
\_______/  \_______/  \__|  \__|

Usage:
  ddn [flags]
  ddn [command]

DDN operations
  project        Manage Hasura DDN Project

Metadata operations
  command        Perform Command related operations
  connector      Perform Connector related operations
  connector-link Perform DataConnectorLink related operations
  model          Perform Model related operations
  relationship   Perform Relationship related operations
  subgraph       Perform Subgraph related operations
  supergraph     Perform Supergraph related operations

Authentication operations
  auth           Manage Hasura DDN CLI Auth

Other operations
  codemod        Perform transformations on your Hasura project directory
  completion     Generate the autocompletion script for the specified shell
  console        Open the DDN console
  context        Perform context operations
  help           Help about any command
  plugins        Manage plugins for the CLI
  run            Run specific script from project's context config
  update-cli     Update this CLI to the latest version or to a specific version
  version        Prints the CLI version

Flags:
  -h, --help               help for ddn
      --log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
      --no-prompt          Do not prompt for required but missing flags
      --out string         Output format. Can be table, json or yaml. (default "table")
      --timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
  -v, --version            Prints the CLI version

Use "ddn [command] --help" for more information about a command.

```



==============================



# ddn.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn

# DDN CLI: ddn

DDN Command Line Interface.

## Synopsis

```

       

DDDDDDD\   DDDDDDD\   NN\   NN\ 
DD  __DD\  DD  __DD\  NNN\  NN |
DD |  DD | DD |  DD | NNNN\ NN |
DD |  DD | DD |  DD | NN NN\NN |
DD |  DD | DD |  DD | NN \NNNN |
DD |  DD | DD |  DD | NN |\NNN |
DDDDDDD  | DDDDDDD  | NN | \NN |
\_______/  \_______/  \__|  \__|



```

```bash
ddn [flags]
```

## Available operations

- [ddn auth](/reference/cli/commands/ddn_auth) - Manage Hasura DDN CLI Auth
- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory
- [ddn command](/reference/cli/commands/ddn_command) - Perform Command-related operations
- [ddn connector](/reference/cli/commands/ddn_connector) - Perform Connector related operations
- [ddn connector-link](/reference/cli/commands/ddn_connector-link) - Perform DataConnectorLink related operations
- [ddn console](/reference/cli/commands/ddn_console) - Open the DDN console
- [ddn context](/reference/cli/commands/ddn_context) - Perform context operations
- [ddn doctor](/reference/cli/commands/ddn_doctor) - Check if the dependencies of DDN CLI are present
- [ddn model](/reference/cli/commands/ddn_model) - Perform Model-related operations
- [ddn plugins](/reference/cli/commands/ddn_plugins) - Manage plugins for the CLI
- [ddn project](/reference/cli/commands/ddn_project) - Manage Hasura DDN Project
- [ddn relationship](/reference/cli/commands/ddn_relationship) - Perform Relationship related operations
- [ddn run](/reference/cli/commands/ddn_run) - Run specific script from project's context config
- [ddn subgraph](/reference/cli/commands/ddn_subgraph) - Perform Subgraph-related operations
- [ddn supergraph](/reference/cli/commands/ddn_supergraph) - Perform Supergraph-related operations

## Options

```sass
-h, --help               help for ddn
    --log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
    --no-prompt          Do not prompt for required but missing flags
    --out string         Output format. Can be table, json or yaml. (default "table")
    --timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```



==============================



# ddn_auth.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_auth

# DDN CLI: ddn auth

Manage Hasura DDN CLI Auth.

## Synopsis

The auth commands allow you to manage the authentication status of your CLI using a browser or an access token. Additionally, you can generate and print secret keys for a PromptQL instance if it's associated with the project.

## Available operations

- [ddn auth generate-promptql-secret-key](/reference/cli/commands/ddn_auth_generate-promptql-secret-key) - Generates the project's PromptQL secret key and saves to global config
- [ddn auth login](/reference/cli/commands/ddn_auth_login) - Login to DDN
- [ddn auth logout](/reference/cli/commands/ddn_auth_logout) - Logout from DDN
- [ddn auth print-access-token](/reference/cli/commands/ddn_auth_print-access-token) - Prints the access token to STDOUT
- [ddn auth print-promptql-secret-key](/reference/cli/commands/ddn_auth_print-promptql-secret-key) - Prints the project's PromptQL secret key to STDOUT

## Options

```sass
-h, --help   help for auth
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



==============================



# ddn_auth_generate-promptql-secret-key.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_auth_generate-promptql-secret-key

# DDN CLI: ddn auth generate-promptql-secret-key

Generates the project's PromptQL secret key and saves to global config.

## Synopsis

Generates the project's PromptQL secret key and saves to global config

```bash
ddn auth generate-promptql-secret-key [flags]
```

## Examples

```bash
# Generates a PromptQL secret key for the project set in the context and saves to global config
 ddn auth generate-promptql-secret-key
```

## Options

```sass
    --ci               Disables the use of context
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for generate-promptql-secret-key
-p, --project string   DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn auth](/reference/cli/commands/ddn_auth) - Manage Hasura DDN CLI Auth



==============================



# ddn_auth_login.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_auth_login

# DDN CLI: ddn auth login

Login to DDN.

## Synopsis

Use this command to authenticate your DDN CLI with your Hasura Cloud account. By passing the --access-token flag, you can bypass the need for a browser, which is optimal for headless and CI environments.

```bash
ddn auth login [flags]
```

## Examples

```bash
# Login with browser
 ddn auth login

# Login with Personal Access Token
 ddn auth login --access-token <your-access-token>
```

## Options

```sass
    --access-token string   The Personal Access Token or Service Account Token [env: HASURA_DDN_ACCESS_TOKEN]
-h, --help                  help for login
    --pat string            Personal Access token [env: HASURA_DDN_PAT] (DEPRECATED: use --access-token instead)
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn auth](/reference/cli/commands/ddn_auth) - Manage Hasura DDN CLI Auth



==============================



# ddn_auth_logout.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_auth_logout

# DDN CLI: ddn auth logout

Logout from DDN.

## Synopsis

Use this command to logout your CLI session from your Hasura Cloud account.

```bash
ddn auth logout [flags]
```

## Examples

```bash
# Logout from DDN CLI
 ddn auth logout
```

## Options

```sass
-h, --help   help for logout
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn auth](/reference/cli/commands/ddn_auth) - Manage Hasura DDN CLI Auth



==============================



# ddn_auth_print-access-token.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_auth_print-access-token

# DDN CLI: ddn auth print-access-token

Prints the access token to STDOUT.

## Synopsis

Your access token is stored locally and is only available after authenticating your CLI using one of the options via the ddn auth login command.

```bash
ddn auth print-access-token [flags]
```

**Alias:** pp, print-pat

## Examples

```bash
# Print the access token as a string to STDOUT
 ddn auth print-access-token

# Print access token as a JSON to STDOUT
 ddn auth print-access-token --out json
```

## Options

```sass
-h, --help   help for print-access-token
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn auth](/reference/cli/commands/ddn_auth) - Manage Hasura DDN CLI Auth



==============================



# ddn_auth_print-promptql-secret-key.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_auth_print-promptql-secret-key

# DDN CLI: ddn auth print-promptql-secret-key

Prints the project's PromptQL secret key to STDOUT.

## Synopsis

Your project's PromptQL secret is stored locally and is only available after enabling PromptQL for a local project and attaching it to a Hasura Cloud project.

```bash
ddn auth print-promptql-secret-key [flags]
```

**Alias:** pp

## Examples

```bash
# Print the PromptQL secret key of the project set in the context
 ddn auth print-promptql-secret-key

# Print PromptQL secret key as JSON
 ddn auth print-promptql-secret-key --out json
```

## Options

```sass
    --ci               Disables the use of context
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for print-promptql-secret-key
-p, --project string   DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn auth](/reference/cli/commands/ddn_auth) - Manage Hasura DDN CLI Auth



==============================



# ddn_codemod.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_codemod

# DDN CLI: ddn codemod

Perform transformations on your Hasura project directory.

## Synopsis

From time to time, the configuration and features available to a local project will evolve. This family of commands allows you to quickly and easily update your local project to enable new features and conventions.

## Available operations

- [ddn codemod configure-header-forwarding](/reference/cli/commands/ddn_codemod_configure-header-forwarding) - Configure headers to be forwarded to your connector
- [ddn codemod fix-traces-env-var](/reference/cli/commands/ddn_codemod_fix-traces-env-var) - Fix env var used for configuring traces for connectors
- [ddn codemod rename-graphql-prefixes](/reference/cli/commands/ddn_codemod_rename-graphql-prefixes) - Rename GraphQL root field and type name prefixes in metadata
- [ddn codemod upgrade-auth-config-to-v3](/reference/cli/commands/ddn_codemod_upgrade-auth-config-to-v3) - Upgrade AuthConfig version from "v1"/"v2" to "v3"
- [ddn codemod upgrade-context-v2-to-v3](/reference/cli/commands/ddn_codemod_upgrade-context-v2-to-v3) - Upgrade project's context config from v2 to v3
- [ddn codemod upgrade-graphqlconfig-aggregate](/reference/cli/commands/ddn_codemod_upgrade-graphqlconfig-aggregate) - Upgrade GraphqlConfig to support aggregates
- [ddn codemod upgrade-graphqlconfig-subscriptions](/reference/cli/commands/ddn_codemod_upgrade-graphqlconfig-subscriptions) - Upgrade GraphqlConfig to support subscriptions
- [ddn codemod upgrade-model-v1-to-v2](/reference/cli/commands/ddn_codemod_upgrade-model-v1-to-v2) - Upgrade model from "version" "v1" to "v2" in metadata
- [ddn codemod upgrade-object-boolean-expression-types](/reference/cli/commands/ddn_codemod_upgrade-object-boolean-expression-types) - Upgrade object boolean expression types metadata
- [ddn codemod upgrade-project-config-v2-to-v3](/reference/cli/commands/ddn_codemod_upgrade-project-config-v2-to-v3) - Upgrade project directory from version v2 to v3
- [ddn codemod upgrade-supergraph-config-v1-to-v2](/reference/cli/commands/ddn_codemod_upgrade-supergraph-config-v1-to-v2) - Upgrade all Supergraph config files at the root of the project directory from v1 to v2

## Options

```sass
-h, --help   help for codemod
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



==============================



# ddn_codemod_configure-header-forwarding.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_codemod_configure-header-forwarding

# DDN CLI: ddn codemod configure-header-forwarding

Configure headers to be forwarded to your connector.

## Synopsis

This codemod updates your local project's metadata to forward headers from the initial request against your API to the specified connector.

```bash
ddn codemod configure-header-forwarding connector-name [flags]
```

## Examples

```bash
# Configure header forwarding for the DataConnectorLink corresponding to Connector `mypostgres`
 ddn codemod configure-header-forwarding mypostgres

# Configure header forwarding for the DataConnectorLink corresponding to Connector config file ./connector.yaml
 ddn codemod configure-header-forwarding --connector ./connector.yaml

# Configure header forwarding for the DataConnectorLink `mypostgres`
 ddn codemod configure-header-forwarding --connector-link mypostgres
```

## Options

```sass
    --connector string        Path to Connector YAML config file
    --connector-link string   Name of the DataConnectorLink to configure
-h, --help                    help for configure-header-forwarding
    --subgraph string         Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory



==============================



# ddn_codemod_enable-promptql.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_codemod_enable-promptql

# DDN CLI: ddn codemod enable-promptql

Enable PromptQL in an existing project.

## Synopsis

Enable PromptQL in an existing project

```bash
ddn codemod enable-promptql --dir <project-dir> [flags]
```

## Examples

```bash
# Enable PromptQL in an existing project
 ddn codemod enable-promptql
```

## Options

```sass
    --ci                         Disables the use of context
    --compose-file-path string   Path to the docker compose file used to run engine
-c, --context string             Name of the context to use. (default <current_context>)
    --dir string                 The Hasura project directory (default ".")
-f, --force                      Run the command without asking for confirmation
-h, --help                       help for enable-promptql
    --subgraph string            Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory



==============================



# ddn_codemod_fix-traces-env-var.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_codemod_fix-traces-env-var

# DDN CLI: ddn codemod fix-traces-env-var

Fix env var used for configuring traces for connectors.

## Synopsis

Fix env var used for configuring traces for connectors

```bash
ddn codemod fix-traces-env-var --dir <project-dir> [flags]
```

## Examples

```bash
# Fix env var used for configuring traces for connector
 ddn codemod fix-traces-env-var --dir .
```

## Options

```sass
    --dir string   The Hasura project directory (required)
-h, --help         help for fix-traces-env-var
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory



==============================



# ddn_codemod_rename-graphql-prefixes.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_codemod_rename-graphql-prefixes

# DDN CLI: ddn codemod rename-graphql-prefixes

Rename GraphQL root field and type name prefixes in metadata.

## Synopsis

Rename GraphQL root field and type name prefixes in metadata. The from prefix will be stripped if provided, and the new prefix will be added. If the new prefix is already present, it will not be reapplied.

By default, subgraph.yaml is updated with the new prefixes.

```bash
ddn codemod rename-graphql-prefixes [flags]
```

## Examples

```bash
# Add root field and type name prefixes to the subgraph set in the context
 ddn codemod rename-graphql-prefixes --graphql-root-field 'app_' --graphql-type-name 'App_'

# Change the root field prefix for the specified subgraph without modifying subgraph.yaml
 ddn codemod rename-graphql-prefixes --subgraph app/subgraph.yaml --graphql-root-field 'foo_' --from-graphql-root-field 'app_' --no-update-subgraph-config
```

## Options

```sass
    --ci                               Disables the use of context
-c, --context string                   Name of the context to use. (default <current_context>)
    --from-graphql-root-field string   The previous GraphQL root field prefix
    --from-graphql-type-name string    The previous GraphQL type name prefix
    --graphql-root-field string        The new GraphQL root field prefix
    --graphql-type-name string         The new GraphQL type name prefix
-h, --help                             help for rename-graphql-prefixes
    --no-update-subgraph-config        Do not update the subgraph config with the new prefixes
    --subgraph string                  Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory



==============================



# ddn_codemod_upgrade-auth-config-to-v3.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_codemod_upgrade-auth-config-to-v3

# DDN CLI: ddn codemod upgrade-auth-config-to-v3

Upgrade AuthConfig version from "v1"/"v2" to "v3".

## Synopsis

Upgrade AuthConfig version from "v1"/"v2" to "v3"

```bash
ddn codemod upgrade-auth-config-to-v3 [flags]
```

## Examples

```bash
# Run on the supergraph defined in the context. Finds `AuthConfig` present in any subgraphs within the supergraph and modifies it to version v3
 ddn codemod upgrade-auth-config-to-v3

# Run on a specific supergraph config file. Finds `AuthConfig` present in any subgraphs within the supergraph and modifies it to version v3
 ddn codemod upgrade-auth-config-to-v3 --supergraph ./supergraph.cloud.yaml

# Run on a specific subgraph config file. Finds `AuthConfig` present in the subgraph and modifies it to version v3
 ddn codemod upgrade-auth-config-to-v3 --subgraph app/subgraph.yaml
```

## Options

```sass
    --ci                  Disables the use of context
-c, --context string      Name of the context to use. (default <current_context>)
-h, --help                help for upgrade-auth-config-to-v3
    --subgraph string     Path to Subgraph config file
    --supergraph string   Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory



==============================



# ddn_codemod_upgrade-context-v2-to-v3.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_codemod_upgrade-context-v2-to-v3

# DDN CLI: ddn codemod upgrade-context-v2-to-v3

Upgrade project's context config from v2 to v3.

## Synopsis

For more information on what's changed, check out the docs: https://hasura.io/docs/3.0/project-configuration/upgrading-project-config/upgrade-context-v3

```bash
ddn codemod upgrade-context-v2-to-v3 --dir <project-dir> [flags]
```

## Examples

```bash
# Upgrade context present in the current Hasura directory from v2 to v3
 ddn codemod upgrade-context-v2-to-v3 --dir .
```

## Options

```sass
    --dir string   The Hasura project directory (required)
-h, --help         help for upgrade-context-v2-to-v3
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory



==============================



# ddn_codemod_upgrade-graphqlconfig-aggregate.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_codemod_upgrade-graphqlconfig-aggregate

# DDN CLI: ddn codemod upgrade-graphqlconfig-aggregate

Upgrade GraphqlConfig to support aggregates.

## Synopsis

For older projects, this codemod will modify your GraphqlConfig's metadata to include support for aggregate types.

```bash
ddn codemod upgrade-graphqlconfig-aggregate [flags]
```

## Examples

```bash
# Run on the supergraph defined in the context
 ddn codemod upgrade-graphqlconfig-aggregate

# Run on a specific supergraph
 ddn codemod upgrade-graphqlconfig-aggregate --supergraph ./supergraph.cloud.yaml

# Run on a specific subgraph
 ddn codemod upgrade-graphqlconfig-aggregate --subgraph app/subgraph.yaml
```

## Options

```sass
    --ci                  Disables the use of context
-c, --context string      Name of the context to use. (default <current_context>)
-h, --help                help for upgrade-graphqlconfig-aggregate
    --subgraph string     Path to Subgraph config file
    --supergraph string   Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory



==============================



# ddn_codemod_upgrade-graphqlconfig-subscriptions.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_codemod_upgrade-graphqlconfig-subscriptions

# DDN CLI: ddn codemod upgrade-graphqlconfig-subscriptions

Upgrade GraphqlConfig to support subscriptions.

## Synopsis

For older projects, this codemod will modify your GraphqlConfig's metadata to include support for subscriptions.

```bash
ddn codemod upgrade-graphqlconfig-subscriptions [flags]
```

## Examples

```bash
# Run on the supergraph defined in the context
 ddn codemod upgrade-graphqlconfig-subscriptions

# Run on a specific supergraph
 ddn codemod upgrade-graphqlconfig-subscriptions --supergraph ./supergraph.cloud.yaml

# Run on a specific subgraph
 ddn codemod upgrade-graphqlconfig-subscriptions --subgraph app/subgraph.yaml
```

## Options

```sass
    --ci                  Disables the use of context
-c, --context string      Name of the context to use. (default <current_context>)
-h, --help                help for upgrade-graphqlconfig-subscriptions
    --subgraph string     Path to Subgraph config file
    --supergraph string   Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory



==============================



# ddn_codemod_upgrade-model-v1-to-v2.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_codemod_upgrade-model-v1-to-v2

# DDN CLI: ddn codemod upgrade-model-v1-to-v2

Upgrade model from "version" "v1" to "v2" in metadata.

## Synopsis

This codemod wil update the model "version" for each model in the supergraph's current context.

```bash
ddn codemod upgrade-model-v1-to-v2 [flags]
```

## Examples

```bash
# Run on the supergraph defined in the context
 ddn codemod upgrade-model-v1-to-v2

# Run on a specific supergraph
 ddn codemod upgrade-model-v1-to-v2 --supergraph ./supergraph.cloud.yaml

# Run on a specific subgraph
 ddn codemod upgrade-model-v1-to-v2 --subgraph app/subgraph.yaml
```

## Options

```sass
    --ci                  Disables the use of context
-c, --context string      Name of the context to use. (default <current_context>)
-h, --help                help for upgrade-model-v1-to-v2
    --subgraph string     Path to Subgraph config file
    --supergraph string   Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory



==============================



# ddn_codemod_upgrade-object-boolean-expression-types.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_codemod_upgrade-object-boolean-expression-types

# DDN CLI: ddn codemod upgrade-object-boolean-expression-types

Upgrade object boolean expression types metadata.

## Synopsis

For older projects, this codemod will modify your metadata to include support for object boolean expression types.

```bash
ddn codemod upgrade-object-boolean-expression-types [flags]
```

## Examples

```bash
# Run on the supergraph defined in the context
 ddn codemod upgrade-object-boolean-expression-types

# Run on a specific supergraph
 ddn codemod upgrade-object-boolean-expression-types --supergraph ./supergraph.cloud.yaml

# Run on a specific subgraph
 ddn codemod upgrade-object-boolean-expression-types --subgraph app/subgraph.yaml
```

## Options

```sass
    --ci                  Disables the use of context
-c, --context string      Name of the context to use. (default <current_context>)
-h, --help                help for upgrade-object-boolean-expression-types
    --subgraph string     Path to Subgraph config file
    --supergraph string   Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory



==============================



# ddn_codemod_upgrade-project-config-v2-to-v3.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_codemod_upgrade-project-config-v2-to-v3

# DDN CLI: ddn codemod upgrade-project-config-v2-to-v3

Upgrade project directory from version v2 to v3.

## Synopsis

For more information on what's changed, check out the docs: https://hasura.io/docs/3.0/project-configuration/upgrading-project-config/upgrade-project-v3

```bash
ddn codemod upgrade-project-config-v2-to-v3 --dir <project-dir> [flags]
```

## Examples

```bash
# Upgrade project in the current directory from v2 to v3
 ddn codemod upgrade-project-config-v2-to-v3 --dir .
```

## Options

```sass
    --dir string   The Hasura project directory (required)
-h, --help         help for upgrade-project-config-v2-to-v3
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory



==============================



# ddn_codemod_upgrade-supergraph-config-v1-to-v2.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_codemod_upgrade-supergraph-config-v1-to-v2

# DDN CLI: ddn codemod upgrade-supergraph-config-v1-to-v2

Upgrade all Supergraph config files at the root of the project directory from v1 to v2.

## Synopsis

For more information on what's changed, check out the docs: https://hasura.io/docs/3.0/project-configuration/upgrading-project-config/upgrade-project-v2

```bash
ddn codemod upgrade-supergraph-config-v1-to-v2 --dir <project-dir> [flags]
```

## Examples

```bash
# Upgrade all Supergraph config files in the current directory from v1 to v2
 ddn codemod upgrade-supergraph-config-v1-to-v2 --dir .
```

## Options

```sass
    --dir string   The Hasura project directory (required)
-h, --help         help for upgrade-supergraph-config-v1-to-v2
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory



==============================



# ddn_command.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_command

# DDN CLI: ddn command

Perform Command-related operations.

## Synopsis

Commands represent actions that can be executed within a data domain. To learn more, check out the docs: https://hasura.io/docs/3.0/reference/metadata-reference/commands/

**Alias:** commands

## Available operations

- [ddn command add](/reference/cli/commands/ddn_command_add) - Add new Commands to the local metadata
- [ddn command list](/reference/cli/commands/ddn_command_list) - Lists details about the functions/procedures of DataConnectorLink, and their corresponding Commands
- [ddn command remove](/reference/cli/commands/ddn_command_remove) - Removes Commands (and related metadata) in the local metadata
- [ddn command show](/reference/cli/commands/ddn_command_show) - Show diff between the command and its corresponding ndc function/procedure
- [ddn command update](/reference/cli/commands/ddn_command_update) - Update Commands in the local metadata

## Options

```sass
-h, --help   help for command
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



==============================



# ddn_command_add.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_command_add

# DDN CLI: ddn command add

Add new Commands to the local metadata.

## Synopsis

When adding a command to your local metadata, the CLI will generate boilerplate metadata that — when the supergraph is built — will make it available to the exposed API.

```bash
ddn command add <connector-link-name> <procedure/function-name> [flags]
```

## Examples

```bash
# Add all Commands for DataConnectorLink "myfns" in the subgraph set in the context
 ddn command add myfns "*"

# Add a new Command "Login" from the procedure "Login" in the DataConnectorLink "myfns" in "app" Subgraph
 ddn command add myfns Login --subgraph ./app/subgraph.yaml --supergraph ./supergraph.yaml

# Add all the Commands from the procedures/functions in the DataConnectorLink "myfns" in "app" Subgraph
 ddn command add myfns "*" --subgraph ./app/subgraph.yaml --supergraph ./supergraph.yaml

# Add Commands filtered by glob pattern from the procedures/functions in the DataConnectorLink "myfns" in "app" Subgraph
 ddn command add myfns "user*" --subgraph ./app/subgraph.yaml
```

## Options

```sass
    --ci                  Disables the use of context
-c, --context string      Name of the context to use. (default <current_context>)
-h, --help                help for add
    --pattern string      Pattern to detect targets. Can be 'glob' or 'literal'. (default "glob")
    --subgraph string     Path to Subgraph config file
    --supergraph string   Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn command](/reference/cli/commands/ddn_command) - Perform Command-related operations



==============================



# ddn_command_list.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_command_list

# DDN CLI: ddn command list

Lists details about the functions/procedures of DataConnectorLink, and their corresponding Commands.

## Synopsis

Lists details about the functions/procedures of DataConnectorLink, and their corresponding Commands

```bash
ddn command list <connector-link-name> [flags]
```

## Examples

```bash
# List details about the functions/procedures of DataConnectorLink `mydb`, and their corresponding Commands
 ddn command list mydb
```

## Options

```sass
    --ci                Disables the use of context
-c, --context string    Name of the context to use. (default <current_context>)
-h, --help              help for list
    --subgraph string   Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn command](/reference/cli/commands/ddn_command) - Perform Command-related operations



==============================



# ddn_command_remove.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_command_remove

# DDN CLI: ddn command remove

Removes Commands (and related metadata) in the local metadata.

## Synopsis

Removes Commands (and related metadata) in the local metadata

```bash
ddn command remove <command-name> [flags]
```

## Examples

```bash
# Remove all Commands using the subgraph set in current context
 ddn command remove "*"

# Remove the Command "Album" in the "app" Subgraph
 ddn command remove Album --subgraph ./app/subgraph.yaml --supergraph ./supergraph.yaml

# Remove all the Commands in the Subgraph "app"
 ddn command remove "*" --subgraph ./app/subgraph.yaml --supergraph ./supergraph.yaml

# Remove Command filtered by glob pattern in the Subgraph "app"
 ddn command remove "user*" --subgraph ./app/subgraph.yaml --supergraph ./supergraph.yaml
```

## Options

```sass
    --ci                  Disables the use of context
-c, --context string      Name of the context to use. (default <current_context>)
-h, --help                help for remove
    --pattern string      Pattern to detect targets. Can be 'glob' or 'literal'. (default "glob")
    --subgraph string     Path to Subgraph config file
    --supergraph string   Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn command](/reference/cli/commands/ddn_command) - Perform Command-related operations



==============================



# ddn_command_show.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_command_show

# DDN CLI: ddn command show

Show diff between the command and its corresponding ndc function/procedure.

## Synopsis

Show diff between the command and its corresponding ndc function/procedure

```bash
ddn command show <command-name> [flags]
```

## Examples

```bash
# Show diff between the 'InsertUsers' command and its corresponding ndc function/procedure
 ddn command show InsertUsers

# Show diff between commands and their corresponding ndc functions/procedures for all commands matching glob pattern 'Delete*'
 ddn command show "Delete*"
```

## Options

```sass
    --ci                Disables the use of context
-c, --context string    Name of the context to use. (default <current_context>)
-h, --help              help for show
    --pattern string    Pattern to detect targets. Can be 'glob' or 'literal'. (default "glob")
    --subgraph string   Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn command](/reference/cli/commands/ddn_command) - Perform Command-related operations



==============================



# ddn_command_update.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_command_update

# DDN CLI: ddn command update

Update Commands in the local metadata.

## Synopsis

When the underlying data source changes, it's necessary to update the command's metadata to ensure Hasura DDN is aware of any modifications. Typically, before updating a command, you'll have re-introspected the underlying data source.

```bash
ddn command update <command-name> [flags]
```

## Examples

```bash
# Update all Commands using the subgraph set in current context
 ddn command update "*"

# Update the Command "Login" in the "app" Subgraph
 ddn command update Login --subgraph ./app/subgraph.yaml --supergraph ./supergraph.yaml

# Update all the Commands in "app" Subgraph
 ddn command update "*" --subgraph ./app/subgraph.yaml --supergraph ./supergraph.yaml

# Update Commands filtered by glob pattern in the Subgraph "app"
 ddn command update "user*" --subgraph ./app/subgraph.yaml --supergraph ./supergraph.yaml
```

## Options

```sass
    --ci                  Disables the use of context
-c, --context string      Name of the context to use. (default <current_context>)
-h, --help                help for update
    --pattern string      Pattern to detect targets. Can be 'glob' or 'literal'. (default "glob")
    --subgraph string     Path to Subgraph config file
    --supergraph string   Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn command](/reference/cli/commands/ddn_command) - Perform Command-related operations



==============================



# ddn_connector.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_connector

# DDN CLI: ddn connector

Perform Connector related operations.

## Synopsis

Native Data Connectors (or connectors for short) are the link between your data source(s) and your API. To learn more, check out the docs: https://hasura.io/docs/3.0/data-sources/overview/

**Alias:** connectors

## Available operations

- [ddn connector build](/reference/cli/commands/ddn_connector_build) - Perform ConnectorBuild-related operations
- [ddn connector env](/reference/cli/commands/ddn_connector_env) - Manage environment variables for Connectors
- [ddn connector init](/reference/cli/commands/ddn_connector_init) - Add a new Connector
- [ddn connector introspect](/reference/cli/commands/ddn_connector_introspect) - Introspect the Connector data source to update the Connector configuration
- [ddn connector list](/reference/cli/commands/ddn_connector_list) - List available versions of connectors
- [ddn connector plugin](/reference/cli/commands/ddn_connector_plugin) - Run a subcommand from a Connector plugin
- [ddn connector remove](/reference/cli/commands/ddn_connector_remove) - Remove a Connector, its corresponding DataConnectorLink and all its associated metadata objects
- [ddn connector setenv](/reference/cli/commands/ddn_connector_setenv) - Run specified command with environment variables set
- [ddn connector show-resources](/reference/cli/commands/ddn_connector_show-resources) - Show resources of a Connector
- [ddn connector upgrade](/reference/cli/commands/ddn_connector_upgrade) - Upgrade connector configuration

## Options

```sass
-h, --help   help for connector
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



==============================



# ddn_connector_build.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_connector_build

# DDN CLI: ddn connector build

Perform ConnectorBuild-related operations.

## Synopsis

The lifecycle of a connector can be independent of the supergraph of which it is a part. These commands allow you to easily create, delete, and inspect builds for a connector.

## Available operations

- [ddn connector build create](/reference/cli/commands/ddn_connector_build_create) - Create a ConnectorBuild on Hasura DDN
- [ddn connector build delete](/reference/cli/commands/ddn_connector_build_delete) - Delete a ConnectorBuild from a Project
- [ddn connector build get](/reference/cli/commands/ddn_connector_build_get) - List ConnectorBuilds or get details of a specific one from Hasura DDN
- [ddn connector build logs](/reference/cli/commands/ddn_connector_build_logs) - Get logs of a ConnectorBuild from Hasura DDN

## Options

```sass
-h, --help   help for build
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector](/reference/cli/commands/ddn_connector) - Perform Connector related operations



==============================



# ddn_connector_build_create.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_connector_build_create

# DDN CLI: ddn connector build create

Create a ConnectorBuild on Hasura DDN.

## Synopsis

Create a ConnectorBuild on Hasura DDN

```bash
ddn connector build create --connector <path-to-connector-config-file-.yaml> [flags]
```

## Examples

```bash
# Create a ConnectorBuild
 ddn connector build create --connector ./app/connector/my_connector/connector.yaml
```

## Options

```sass
    --ci                             Disables the use of context
    --connector string               Path to Connector YAML config file
-c, --context string                 Name of the context to use. (default <current_context>)
-e, --env stringArray                Environment variable, e.g. key=val. Can be repeated to provide multiple env vars
    --env-file stringArray           Path to .env file. Can be repeated to provide multiple env files
-h, --help                           help for create
-p, --project string                 DDN Project name
    --target-connector-link string   DataConnectorLink to update with the schema from the ConnectorBuild
    --target-env-file string         Path to the env file in which the Build URLs should be updated in
    --target-subgraph string         Path to Subgraph config file containing target DataConnectorLink
    --update-connector-link-schema   Update DataConnectorLink schema with the NDC schema of the built connector. (default: false)
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector build](/reference/cli/commands/ddn_connector_build) - Perform ConnectorBuild-related operations



==============================



# ddn_connector_build_delete.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_connector_build_delete

# DDN CLI: ddn connector build delete

Delete a ConnectorBuild from a Project.

## Synopsis

Delete a ConnectorBuild from a Project

```bash
ddn connector build delete <connector-build-id> [flags]
```

## Examples

```bash
# Delete a ConnectorBuild
 ddn connector build delete 1d4f4831-54a2-4ded-b680-07d00510a522
```

## Options

```sass
-h, --help   help for delete
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector build](/reference/cli/commands/ddn_connector_build) - Perform ConnectorBuild-related operations



==============================



# ddn_connector_build_get.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_connector_build_get

# DDN CLI: ddn connector build get

List ConnectorBuilds or get details of a specific one from Hasura DDN.

## Synopsis

List ConnectorBuilds or get details of a specific one from Hasura DDN

```bash
ddn connector build get [connector-build-id] [flags]
```

## Examples

```bash
# Get details of a ConnectorBuild
 ddn connector build get 1d4f4831-54a2-4ded-b680-07d00510a522

# Get details of all ConnectorBuilds for Connector "mydb" for a Project and Subgraph
 ddn connector build get --connector-name mydb --project myproject --subgraph-name myapp

# Get details of all ConnectorBuilds for Connector defined in a Connector config file
 ddn connector build get --connector ./myapp/connector/connector.cloud.yaml

# Get details of all ConnectorBuilds for a Project
 ddn connector build get --project myproject

# Get details of all ConnectorBuilds for a Subgraph in a Project
 ddn connector build get --project myproject --subgraph-name myapp

# Get NDC Schema of a ConnectorBuild
 ddn connector build get 1d4f4831-54a2-4ded-b680-07d00510a522 --schema
```

## Options

```sass
    --ci                      Disables the use of context
    --connector string        Path to Connector YAML config file
    --connector-name string   Connector name
-c, --context string          Name of the context to use. (default <current_context>)
-h, --help                    help for get
-p, --project string          DDN Project name
    --schema                  Get NDC schema of ConnectorBuild
    --subgraph-name string    Subgraph name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector build](/reference/cli/commands/ddn_connector_build) - Perform ConnectorBuild-related operations



==============================



# ddn_connector_build_logs.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_connector_build_logs

# DDN CLI: ddn connector build logs

Get logs of a ConnectorBuild from Hasura DDN.

## Synopsis

Get logs of a ConnectorBuild from Hasura DDN

```bash
ddn connector build logs <connector-build-id> [flags]
```

## Examples

```bash
# Get running deploy logs
 ddn connector build logs <connector-build-id>

# Get running deploy logs and keep following
 ddn connector build logs <connector-build-id> --follow

# Get running deploy logs and keep following since a specified time duration
 ddn connector build logs <connector-build-id> --follow --since 10m

# Get build logs of a ConnectorBuild
 ddn connector build logs <connector-build-id> --build
```

## Options

```sass
    --build          Specifies whether to show the build logs. (default: false)
    --follow         Specifies whether to continuously follow the deployment logs. (default: false)
-h, --help           help for logs
    --since string   Specifies the starting time for log retrieval. Can be in ISO format (e.g. 2024-03-26T11:05:15Z) or a duration (e.g. 5m for 5 minutes ago). (By default, prints the entire available logs).
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector build](/reference/cli/commands/ddn_connector_build) - Perform ConnectorBuild-related operations



==============================



# ddn_connector_env.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_connector_env

# DDN CLI: ddn connector env

Manage environment variables for Connectors.

## Synopsis

Manage environment variables for Connectors

## Available operations

- [ddn connector env add](/reference/cli/commands/ddn_connector_env_add) - Add environment variables and its corresponding mapping to a Connector

## Options

```sass
-h, --help   help for env
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector](/reference/cli/commands/ddn_connector) - Perform Connector related operations



==============================



# ddn_connector_env_add.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_connector_env_add

# DDN CLI: ddn connector env add

Add environment variables and its corresponding mapping to a Connector.

## Synopsis

This convenience command makes it easy to add environment variables (such as to the NodeJS Lambda connector) which can then be referenced in the connector's code.

```bash
ddn connector env add [flags]
```

## Examples

```bash
# Add new environment variable to Connector mydb
 ddn connector env add mydb --env NEW_VAR=value

# Add new environment variables to a specific env file
 ddn connector env add mydb --subgraph ./subgraph/subgraph.yaml --env VAR1=VAL1 --env VAR2=VAL2 --target-env-file .env.local

# Add new environment variable to Connector located at ./foo/my_db/connector.yaml
 ddn connector env add --connector ./foo/my_db/connector.yaml --env NEW_VAR=value
```

## Options

```sass
    --ci                            Disables the use of context
    --connector string              Path to Connector YAML config file
-c, --context string                Name of the context to use. (default <current_context>)
-e, --env stringArray               Environment variable, e.g. key=val. Can be repeated to provide multiple env vars
-h, --help                          help for add
    --target-env-file stringArray   Path to the specific environment file to update (optional)
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector env](/reference/cli/commands/ddn_connector_env) - Manage environment variables for Connectors



==============================



# ddn_connector_init.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_connector_init

# DDN CLI: ddn connector init

Add a new Connector.

## Synopsis

For each data source you wish to connect to your project and eventually expose via your API, you'll need to initialize a new data connector.

```bash
ddn connector init [connector-name] --hub-connector <connector-type> [flags]
```

## Examples

```bash
# Initialize a Connector interactively in a step by step manner
 ddn connector init -i

# Initialize a Postgres Connector "mydb" in the Subgraph "app"
 ddn connector init mydb --subgraph ./app/subgraph.yaml --hub-connector hasura/postgres

# Initialize a Postgres Connector "mydb" inside the directory ./connector
 ddn connector init mydb --dir ./connector --hub-connector hasura/postgres

# Initialize a NodeJS Connector "mylambda" in the Subgraph "app" on port 8765
 ddn connector init mylambda --subgraph ./app/subgraph.yaml  --hub-connector hasura/nodejs --configure-port 8765 
```

## Options

```sass
    --add-env stringArray            Environment variable to set in the Connector. Can be repeated to provide multiple env vars
    --add-to-compose-file string     The compose file to include the generated connector compose file
    --ci                             Disables the use of context
    --configure-port string          Initialize the connector with an already configured port
-c, --context string                 Name of the context to use. (default <current_context>)
    --dir string                     Directory to initialize the Connector
-h, --help                           help for init
    --hub-connector string           Name and version of Connector in Hasura Connector Hub. ref: https://hasura.io/connectors
-i, --interactive                    Interactive mode
    --no-link                        Do not create a ConnectorLink
    --subgraph string                Subgraph to initialize the Connector in
    --target-cloud-env-file string   Path to the cloud environment file
    --target-local-env-file string   Path to the local environment file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector](/reference/cli/commands/ddn_connector) - Perform Connector related operations



==============================



# ddn_connector_introspect.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_connector_introspect

# DDN CLI: ddn connector introspect

Introspect the Connector data source to update the Connector configuration.

## Synopsis

After you've initialized a data connector, you'll need to run this introspection command to complete or update the local configuration for the connector. The generated configuration file will then be used by the CLI to incorporate various resources (such as models, commands, and relationships) into your API.

```bash
ddn connector introspect <connector-name> --subgraph <path-to-subgraph-config-file> [flags]
```

## Examples

```bash
# Introspect Connector my_db from Subgraph located at ./foo/subgraph.yaml and update DataConnectorLink
 ddn connector introspect my_db --subgraph ./foo/subgraph.yaml

# Introspect Connector located at ./foo/my_db/connector.yaml
 ddn connector introspect --connector ./foo/my_db/connector.yaml

# Introspect Connector my_db but do not update DataConnectorLink
 ddn connector introspect my_db --subgraph ./foo/subgraph.yaml --no-update-link
```

## Options

```sass
    --add-all-resources      Add all Models, Commands and Relationships from the updated DataConnectorLink to the local metadata
    --ci                     Disables the use of context
    --connector string       Path to Connector YAML config file
-c, --context string         Name of the context to use. (default <current_context>)
-e, --env stringArray        Environment variable, e.g. key=val. Can be repeated to provide multiple env vars
    --env-file stringArray   Path to .env file. Can be repeated to provide multiple env files
-h, --help                   help for introspect
    --no-update-link         Ignore updating DataConnectorLink in the metadata
    --subgraph string        Path to Subgraph config file
    --supergraph string      Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector](/reference/cli/commands/ddn_connector) - Perform Connector related operations



==============================



# ddn_connector_list.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_connector_list

# DDN CLI: ddn connector list

List available versions of connectors.

## Synopsis

List available versions of connectors

```bash
ddn connector list [flags]
```

## Examples

```bash
# List all versions of all connectors
 ddn connector list

# List all versions of the Postgres connector
 ddn connector list --hub-connector hasura/postgres
```

## Options

```sass
-h, --help                   help for list
    --hub-connector string   Name of the Connector in Hasura Connector Hub, ref: https://hasura.io/connectors. eg: hasura/nodejs
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector](/reference/cli/commands/ddn_connector) - Perform Connector related operations



==============================



# ddn_connector_plugin.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_connector_plugin

# DDN CLI: ddn connector plugin

Run a subcommand from a Connector plugin.

## Synopsis

Run a subcommand from a Connector plugin

```bash
ddn connector plugin [flags] [-- <args>]
```

## Examples

```bash
# Run the "update" command of a connector
 ddn connector plugin --connector app/connector/mypostgres/connector.yaml -- update
```

## Options

```sass
    --ci                     Disables the use of context
    --connector string       Path to Connector YAML config file
-c, --context string         Name of the context to use. (default <current_context>)
-e, --env stringArray        Environment variable, e.g. key=val. Can be repeated to provide multiple env vars
    --env-file stringArray   Path to .env file. Can be repeated to provide multiple env files
-h, --help                   help for plugin
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector](/reference/cli/commands/ddn_connector) - Perform Connector related operations



==============================



# ddn_connector_remove.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_connector_remove

# DDN CLI: ddn connector remove

Remove a Connector, its corresponding DataConnectorLink and all its associated metadata objects.

## Synopsis

Remove a Connector, its corresponding DataConnectorLink and all its associated metadata objects

```bash
ddn connector remove <connector-name> [flags]
```

**Alias:** rm

## Examples

```bash
# Remove Connector my_db and associated DataConnectorLink from the default subgraph
 ddn connector remove my_db

# Remove Connector my_db from Subgraph located at ./foo/subgraph.yaml and associated DataConnectorLink
 ddn connector remove my_db --subgraph ./foo/subgraph.yaml

# Remove Connector located at ./foo/my_db/connector.yaml with DataConnectorLink given by app/subgraph.yaml and name my_db
 ddn connector remove --connector ./foo/my_db/connector.yaml --subgraph app/subgraph.yaml --connector-link my_db
```

## Options

```sass
    --ci                                Disables the use of context
    --connector string                  Path to Connector YAML config file
    --connector-link string             DataConnectorLink corresponding to the Connector
-c, --context string                    Name of the context to use. (default <current_context>)
-h, --help                              help for remove
    --remove-from-compose-file string   The compose file to include the generated connector compose file
    --subgraph string                   SubgraphConfig file to remove the Connector from
    --target-env-file stringArray       Paths to the .env files
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector](/reference/cli/commands/ddn_connector) - Perform Connector related operations



==============================



# ddn_connector_setenv.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_connector_setenv

# DDN CLI: ddn connector setenv

Run specified command with environment variables set.

## Synopsis

If you wish to run a connector independently of your supergraph (e.g., for testing), you can run a specific command along with any necessary environment variables the connector requires.

```bash
ddn connector setenv --connector <path-to-connector-config-file-.yaml> -- <command> [flags]
```

## Examples

```bash
# Set environment variables for the Connector data source
 ddn connector setenv --connector ./foo/my_db.connector.local.yaml -- npm run start
```

## Options

```sass
    --ci                     Disables the use of context
    --connector string       Path to Connector YAML config file
-c, --context string         Name of the context to use. (default <current_context>)
-e, --env stringArray        Environment variable, e.g. key=val. Can be repeated to provide multiple env vars
    --env-file stringArray   Path to .env file. Can be repeated to provide multiple env files
-h, --help                   help for setenv
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector](/reference/cli/commands/ddn_connector) - Perform Connector related operations



==============================



# ddn_connector_show-resources.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_connector_show-resources

# DDN CLI: ddn connector show-resources

Show resources of a Connector.

## Synopsis

A connector will typically have a number of resources associated with it. This convenience command will show you all available resources (such as models, commands, and relationships) along with their current state.

```bash
ddn connector show-resources <connector-name> [flags]
```

## Examples

```bash
# Show resources for Connector `mydb`
 ddn connector show-resources mydb
```

## Options

```sass
    --ci                Disables the use of context
-c, --context string    Name of the context to use. (default <current_context>)
-h, --help              help for show-resources
    --subgraph string   Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector](/reference/cli/commands/ddn_connector) - Perform Connector related operations



==============================



# ddn_connector_upgrade.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_connector_upgrade

# DDN CLI: ddn connector upgrade

Upgrade connector configuration.

## Synopsis

Connectors are released on their own cadence; new versions may have new features, fixes, or security updates. To upgrade a connector's version to the latest — or a specific — configuration version, use this command.

```bash
ddn connector upgrade [flags]
```

## Examples

```bash
# Upgrade Connector to latest version
 ddn connector upgrade --connector path/to/connector.yaml

# Upgrade Connector to specific version
 ddn connector upgrade --connector path/to/connector.yaml --version v1.2.3
```

## Options

```sass
    --ci                     Disables the use of context
    --connector string       Path to Connector YAML config file
-c, --context string         Name of the context to use. (default <current_context>)
    --env-file stringArray   Path to .env file. Can be repeated to provide multiple env files
-h, --help                   help for upgrade
    --version string         Target version for the connector
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector](/reference/cli/commands/ddn_connector) - Perform Connector related operations



==============================



# ddn_connector-link.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_connector-link

# DDN CLI: ddn connector-link

Perform DataConnectorLink related operations.

## Synopsis

DataConnectorLinks are the bridge between your data source(s) — wired into your API via a Native Data Connector — and your API. To learn more, check out the docs: https://hasura.io/docs/3.0/reference/metadata-reference/data-connector-links/

**Alias:** connector-links

## Available operations

- [ddn connector-link add](/reference/cli/commands/ddn_connector-link_add) - Add a new DataConnectorLink to a Subgraph
- [ddn connector-link add-resources](/reference/cli/commands/ddn_connector-link_add-resources) - Add all models, commands and relationships from a DataConnectorLink's schema
- [ddn connector-link remove](/reference/cli/commands/ddn_connector-link_remove) - Remove a DataConnectorLink from a Subgraph
- [ddn connector-link show](/reference/cli/commands/ddn_connector-link_show) - Show DataConnectorLink details
- [ddn connector-link update](/reference/cli/commands/ddn_connector-link_update) - Fetch NDC details from the Connector and update the DataConnectorLink

## Options

```sass
-h, --help   help for connector-link
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



==============================



# ddn_connector-link_add.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_connector-link_add

# DDN CLI: ddn connector-link add

Add a new DataConnectorLink to a Subgraph.

## Synopsis

When you initialize a new connector, a DataConnectorLink is automatically added and configured to the current or specified subrgaph.

```bash
ddn connector-link add <connector-link-name> [flags]
```

## Examples

```bash
# Add a DataConnectorLink to the Subgraph "app"
 ddn connector-link add mydb --subgraph ./app/subgraph.yaml

# Add a DataConnectorLink to the Subgraph "app" and configure its connector URL as the Connector's local Docker service URL
 ddn connector-link add mydb --subgraph app/subgraph.yaml --configure-host http://local.hasura.dev:<port>
```

## Options

```sass
    --ci                                 Disables the use of context
    --configure-connector-token string   Token used to authenticate requests to the Connector
    --configure-host string              Read and Write URL of the Connector
-c, --context string                     Name of the context to use. (default <current_context>)
-h, --help                               help for add
    --subgraph string                    Path to Subgraph config file
    --target-env-file string             Subgraph env file to write the connector URLs to
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector-link](/reference/cli/commands/ddn_connector-link) - Perform DataConnectorLink related operations



==============================



# ddn_connector-link_add-resources.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_connector-link_add-resources

# DDN CLI: ddn connector-link add-resources

Add all models, commands and relationships from a DataConnectorLink's schema.

## Synopsis

Add all models, commands and relationships from a DataConnectorLink's schema

```bash
ddn connector-link add-resources <connector-link-name> [flags]
```

## Examples

```bash
# Add all models, commands and relationships from the schema of DataConnectorLink 'mydb' for Subgraph config 'app'
 ddn connector-link add-resources mydb --subgraph ./app/subgraph.yaml --supergraph ./supergraph.yaml
```

## Options

```sass
    --ci                  Disables the use of context
-c, --context string      Name of the context to use. (default <current_context>)
-h, --help                help for add-resources
    --subgraph string     Path to Subgraph config file
    --supergraph string   Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector-link](/reference/cli/commands/ddn_connector-link) - Perform DataConnectorLink related operations



==============================



# ddn_connector-link_remove.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_connector-link_remove

# DDN CLI: ddn connector-link remove

Remove a DataConnectorLink from a Subgraph.

## Synopsis

Remove a DataConnectorLink from a Subgraph

```bash
ddn connector-link remove <connector-link-name> [flags]
```

**Alias:** rm

## Examples

```bash
# Remove a DataConnectorLink from the Subgraph "app"
 ddn connector-link remove mydb --subgraph ./app/subgraph.yaml
```

## Options

```sass
    --ci                            Disables the use of context
-c, --context string                Name of the context to use. (default <current_context>)
-h, --help                          help for remove
    --subgraph string               Path to Subgraph config file
    --target-env-file stringArray   Env file to remove ConnectorLink environment variables from
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector-link](/reference/cli/commands/ddn_connector-link) - Perform DataConnectorLink related operations



==============================



# ddn_connector-link_show.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_connector-link_show

# DDN CLI: ddn connector-link show

Show DataConnectorLink details.

## Synopsis

Show DataConnectorLink details

```bash
ddn connector-link show <connector-link-name> [flags]
```

## Examples

```bash
# Show DataConnectorLink details for `mydb`
 ddn connector-link show mydb
```

## Options

```sass
    --ci                Disables the use of context
-c, --context string    Name of the context to use. (default <current_context>)
-h, --help              help for show
    --subgraph string   Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector-link](/reference/cli/commands/ddn_connector-link) - Perform DataConnectorLink related operations



==============================



# ddn_connector-link_update.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_connector-link_update

# DDN CLI: ddn connector-link update

Fetch NDC details from the Connector and update the DataConnectorLink.

## Synopsis

This command is useful whenever the underlying data source has changed and already been re-introspected.

```bash
ddn connector-link update <connector-link-name> [flags]
```

## Examples

```bash
# Update the schema of a DataConnectorLink 'mydb' for Subgraph config 'app'
 ddn connector-link update mydb --subgraph ./app/subgraph.yaml

# Update the schema of a DataConnectorLink 'mydb' and add all Models, Commands and Relationships to the metadata for Subgraph config 'app'
 ddn connector-link update mydb --add-all-resources --subgraph ./app/subgraph.yaml
```

## Options

```sass
    --add-all-resources      Add all Models, Commands and Relationships from the updated DataConnectorLink to the local metadata
    --ci                     Disables the use of context
-c, --context string         Name of the context to use. (default <current_context>)
-e, --env stringArray        Environment variable, e.g. key=val. Can be repeated to provide multiple env vars
    --env-file stringArray   Path to .env file. Can be repeated to provide multiple env files
-h, --help                   help for update
    --subgraph string        Path to Subgraph config file
    --supergraph string      Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector-link](/reference/cli/commands/ddn_connector-link) - Perform DataConnectorLink related operations



==============================



# ddn_console.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_console

# DDN CLI: ddn console

Open the DDN console.

## Synopsis

Open the DDN console

```bash
ddn console [flags]
```

## Examples

```bash
# Open the console for the DDN project set in the context
 ddn console

# Open the local dev console
 ddn console --local

# Open the local dev console with a specific engine url
 ddn console --url http://localhost:8080

# Open the console for a specific DDN project
 ddn console --project my-project-123

# Open the console for a specific SupergraphBuild
 ddn console --project my-project-123 --build-version build-version-123
```

## Options

```sass
    --build-version string   SupergraphBuild version
-h, --help                   help for console
    --local                  Open the local dev console
-p, --project string         DDN Project name
    --url string             Local engine url
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



==============================



# ddn_context.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_context

# DDN CLI: ddn context

Perform context operations.

## Synopsis

Manage default value of keys to be used in DDN CLI commands. Contexts allow you to simplify your development workflow. To learn more, check out the docs: https://hasura.io/docs/3.0/project-configuration/project-management/manage-contexts

## Available operations

- [ddn context create-context](/reference/cli/commands/ddn_context_create-context) - Create a new context
- [ddn context get](/reference/cli/commands/ddn_context_get) - Get the value of a key in the context
- [ddn context get-context](/reference/cli/commands/ddn_context_get-context) - List contexts or get details of a specific context
- [ddn context get-current-context](/reference/cli/commands/ddn_context_get-current-context) - Get name and contents of current context
- [ddn context set](/reference/cli/commands/ddn_context_set) - Set the value of a key in the context
- [ddn context set-current-context](/reference/cli/commands/ddn_context_set-current-context) - Set the current context
- [ddn context unset](/reference/cli/commands/ddn_context_unset) - Unset the value of a key in the context

## Options

```sass
-h, --help   help for context
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



==============================



# ddn_context_create-context.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_context_create-context

# DDN CLI: ddn context create-context

Create a new context.

## Synopsis

Create a new context

```bash
ddn context create-context <name> [flags]
```

## Examples

```bash
# Create a new context called staging
 ddn context create-context staging
```

## Options

```sass
-h, --help   help for create-context
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn context](/reference/cli/commands/ddn_context) - Perform context operations



==============================



# ddn_context_get.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_context_get

# DDN CLI: ddn context get

Get the value of a key in the context.

## Synopsis

Get the value of a key in the context

```bash
ddn context get <key> (Allowed keys: project, supergraph, subgraph, localEnvFile, cloudEnvFile, selfHostedDataPlane, noBuildConnectors) [flags]
```

## Examples

```bash
# Get the Project name set in the context
 ddn context get project

# Get the Supergraph config file path set in the context
 ddn context get supergraph
```

## Options

```sass
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for get
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn context](/reference/cli/commands/ddn_context) - Perform context operations



==============================



# ddn_context_get-context.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_context_get-context

# DDN CLI: ddn context get-context

List contexts or get details of a specific context.

## Synopsis

List contexts or get details of a specific context

```bash
ddn context get-context [name] [flags]
```

## Examples

```bash
# Get list of contexts
 ddn context get-context

# Get details of context 'default'
 ddn context get-context default
```

## Options

```sass
-h, --help   help for get-context
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn context](/reference/cli/commands/ddn_context) - Perform context operations



==============================



# ddn_context_get-current-context.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_context_get-current-context

# DDN CLI: ddn context get-current-context

Get name and contents of current context.

## Synopsis

Get name and contents of current context

```bash
ddn context get-current-context [flags]
```

## Examples

```bash
# Get name and contents of current context
 ddn context get-current-context
```

## Options

```sass
-h, --help   help for get-current-context
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn context](/reference/cli/commands/ddn_context) - Perform context operations



==============================



# ddn_context_set.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_context_set

# DDN CLI: ddn context set

Set the value of a key in the context.

## Synopsis

Set default value of keys to be used in DDN CLI commands.

```bash
ddn context set <key> <value> (Allowed keys: subgraph, localEnvFile, cloudEnvFile, selfHostedDataPlane, noBuildConnectors, project, supergraph) [flags]
```

## Examples

```bash
# Set the Project name in the context
 ddn context set project foo-bar-1234

# Set the local Supergraph config file path in the context
 ddn context set supergraph ./supergraph.local.yaml

# Set the Subgraph config file path in the context
 ddn context set subgraph ./app/subgraph.yaml
```

## Options

```sass
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for set
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn context](/reference/cli/commands/ddn_context) - Perform context operations



==============================



# ddn_context_set-current-context.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_context_set-current-context

# DDN CLI: ddn context set-current-context

Set the current context.

## Synopsis

Set the current context

```bash
ddn context set-current-context <name> [flags]
```

## Examples

```bash
# Set current context to 'default'
 ddn context set-current-context default
```

## Options

```sass
-h, --help   help for set-current-context
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn context](/reference/cli/commands/ddn_context) - Perform context operations



==============================



# ddn_context_unset.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_context_unset

# DDN CLI: ddn context unset

Unset the value of a key in the context.

## Synopsis

Unset the value of a key in the context

```bash
ddn context unset <key> (Allowed keys: cloudEnvFile, selfHostedDataPlane, noBuildConnectors, project, supergraph, subgraph, localEnvFile) [flags]
```

## Examples

```bash
# Unset the Project name set in the context
 ddn context unset project

# Unset the Supergraph config file path set in the context
 ddn context unset supergraph
```

## Options

```sass
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for unset
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn context](/reference/cli/commands/ddn_context) - Perform context operations



==============================



# ddn_doctor.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_doctor

# DDN CLI: ddn doctor

Check if the dependencies of DDN CLI are present.

## Synopsis

Check if the dependencies (Docker and Docker Compose) of DDN CLI are installed, are of the required version and if they are running.

```bash
ddn doctor [flags]
```

## Options

```sass
-h, --help   help for doctor
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



==============================



# ddn_model.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_model

# DDN CLI: ddn model

Perform Model-related operations.

## Synopsis

Models represent a collection of data objects within a data domain. To learn more, check out the docs: https://hasura.io/docs/3.0/reference/metadata-reference/models/

**Alias:** models

## Available operations

- [ddn model add](/reference/cli/commands/ddn_model_add) - Add new Models to the local metadata
- [ddn model list](/reference/cli/commands/ddn_model_list) - Lists details about the collections of DataConnectorLink, and their corresponding Models
- [ddn model remove](/reference/cli/commands/ddn_model_remove) - Removes Models (and related metadata) in the local metadata
- [ddn model show](/reference/cli/commands/ddn_model_show) - Show diff between the model and its corresponding ndc collection
- [ddn model update](/reference/cli/commands/ddn_model_update) - Update Models in the local metadata

## Options

```sass
-h, --help   help for model
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



==============================



# ddn_model_add.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_model_add

# DDN CLI: ddn model add

Add new Models to the local metadata.

## Synopsis

When adding a model to your local metadata, the CLI will generate boilerplate metadata that — when the supergraph is built — will make it available to the exposed API.

```bash
ddn model add <connector-link-name> <collection-name> [flags]
```

## Examples

```bash
# Add all Models for DataConnectorLink "mydb" in the subgraph set in the context
 ddn model add mydb "*"

# Add a new Model "Album" from the collection "Album" in the DataConnectorLink "mydb" in Subgraph "app"
 ddn model add mydb Album --subgraph ./app/subgraph.yaml --supergraph ./supergraph.yaml

# Add all the Models from the collections in the DataConnectorLink "mydb" in Subgraph "app"
 ddn model add mydb "*" --subgraph ./app/subgraph.yaml --supergraph ./supergraph.yaml

# Add Models filtered by glob pattern from the collections in the DataConnectorLink "mydb" in Subgraph "app"
 ddn model add mydb "user*" --subgraph ./app/subgraph.yaml
```

## Options

```sass
    --ci                  Disables the use of context
-c, --context string      Name of the context to use. (default <current_context>)
-h, --help                help for add
    --pattern string      Pattern to detect targets. Can be 'glob' or 'literal'. (default "glob")
    --subgraph string     Path to Subgraph config file
    --supergraph string   Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn model](/reference/cli/commands/ddn_model) - Perform Model-related operations



==============================



# ddn_model_list.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_model_list

# DDN CLI: ddn model list

Lists details about the collections of DataConnectorLink, and their corresponding Models.

## Synopsis

Lists details about the collections of DataConnectorLink, and their corresponding Models

```bash
ddn model list <connector-link-name> [flags]
```

## Examples

```bash
# List details about the collections of DataConnectorLink `mydb`, and their corresponding Models
 ddn model list mydb
```

## Options

```sass
    --ci                Disables the use of context
-c, --context string    Name of the context to use. (default <current_context>)
-h, --help              help for list
    --subgraph string   Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn model](/reference/cli/commands/ddn_model) - Perform Model-related operations



==============================



# ddn_model_remove.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_model_remove

# DDN CLI: ddn model remove

Removes Models (and related metadata) in the local metadata.

## Synopsis

Removes Models (and related metadata) in the local metadata

```bash
ddn model remove <model-name> [flags]
```

## Examples

```bash
# Remove all Models using the subgraph set in current context
 ddn model remove "*"

# Remove the Model "Album" in the "app" Subgraph
 ddn model remove Album --subgraph ./app/subgraph.yaml --supergraph ./supergraph.yaml

# Remove all the Models in the Subgraph "app"
 ddn model remove "*" --subgraph ./app/subgraph.yaml --supergraph ./supergraph.yaml

# Remove Models filtered by glob pattern in the Subgraph "app"
 ddn model remove "user*" --subgraph ./app/subgraph.yaml --supergraph ./supergraph.yaml
```

## Options

```sass
    --ci                  Disables the use of context
-c, --context string      Name of the context to use. (default <current_context>)
-h, --help                help for remove
    --pattern string      Pattern to detect targets. Can be 'glob' or 'literal'. (default "glob")
    --subgraph string     Path to Subgraph config file
    --supergraph string   Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn model](/reference/cli/commands/ddn_model) - Perform Model-related operations



==============================



# ddn_model_show.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_model_show

# DDN CLI: ddn model show

Show diff between the model and its corresponding ndc collection.

## Synopsis

Show diff between the model and its corresponding ndc collection

```bash
ddn model show <model-name> [flags]
```

## Examples

```bash
# Show diff between the 'Carts' model and its corresponding ndc collection
 ddn model show Carts

# Show diff between models and their corresponding ndc collections for all models matching glob pattern 'User*'
 ddn model show "User*"
```

## Options

```sass
    --ci                Disables the use of context
-c, --context string    Name of the context to use. (default <current_context>)
-h, --help              help for show
    --pattern string    Pattern to detect targets. Can be 'glob' or 'literal'. (default "glob")
    --subgraph string   Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn model](/reference/cli/commands/ddn_model) - Perform Model-related operations



==============================



# ddn_model_update.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_model_update

# DDN CLI: ddn model update

Update Models in the local metadata.

## Synopsis

When the underlying data source changes, it's necessary to update the model's metadata to ensure Hasura DDN is aware of any modifications. Typically, before updating a model, you'll have re-introspected the underlying data source.

```bash
ddn model update <model-name> [flags]
```

## Examples

```bash
# Update all Models using the subgraph set in current context
 ddn model update "*"

# Update the Model "Album" in the "app" Subgraph
 ddn model update Album --subgraph ./app/subgraph.yaml --supergraph ./supergraph.yaml

# Update all the Models in the Subgraph "app"
 ddn model update "*" --subgraph ./app/subgraph.yaml --supergraph ./supergraph.yaml

# Update Models filtered by glob pattern in the Subgraph "app"
 ddn model update "user*" --subgraph ./app/subgraph.yaml --supergraph ./supergraph.yaml
```

## Options

```sass
    --ci                  Disables the use of context
-c, --context string      Name of the context to use. (default <current_context>)
-h, --help                help for update
    --pattern string      Pattern to detect targets. Can be 'glob' or 'literal'. (default "glob")
    --subgraph string     Path to Subgraph config file
    --supergraph string   Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn model](/reference/cli/commands/ddn_model) - Perform Model-related operations



==============================



# ddn_plugins.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_plugins

# DDN CLI: ddn plugins

Manage plugins for the CLI.

## Synopsis

The functionality of the CLI can be extended by using plugins. For a list of all available plugins, run ``ddn plugins list``, or visit this repository: https://github.com/hasura/cli-plugins-index.

If you're interested in contributing, please open a PR against this repo.

**Alias:** plugin

## Available operations

- [ddn plugins install](/reference/cli/commands/ddn_plugins_install) - Install a plugin from the index
- [ddn plugins list](/reference/cli/commands/ddn_plugins_list) - List all available plugins with index, versions and installation status
- [ddn plugins uninstall](/reference/cli/commands/ddn_plugins_uninstall) - Uninstall a plugin
- [ddn plugins upgrade](/reference/cli/commands/ddn_plugins_upgrade) - Upgrade a plugin to a newer version

## Options

```sass
-h, --help   help for plugins
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



==============================



# ddn_plugins_install.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_plugins_install

# DDN CLI: ddn plugins install

Install a plugin from the index.

## Synopsis

To install plugins that extend the functionality of the DDN CLI, you can use the install command. This command will install the plugin from the index and add it to your configuration file.

```bash
ddn plugins install <name> [flags]
```

**Alias:** add

## Examples

```bash
# Install a plugin named "ndc-postgres"
 ddn plugins install ndc-postgres
```

## Options

```sass
-h, --help             help for install
    --version string   Version to be installed
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn plugins](/reference/cli/commands/ddn_plugins) - Manage plugins for the CLI



==============================



# ddn_plugins_list.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_plugins_list

# DDN CLI: ddn plugins list

List all available plugins with index, versions and installation status.

## Synopsis

Run the plugins list command to see a list of all the available plugins which extend the functionality of the CLI, their versions and installation status.

```bash
ddn plugins list [flags]
```

**Alias:** ls

## Examples

```bash
# List all plugins
 ddn plugins list

# List all plugins without updating the plugin index local cache
 ddn plugins list --dont-update-index
```

## Options

```sass
    --dont-update-index   Don't update the plugin index local cache, only show the list
-h, --help                help for list
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn plugins](/reference/cli/commands/ddn_plugins) - Manage plugins for the CLI



==============================



# ddn_plugins_uninstall.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_plugins_uninstall

# DDN CLI: ddn plugins uninstall

Uninstall a plugin.

## Synopsis

To uninstall a plugin, run the uninstall command with the name of the plugin as an argument. If unsure of the plugin's name, you can run the `ddn plugins list` command to see a list of all the available plugins.

```bash
ddn plugins uninstall <plugin-name> [flags]
```

**Alias:** remove

## Examples

```bash
# Uninstall a plugin named "ndc-postgres"
 ddn plugins uninstall ndc-postgres
```

## Options

```sass
-h, --help   help for uninstall
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn plugins](/reference/cli/commands/ddn_plugins) - Manage plugins for the CLI



==============================



# ddn_plugins_upgrade.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_plugins_upgrade

# DDN CLI: ddn plugins upgrade

Upgrade a plugin to a newer version.

## Synopsis

To upgrade a plugin, run the upgrade command with the name of the plugin as an argument. If unsure of the plugin's name, you can run the `ddn plugins list` command to see a list of all the available plugins.

```bash
ddn plugins upgrade <plugin-name> [flags]
```

**Alias:** update

## Examples

```bash
# Upgrade a plugin "ndc-postgres" to a newer version
 ddn plugins upgrade ndc-postgres
```

## Options

```sass
-h, --help             help for upgrade
    --version string   Version to be upgraded
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn plugins](/reference/cli/commands/ddn_plugins) - Manage plugins for the CLI



==============================



# ddn_project.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_project

# DDN CLI: ddn project

Manage Hasura DDN Project.

## Synopsis

This subset of commands makes it easy to control the lifecycle of a project hosted on Hasura Cloud.

## Available operations

- [ddn project create](/reference/cli/commands/ddn_project_create) - Create an empty Project on Hasura DDN. See 'ddn project init' to create a Project, create subgraphs, and configure the local directory to use the Project.
- [ddn project delete](/reference/cli/commands/ddn_project_delete) - Delete a Project on Hasura DDN
- [ddn project get](/reference/cli/commands/ddn_project_get) - List Hasura DDN Projects or get details of a specific one
- [ddn project init](/reference/cli/commands/ddn_project_init) - Configure the local directory to use a Hasura DDN project, creating a DDN project and subgraphs as necessary
- [ddn project set-api-access-mode](/reference/cli/commands/ddn_project_set-api-access-mode) - Set the API access mode for a Project on Hasura DDN
- [ddn project set-self-hosted-engine-url](/reference/cli/commands/ddn_project_set-self-hosted-engine-url) - Set the engine's URL for a project in a self hosted data plane.
- [ddn project subgraph](/reference/cli/commands/ddn_project_subgraph) - Manage Subgraphs in a Hasura DDN Project

## Options

```sass
-h, --help   help for project
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



==============================



# ddn_project_create.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_project_create

# DDN CLI: ddn project create

Create an empty Project on Hasura DDN. See 'ddn project init' to create a Project, create subgraphs, and configure the local directory to use the Project..

## Synopsis

Create an empty Project on Hasura DDN. See 'ddn project init' to create a Project, create subgraphs, and configure the local directory to use the Project.

```bash
ddn project create [project-name] [flags]
```

## Examples

```bash
# Create a Project on Hasura DDN with auto-generated name
 ddn project create

# Create a Project with name "test-project" on Hasura DDN
 ddn project create test-project
```

## Options

```sass
    --data-plane-id uuid   The DDN instance where the Project should be hosted
-h, --help                 help for create
    --plan string          DDN Project plan
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn project](/reference/cli/commands/ddn_project) - Manage Hasura DDN Project



==============================



# ddn_project_delete.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_project_delete

# DDN CLI: ddn project delete

Delete a Project on Hasura DDN.

## Synopsis

Delete a Project on Hasura DDN

```bash
ddn project delete <project-name> [flags]
```

## Examples

```bash
# Delete a Project "pet-lion-2649" on Hasura DDN
 ddn project delete pet-lion-2649
```

## Options

```sass
-f, --force   Delete project without confirmation
-h, --help    help for delete
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn project](/reference/cli/commands/ddn_project) - Manage Hasura DDN Project



==============================



# ddn_project_get.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_project_get

# DDN CLI: ddn project get

List Hasura DDN Projects or get details of a specific one.

## Synopsis

List Hasura DDN Projects or get details of a specific one

```bash
ddn project get [project-name] [flags]
```

## Examples

```bash
# List all your Projects on Hasura DDN.
 ddn project get

# Get details of a Project "pet-lion-2649" on Hasura DDN.
 ddn project get pet-lion-2649
```

## Options

```sass
-h, --help   help for get
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn project](/reference/cli/commands/ddn_project) - Manage Hasura DDN Project



==============================



# ddn_project_init.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_project_init

# DDN CLI: ddn project init

Configure the local directory to use a Hasura DDN project, creating a DDN project and subgraphs as necessary.

## Synopsis

Configure the local directory to use a Hasura DDN project, creating a DDN project and subgraphs as necessary

```bash
ddn project init [project-name] [flags]
```

## Examples

```bash
# Initialize a new Hasura DDN project (with an auto-generated name) with subgraphs based on your local directory
 ddn project init

# Configure the local directory to use an existing Hasura DDN project creating subgraphs on the DDN project as necessary
 ddn project init --with-project myproject
```

## Options

```sass
    --ci                     Disables the use of context
-c, --context string         Name of the context to use. (default <current_context>)
    --data-plane-id uuid     The DDN instance where the Project should be hosted
    --env-file-name string   Env file to be created and added to context as cloudEnvFile (default ".env.cloud")
    --from-env-file string   Env file to initialize the cloudEnvFile from
-h, --help                   help for init
    --plan string            DDN Project plan
    --supergraph string      Path to Supergraph config file
    --with-project string    Use an existing project instead of creating a new one
    --with-promptql          Initialize with PromptQL support (alpha)
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn project](/reference/cli/commands/ddn_project) - Manage Hasura DDN Project



==============================



# ddn_version.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_version

# DDN CLI: ddn version

Prints the CLI version.

## Synopsis

Use this command to print the current version of the CLI. This command can also be used to check if a new version of the
CLI is available.

```bash
ddn version [flags]
```

## Options

```sass
-h, --help   help for version
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



==============================



# ddn_project_set-api-access-mode.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_project_set-api-access-mode

# DDN CLI: ddn project set-api-access-mode

Set the API access mode for a Project on Hasura DDN.

## Synopsis

Hosted projects are set to private by default. This protects your data before configuring any kind of authentication mode aside from the default (NoAuth) mode. Switching a project to public — while NoAuth mode is configured — will allow any client to access the API.

To learn more about setting an auth mode, see the docs: https://hasura.io/docs/3.0/auth/overview

```bash
ddn project set-api-access-mode <public|private> [flags]
```

## Examples

```bash
# Set the API access mode for the DDN project set in the context as private
 ddn project set-api-access-mode private

# Set the API access mode for the DDN project set in the context as public
 ddn project set-api-access-mode public

# Set the API access mode for the DDN project 'my-project-123' as private
 ddn project set-api-access-mode private --project my-project-123

# Set the API access mode for the DDN project 'my-project-123' as public
 ddn project set-api-access-mode public --project my-project-123
```

## Options

```sass
    --ci               Disables the use of context
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for set-api-access-mode
-p, --project string   DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn project](/reference/cli/commands/ddn_project) - Manage Hasura DDN Project



==============================



# ddn_project_set-self-hosted-engine-url.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_project_set-self-hosted-engine-url

# DDN CLI: ddn project set-self-hosted-engine-url

Set the engine's URL for a project in a self hosted data plane..

## Synopsis

Set the engine's URL for a project in a self hosted data plane.

```bash
ddn project set-self-hosted-engine-url <url> [flags]
```

## Examples

```bash
# Set project URL to "example.com:3000" for project "pet-lion-2649"
 ddn project set-self-hosted-engine-url example.com:3000 --project pet-lion-2649
```

## Options

```sass
    --ci               Disables the use of context
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for set-self-hosted-engine-url
-p, --project string   DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn project](/reference/cli/commands/ddn_project) - Manage Hasura DDN Project



==============================



# ddn_project_subgraph.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_project_subgraph

# DDN CLI: ddn project subgraph

Manage Subgraphs in a Hasura DDN Project.

## Synopsis

Manage Subgraphs in a Hasura DDN Project

## Available operations

- [ddn project subgraph create](/reference/cli/commands/ddn_project_subgraph_create) - Create a new Subgraph in a Hasura DDN Project
- [ddn project subgraph delete](/reference/cli/commands/ddn_project_subgraph_delete) - Delete a Subgraph from a Project
- [ddn project subgraph get](/reference/cli/commands/ddn_project_subgraph_get) - List Subgraphs for a Hasura DDN Project or get details of a specific one

## Options

```sass
-h, --help   help for subgraph
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn project](/reference/cli/commands/ddn_project) - Manage Hasura DDN Project



==============================



# ddn_project_subgraph_create.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_project_subgraph_create

# DDN CLI: ddn project subgraph create

Create a new Subgraph in a Hasura DDN Project.

## Synopsis

Create a new Subgraph in a Hasura DDN Project

```bash
ddn project subgraph create <subgraph-name> [flags]
```

## Examples

```bash
# Create a new Subgraph "app" in a Project
 ddn project subgraph create app --project pet-lion-2649

# Create a new Subgraph "app" in a Project with a description
 ddn project subgraph create app --project pet-lion-2649 --description "application management APIs"
```

## Options

```sass
    --ci                   Disables the use of context
-c, --context string       Name of the context to use. (default <current_context>)
-d, --description string   (Optional) description of the subgraph
-h, --help                 help for create
-p, --project string       DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn project subgraph](/reference/cli/commands/ddn_project_subgraph) - Manage Subgraphs in a Hasura DDN Project



==============================



# ddn_project_subgraph_delete.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_project_subgraph_delete

# DDN CLI: ddn project subgraph delete

Delete a Subgraph from a Project.

## Synopsis

Delete a Subgraph from a Project

```bash
ddn project subgraph delete <subgraph-name> [flags]
```

## Examples

```bash
# Delete a Subgraph 'app' from a Project
 ddn project subgraph delete app --project pet-lion-2649
```

## Options

```sass
    --ci               Disables the use of context
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for delete
-p, --project string   DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn project subgraph](/reference/cli/commands/ddn_project_subgraph) - Manage Subgraphs in a Hasura DDN Project



==============================



# ddn_project_subgraph_get.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_project_subgraph_get

# DDN CLI: ddn project subgraph get

List Subgraphs for a Hasura DDN Project or get details of a specific one.

## Synopsis

List Subgraphs for a Hasura DDN Project or get details of a specific one

```bash
ddn project subgraph get [subgraph-name] [flags]
```

## Examples

```bash
# List all Subgraphs for a Project
 ddn project subgraph get --project pet-lion-2649

# View details of a Subgraph "app" in a Project
 ddn project subgraph get app --project pet-lion-2649
```

## Options

```sass
    --ci               Disables the use of context
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for get
-p, --project string   DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn project subgraph](/reference/cli/commands/ddn_project_subgraph) - Manage Subgraphs in a Hasura DDN Project



==============================



# ddn_relationship.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_relationship

# DDN CLI: ddn relationship

Perform Relationship related operations.

## Synopsis

Relationships allow you to make queries across linked information. To learn more, check out the docs: https://hasura.io/docs/3.0/reference/metadata-reference/relationships/

**Alias:** relationships

## Available operations

- [ddn relationship add](/reference/cli/commands/ddn_relationship_add) - Adds Relationships from foreign keys on collection-name or targeting collection-name
- [ddn relationship list](/reference/cli/commands/ddn_relationship_list) - Lists Relationships for a given DataConnectorLink

## Options

```sass
-h, --help   help for relationship
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



==============================



# ddn_relationship_add.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_relationship_add

# DDN CLI: ddn relationship add

Adds Relationships from foreign keys on collection-name or targeting collection-name.

## Synopsis

Adds Relationships from foreign keys on collection-name or targeting collection-name

```bash
ddn relationship add <connector-link-name> <collection-name> [flags]
```

## Examples

```bash
# Add all Relationships for DataConnectorLink "mydb" in the subgraph set in the context
 ddn relationship add mydb "*"

# Add Relationships for the collection "Album" in the DataConnectorLink "mydb" in the Subgraph "app"
 ddn relationship add mydb Album --subgraph ./app/subgraph.yaml

# Add Relationships for collections that match the glob pattern "sales_*"
 ddn relationship add mydb "sales_*"

# Add Relationships for the collection "Album" defined by the foreign key "artists_album_id_fkey" on the collection "Artist"
 ddn relationship add mydb Album --fk-collection Artist --fk-name artists_album_id_fkey
```

## Options

```sass
    --ci                     Disables the use of context
-c, --context string         Name of the context to use. (default <current_context>)
    --fk-collection string   Only consider foreign keys defined on this collection
    --fk-name string         Only consider foreign keys with this name
-h, --help                   help for add
    --pattern string         Pattern to detect targets. Can be 'glob' or 'literal'. (default "glob")
    --subgraph string        Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn relationship](/reference/cli/commands/ddn_relationship) - Perform Relationship related operations



==============================



# ddn_relationship_list.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_relationship_list

# DDN CLI: ddn relationship list

Lists Relationships for a given DataConnectorLink.

## Synopsis

Lists Relationships for a given DataConnectorLink

```bash
ddn relationship list [flags]
```

## Examples

```bash
# List all Relationships for the supergraph set in the context
 ddn relationship list

# List all Relationships for a specific supergraph
 ddn relationship list --supergraph ./supergraph.cloud.yaml

# List all Relationships for a specific subgraph
 ddn relationship list --subgraph app/subgraph.yaml
```

## Options

```sass
    --ci                  Disables the use of context
-c, --context string      Name of the context to use. (default <current_context>)
-h, --help                help for list
    --subgraph string     Path to Subgraph config file
    --supergraph string   Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn relationship](/reference/cli/commands/ddn_relationship) - Perform Relationship related operations



==============================



# ddn_run.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_run

# DDN CLI: ddn run

Run specific script from project's context config.

## Synopsis

Run custom scripts defined in project's context config (typically at `<project-root>/.hasura/context.yaml`)

```bash
ddn run <script-name> [flags] [-- <args>]
```

## Examples

```bash
# Run script `docker-start` defined in project's context config
 ddn run docker-start

# Run script `docker-start` defined in project's context config in detached mode
 ddn run docker-start -- -d
```

## Options

```sass
-h, --help   help for run
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



==============================



# ddn_subgraph.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_subgraph

# DDN CLI: ddn subgraph

Perform Subgraph-related operations.

## Synopsis

Perform Subgraph-related operations

**Alias:** subgraphs

## Available operations

- [ddn subgraph add](/reference/cli/commands/ddn_subgraph_add) - Add a Subgraph config file to Supergraph config file
- [ddn subgraph build](/reference/cli/commands/ddn_subgraph_build) - Perform SubgraphBuild-related operations
- [ddn subgraph delete](/reference/cli/commands/ddn_subgraph_delete) - Delete a Subgraph, its corresponding Connectors and all its associated metadata objects from local project metadata
- [ddn subgraph init](/reference/cli/commands/ddn_subgraph_init) - Initialize a new Subgraph in local metadata

## Options

```sass
-h, --help   help for subgraph
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



==============================



# ddn_subgraph_add.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_subgraph_add

# DDN CLI: ddn subgraph add

Add a Subgraph config file to Supergraph config file.

## Synopsis

Adding a subgraph configuration file to the supergraph config allows the CLI to recognize the subgraph when generating new builds using the supergraph config.

```bash
ddn subgraph add --subgraph <path-to-subgraph-config-file> --target-supergraph <path-to-supergraph-config-file> [flags]
```

## Examples

```bash
# Add a Subgraph config file "./app/subgraph.yaml" to the Supergraph config file "./supergraph.yaml"
 ddn subgraph add --subgraph ./app/subgraph.yaml --target-supergraph ./supergraph.yaml

# Add a Subgraph config file "./app/subgraph.yaml" to multiple Supergraph config files"
 ddn subgraph add --subgraph ./app/subgraph.yaml --target-supergraph ./supergraph.stg.yaml --target-supergraph ./supergraph.prod.yaml
```

## Options

```sass
-h, --help                            help for add
    --subgraph string                 Path to Subgraph config file (required)
    --target-supergraph stringArray   Supergraph config file to add the Subgraph. Can be repeated to provide multiple Supergraph config files (required)
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn subgraph](/reference/cli/commands/ddn_subgraph) - Perform Subgraph-related operations



==============================



# ddn_subgraph_build.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_subgraph_build

# DDN CLI: ddn subgraph build

Perform SubgraphBuild-related operations.

## Synopsis

Perform SubgraphBuild-related operations

## Available operations

- [ddn subgraph build apply](/reference/cli/commands/ddn_subgraph_build_apply) - Apply a Subgraph build on Hasura DDN
- [ddn subgraph build create](/reference/cli/commands/ddn_subgraph_build_create) - Create a SubgraphBuild on Hasura DDN
- [ddn subgraph build get](/reference/cli/commands/ddn_subgraph_build_get) - List SubgraphBuilds or get details of a specific one

## Options

```sass
-h, --help   help for build
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn subgraph](/reference/cli/commands/ddn_subgraph) - Perform Subgraph-related operations



==============================



# ddn_subgraph_build_apply.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_subgraph_build_apply

# DDN CLI: ddn subgraph build apply

Apply a Subgraph build on Hasura DDN.

## Synopsis

Apply a Subgraph build on Hasura DDN

```bash
ddn subgraph build apply <subgraph-build-version> [flags]
```

## Examples

```bash
# Apply a Subgraph build to Project "pet-lion-2649"
 ddn subgraph build apply <subgraph-build-version> --project pet-lion-2649
```

## Options

```sass
    --ci                       Disables the use of context
-c, --context string           Name of the context to use. (default <current_context>)
-h, --help                     help for apply
-p, --project string           DDN Project name
    --self-hosted-data-plane   Is the data plane self hosted?
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn subgraph build](/reference/cli/commands/ddn_subgraph_build) - Perform SubgraphBuild-related operations



==============================



# ddn_subgraph_build_create.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_subgraph_build_create

# DDN CLI: ddn subgraph build create

Create a SubgraphBuild on Hasura DDN.

## Synopsis

Create a SubgraphBuild on Hasura DDN

```bash
ddn subgraph build create [flags]
```

## Examples

```bash
# Build the Subgraph from a config file for Project with some environment variables
 ddn subgraph build create --subgraph ./app/subgraph.yaml --project pet-lion-2649 --env key1=val1
```

## Options

```sass
    --ci                             Disables the use of context
-c, --context string                 Name of the context to use. (default <current_context>)
-d, --description string             (Optional) description of the build
-e, --env stringArray                Environment variable, e.g. key=val. Can be repeated to provide multiple env vars
    --env-file stringArray           Path to .env file. Can be repeated to provide multiple env files
-h, --help                           help for create
    --no-build-connectors            Do not recursively build all connectors in the subgraph and use their URLs for subgraph build. (default: false)
-p, --project string                 DDN Project name
    --self-hosted-data-plane         Is the data plane self hosted?
    --subgraph string                Path to Subgraph config file
    --target-env-file string         Env file to write the connector build URLs to.
    --update-connector-link-schema   Update DataConnectorLink schema with the NDC schema of the connectors built recursively. (default: false)
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn subgraph build](/reference/cli/commands/ddn_subgraph_build) - Perform SubgraphBuild-related operations



==============================



# ddn_subgraph_build_get.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_subgraph_build_get

# DDN CLI: ddn subgraph build get

List SubgraphBuilds or get details of a specific one.

## Synopsis

List SubgraphBuilds or get details of a specific one

```bash
ddn subgraph build get [subgraph-build-version] [flags]
```

## Examples

```bash
# View details of a SubgraphBuild in the Project 'pet-lion-2649'
 ddn subgraph build get <subgraph-build-version> --project pet-lion-2649

# List all SubgraphBuilds of a Project 'pet-lion-2649'
 ddn subgraph build get --project pet-lion-2649
```

## Options

```sass
    --ci               Disables the use of context
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for get
-p, --project string   DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn subgraph build](/reference/cli/commands/ddn_subgraph_build) - Perform SubgraphBuild-related operations



==============================



# ddn_subgraph_delete.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_subgraph_delete

# DDN CLI: ddn subgraph delete

Delete a Subgraph, its corresponding Connectors and all its associated metadata objects from local project metadata.

## Synopsis

Delete a Subgraph, its corresponding Connectors and all its associated metadata objects from local project metadata

```bash
ddn subgraph delete <subgraph-name> [flags]
```

**Alias:** rm, remove

## Examples

```bash
# Removes subgraph corresponding to the subgraph config file, its dependent metadata objects, and associated connectors from the local project,removes subgraph from the supergraph config set in context
 ddn subgraph delete --subgraph ./foo/subgraph.yaml

# Removes subgraph corresponding to the subgraph config file, its dependent metadata objects, and associated connectors from the local project, removes subgraph from the supergraph config "./supergraph.yaml"
 ddn subgraph delete --subgraph app/subgraph.yaml --supergraph ./supergraph.yaml
```

## Options

```sass
    --ci                  Disables the use of context
-c, --context string      Name of the context to use. (default <current_context>)
-h, --help                help for delete
    --subgraph string     Path to Subgraph config file (required)
    --supergraph string   Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn subgraph](/reference/cli/commands/ddn_subgraph) - Perform Subgraph-related operations



==============================



# ddn_update-cli.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_update-cli

# DDN CLI: ddn update-cli

Update this CLI to the latest version or to a specific version.

## Synopsis

You can use this command to update the CLI to the latest version or a specific version.

```bash
ddn update-cli [flags]
```

## Examples

```bash
# Update CLI to latest version:
 ddn update-cli

# Update CLI to a specific version (say v1.0.0):
 ddn update-cli --version v1.0.0

# To disable the auto-update check on the CLI, set
# "show_update_notification": false
# in ~/.ddn/config.yaml
```

## Options

```sass
-h, --help             help for update-cli
    --version string   A specific version to install
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



==============================



# ddn_subgraph_init.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_subgraph_init

# DDN CLI: ddn subgraph init

Initialize a new Subgraph in local metadata.

## Synopsis

This command will create a new named directory which will contain all relevant metadata and connector configuration files for any data sources added to this subgraph.

```bash
ddn subgraph init <subgraph-name> --dir <dir-name> [flags]
```

## Examples

```bash
# Initialize a Subgraph "app"
 ddn subgraph init app

# Initialize a Subgraph "app" in the directory "./app" and add it to Supergraph config files "./supergraph.yaml" and "./supergraph.cloud.yaml"
 ddn subgraph init app --dir ./app --target-supergraph ./supergraph.yaml --target-supergraph ./supergraph.cloud.yaml
```

## Options

```sass
    --dir string                          Directory to initialize the Subgraph (Defaults to subgraph name)
    --graphql-root-field-prefix string    Prefix to use while generating GraphQL root fields
    --graphql-type-name-prefix string     Prefix to use while generating GraphQL type names
-h, --help                                help for init
    --subgraph-naming-convention string   Naming convention for the subgraph. Can be 'graphql', 'snake_case' or 'none'.
    --target-supergraph stringArray       Supergraph config file to add the Subgraph. Can be repeated to provide multiple Supergraph config files
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn subgraph](/reference/cli/commands/ddn_subgraph) - Perform Subgraph-related operations



==============================



# ddn_supergraph.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_supergraph

# DDN CLI: ddn supergraph

Perform Supergraph-related operations.

## Synopsis

Perform Supergraph-related operations

**Alias:** supergraphs

## Available operations

- [ddn supergraph build](/reference/cli/commands/ddn_supergraph_build) - Perform SupergraphBuild-related operations
- [ddn supergraph init](/reference/cli/commands/ddn_supergraph_init) - Initialize a new Supergraph project directory
- [ddn supergraph prune](/reference/cli/commands/ddn_supergraph_prune) - Removes unused metadata of the Supergraph

## Options

```sass
-h, --help   help for supergraph
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



==============================



# ddn_supergraph_build.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_supergraph_build

# DDN CLI: ddn supergraph build

Perform SupergraphBuild-related operations.

## Synopsis

This subset of commands will help you manage the lifecycle of local and cloud builds for a Hasura DDN supergraph.

## Available operations

- [ddn supergraph build apply](/reference/cli/commands/ddn_supergraph_build_apply) - Apply a SupergraphBuild to its Project on Hasura DDN
- [ddn supergraph build create](/reference/cli/commands/ddn_supergraph_build_create) - Create a SupergraphBuild on Hasura DDN
- [ddn supergraph build delete](/reference/cli/commands/ddn_supergraph_build_delete) - Delete a SupergraphBuild from a Project
- [ddn supergraph build diff](/reference/cli/commands/ddn_supergraph_build_diff) - See changes made to the GraphQL schema from one build version to another.
- [ddn supergraph build export-descriptions](/reference/cli/commands/ddn_supergraph_build_export-descriptions) - Export descriptions for a specific supergraph build
- [ddn supergraph build get](/reference/cli/commands/ddn_supergraph_build_get) - List SupergraphBuilds or get details of a specific one
- [ddn supergraph build local](/reference/cli/commands/ddn_supergraph_build_local) - Build the Supergraph and generate assets to run the local Engine
- [ddn supergraph build set-self-hosted-engine-url](/reference/cli/commands/ddn_supergraph_build_set-self-hosted-engine-url) - Set the engine's URL for a build for a project in a self hosted data plane.

## Options

```sass
-h, --help   help for build
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn supergraph](/reference/cli/commands/ddn_supergraph) - Perform Supergraph-related operations



==============================



# ddn_completion.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_completion

# DDN CLI: ddn completion

Generate autocompletion scripts for the DDN CLI for the specified shell.

## Synopsis

Generate the autocompletion script for DDN for the specified shell.  
See each sub-command's help for details on how to use the generated script.

```bash
ddn completion [command] [flags]
```

## Available commands

- [bash](/reference/cli/commands/ddn_completion_bash): Generate the autocompletion script for bash
- [fish](/reference/cli/commands/ddn_completion_fish): Generate the autocompletion script for fish
- [powershell](/reference/cli/commands/ddn_completion_powershell): Generate the autocompletion script for PowerShell
- [zsh](/reference/cli/commands/ddn_completion_zsh): Generate the autocompletion script for zsh

## Options

```sass
-h, --help   help for completion
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



==============================



# ddn_completion_bash.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_completion_bash

# DDN CLI: ddn completion

Generate autocompletion scripts for the DDN CLI for the bash shell.

## Synopsis

Generate the autocompletion script for DDN for the bash shell.

To load completions in your current shell session:

```sass
        source <(ddn completion bash)
```

To load completions for every new session, execute once:

#### Linux:

```sass
        ddn completion bash > /etc/bash_completion.d/ddn
```

#### macOS:

```sass
        ddn completion bash > $(brew --prefix)/etc/bash_completion.d/ddn
```

You will need to start a new shell for this setup to take effect.

## Options

```sass
-h, --help   help for bash completion
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn completion](/reference/cli/commands/ddn_completion) - Generate autocompletion scripts for the DDN CLI



==============================



# ddn_supergraph_build_apply.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_supergraph_build_apply

# DDN CLI: ddn supergraph build apply

Apply a SupergraphBuild to its Project on Hasura DDN.

## Synopsis

Apply a SupergraphBuild to its Project on Hasura DDN

```bash
ddn supergraph build apply <supergraph-build-version> [flags]
```

## Examples

```bash
# Apply a SupergraphBuild to a Project "pet-lion-2649"
 ddn supergraph build apply <supergraph-build-version> --project pet-lion-2649
```

## Options

```sass
    --ci                       Disables the use of context
-c, --context string           Name of the context to use. (default <current_context>)
-h, --help                     help for apply
    --no-diff                  Do not do a GraphQL schema diff against the applied build
-p, --project string           DDN Project name
    --self-hosted-data-plane   Is the data plane self hosted?
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn supergraph build](/reference/cli/commands/ddn_supergraph_build) - Perform SupergraphBuild-related operations



==============================



# ddn_completion_fish.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_completion_fish

# DDN CLI: ddn completion

Generate autocompletion scripts for the DDN CLI for the fish shell.

## Synopsis

Generate the autocompletion script for DDN for the fish shell.

To load completions in your current shell session:

```sass
        ddn completion fish | source
```

To load completions for every new session, execute once:

```sass
        ddn completion fish > ~/.config/fish/completions/ddn.fish
```

You will need to start a new shell for this setup to take effect.

## Options

```sass
-h, --help   help for fish completion
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn completion](/reference/cli/commands/ddn_completion) - Generate autocompletion scripts for the DDN CLI



==============================



# ddn_supergraph_build_create.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_supergraph_build_create

# DDN CLI: ddn supergraph build create

Create a SupergraphBuild on Hasura DDN.

## Synopsis

Create a SupergraphBuild on Hasura DDN

```bash
ddn supergraph build create [flags]
```

## Examples

```bash
# Build the Connectors and the Supergraph
 ddn supergraph build create --supergraph supergraph.yaml --project pet-lion-2649 --env-file .env.cloud

# Build the Supergraph without building the Connectors
 ddn supergraph build create --supergraph supergraph.yaml --project pet-lion-2649 --no-build-connectors

# Build the Supergraph and update the link schema and target env file
 ddn supergraph build create --supergraph supergraph.yaml --project pet-lion-2649 --update-connector-link-schema --target-env-file .env.cloud

# Build a composed Supergraph using Subgraphs with build versions and using the applied Supergraph Build as the base Supergraph (Advanced plan only)
 ddn supergraph build create --subgraph-version globals:c15b0b4031 --subgraph-version my_subgraph:c15b0b4031 --base-supergraph-on-applied

# Build a composed Supergraph using Subgraphs with build versions and base Supergraph version (Advanced plan only)
 ddn supergraph build create --subgraph-version globals:c15b0b4031 --subgraph-version my_subgraph:c15b0b4031 --base-supergraph-version c15b0b4871

# Build the Supergraph and also apply the build
 ddn supergraph build create --supergraph supergraph.yaml --project pet-lion-2649 --apply
```

## Options

```sass
    --apply                            Apply the build created after it is completed
    --base-supergraph-on-applied       Use the applied Supergraph as the base supergraph
    --base-supergraph-version string   Base Supergraph version for the compose build
    --ci                               Disables the use of context
-c, --context string                   Name of the context to use. (default <current_context>)
-d, --description string               (Optional) description of the build
-e, --env stringArray                  Environment variable, e.g. key=val. Can be repeated to provide multiple env vars
    --env-file stringArray             Path to .env file. Can be repeated to provide multiple env files
-h, --help                             help for create
    --no-build-connectors              Do not recursively build all connectors in all subgraphs and use their URLs for supergraph build. (default: false)
    --no-diff                          Do not do a GraphQL schema diff against the applied build
    --output-dir string                Path to the directory to output the build artifacts
-p, --project string                   DDN Project name
    --self-hosted-data-plane           Is the data plane self hosted?
    --subgraph-version stringArray     Subgraph(s) with build version to compose
    --supergraph string                Path to Supergraph config file
    --target-env-file string           Env file to write the connector build URLs to.
    --update-connector-link-schema     Update DataConnectorLink schema with the NDC schema of the connectors built recursively. (default: false)
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn supergraph build](/reference/cli/commands/ddn_supergraph_build) - Perform SupergraphBuild-related operations



==============================



# ddn_completion_powershell.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_completion_powershell

# DDN CLI: ddn completion

Generate autocompletion scripts for the DDN CLI for the PowerShell shell.

## Synopsis

Generate the autocompletion script for DDN for the PowerShell shell.

To load completions in your current shell session:

```sass
         ddn completion powershell | Out-String | Invoke-Expression
```

To load completions for every new session, add the output of the above command to your powershell profile.

## Options

```sass
-h, --help   help for powershell completion
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn completion](/reference/cli/commands/ddn_completion) - Generate autocompletion scripts for the DDN CLI



==============================



# ddn_supergraph_build_delete.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_supergraph_build_delete

# DDN CLI: ddn supergraph build delete

Delete a SupergraphBuild from a Project.

## Synopsis

Delete a SupergraphBuild from a Project

```bash
ddn supergraph build delete <supergraph-build-version> [flags]
```

## Examples

```bash
# Delete a SupergraphBuild from a Project "pet-lion-2649"
 ddn supergraph build delete <supergraph-build-version> --project pet-lion-2649
```

## Options

```sass
    --ci               Disables the use of context
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for delete
-p, --project string   DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn supergraph build](/reference/cli/commands/ddn_supergraph_build) - Perform SupergraphBuild-related operations



==============================



# ddn_completion_zsh.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_completion_zsh

# DDN CLI: ddn completion

Generate autocompletion scripts for the DDN CLI for the zsh shell.

## Synopsis

Generate the autocompletion script for DDN for the zsh shell.

If shell completion is not already enabled in your environment you will need to enable it. You can execute the following
once:

```sass
        echo "autoload -U compinit; compinit" >> ~/.zshrc
```

To load completions in your current shell session:

```sass
        source <(ddn completion zsh)
```

To load completions for every new session, execute once:

#### Linux:

```sass
         ddn completion zsh > "${fpath[1]}/_ddn"
```

#### macOS:

```sass
        ddn completion zsh > $(brew --prefix)/share/zsh/site-functions/_ddn
```

You will need to start a new shell for this setup to take effect.

## Options

```sass
-h, --help   help for zsh completion
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn completion](/reference/cli/commands/ddn_completion) - Generate autocompletion scripts for the DDN CLI



==============================



# ddn_supergraph_build_diff.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_supergraph_build_diff

# DDN CLI: ddn supergraph build diff

See changes made to the GraphQL schema from one build version to another..

## Synopsis

See changes made to the GraphQL schema from one build version to another.

```bash
ddn supergraph build diff <build-version-1> <build-version-2> [flags]
```

## Examples

```bash
# Compare changes made to the GraphQL schema from build version "qfrr5e5jyw" to "g6v6nh73h0" for Project "pet-lion-2649"
 ddn supergraph build diff qfrr5e5jyw g6v6nh73h0 --project pet-lion-2649
```

## Options

```sass
    --ci               Disables the use of context
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for diff
-p, --project string   DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn supergraph build](/reference/cli/commands/ddn_supergraph_build) - Perform SupergraphBuild-related operations



==============================



# ddn_supergraph_build_export-descriptions.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_supergraph_build_export-descriptions

# DDN CLI: ddn supergraph build export-descriptions

Export descriptions for a specific supergraph build.

## Synopsis

Export descriptions for a specific supergraph build

```bash
ddn supergraph build export-descriptions <supergraph-build-version> [flags]
```

## Examples

```bash
# Export descriptions for a specific supergraph build version
 ddn supergraph build export-descriptions <supergraph-build-version>
```

## Options

```sass
    --ci               Disables the use of context
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for export-descriptions
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn supergraph build](/reference/cli/commands/ddn_supergraph_build) - Perform SupergraphBuild-related operations



==============================



# ddn_supergraph_build_get.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_supergraph_build_get

# DDN CLI: ddn supergraph build get

List SupergraphBuilds or get details of a specific one.

## Synopsis

List SupergraphBuilds or get details of a specific one

```bash
ddn supergraph build get [supergraph-build-version] [flags]
```

## Examples

```bash
# View details of a SupergraphBuild in the Project "pet-lion-2649"
 ddn supergraph build get <supergraph-build-version> --project pet-lion-2649

# List all SupergraphBuilds of a Project "pet-lion-2649"
 ddn supergraph build get --project pet-lion-2649
```

## Options

```sass
    --ci               Disables the use of context
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for get
-p, --project string   DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn supergraph build](/reference/cli/commands/ddn_supergraph_build) - Perform SupergraphBuild-related operations



==============================



# ddn_supergraph_build_local.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_supergraph_build_local

# DDN CLI: ddn supergraph build local

Build the Supergraph and generate assets to run the local Engine.

## Synopsis

Build the Supergraph and generate assets to run the local Engine using the DDN hosted metadata build service.

Any relationships to subgraphs defined in other repositories will not be validated and will be ignored.

```bash
ddn supergraph build local [flags]
```

## Examples

```bash
# Build the Supergraph using the supergraph config and local env file from context and output it to default engine directory i.e. <project-root>/engine/build
 ddn supergraph build local

# Build the Supergraph using a specific supergraph config file and env file and output it to a specific directory
 ddn supergraph build local --output-dir <path-to-engine-directory> --supergraph supergraph.yaml --env-file .env
```

## Options

```sass
    --ci                     Disables the use of context
-c, --context string         Name of the context to use. (default <current_context>)
-e, --env stringArray        Environment variable, e.g. key=val. Can be repeated to provide multiple env vars
    --env-file stringArray   Path to .env file. Can be repeated to provide multiple env files
-h, --help                   help for local
    --output-dir string      Path to the engine directory to output the build artifacts. (defaults to `<project-root>/engine/build)
    --supergraph string      Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn supergraph build](/reference/cli/commands/ddn_supergraph_build) - Perform SupergraphBuild-related operations



==============================



# ddn_supergraph_build_set-self-hosted-engine-url.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_supergraph_build_set-self-hosted-engine-url

# DDN CLI: ddn supergraph build set-self-hosted-engine-url

Set the engine's URL for a build for a project in a self hosted data plane..

## Synopsis

Set the engine's URL for a build for a project in a self hosted data plane.

```bash
ddn supergraph build set-self-hosted-engine-url <url> --build-version <build-version> [flags]
```

## Examples

```bash
# Set build URL to "example.com:3000" for project "pet-lion-2649"
 ddn supergraph build set-self-hosted-engine-url example.com:3000 --build-version <build-version> --project pet-lion-2649
```

## Options

```sass
    --build-version string   SupergraphBuild version (required)
    --ci                     Disables the use of context
-c, --context string         Name of the context to use. (default <current_context>)
-h, --help                   help for set-self-hosted-engine-url
-p, --project string         DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn supergraph build](/reference/cli/commands/ddn_supergraph_build) - Perform SupergraphBuild-related operations



==============================



# ddn_supergraph_init.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_supergraph_init

# DDN CLI: ddn supergraph init

Initialize a new Supergraph project directory.

## Synopsis

This command is most often the first run with a new project. Use this to create a new supergraph directory and then begin to create your API. To learn more, check out the quickstart: https://hasura.io/docs/3.0/quickstart

```bash
ddn supergraph init <path-to-project-dir> [flags]
```

## Examples

```bash
# Initialize a new Supergraph project directory with a default subgraph 'app'
 ddn supergraph init <path-to-project-dir>

# Initialize a new Supergraph project directory with a subgraph 'mysg'
 ddn supergraph init <path-to-project-dir> --create-subgraph mysg
```

## Options

```sass
    --create-project                      Create a project while initializing the supergraph
    --create-subgraph string              The default subgraph to add (default "app")
    --globals-subgraph string             Name of the globals subgraph (default "globals")
-h, --help                                help for init
    --no-globals-subgraph                 Do not add a globals subgraph
    --no-subgraph                         Do not add a default subgraph
    --project-data-plane-id uuid          The DDN instance where the Project should be hosted, only used with --create-project flag or --with-promptql flag
    --project-name string                 Create a new project with this name, only used with --create-project flag or --with-promptql flag
    --project-plan string                 DDN Project plan, only used with --create-project flag or --with-promptql flag
    --subgraph-naming-convention string   Naming convention for the subgraph. Can be 'graphql', 'snake_case' or 'none'.
    --with-project string                 Use an existing project instead of creating a new one, only used with --create-project flag or --with-promptql flag
    --with-promptql                       Initialize with PromptQL support, this also initializes a project with PromptQL enabled. (alpha)
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn supergraph](/reference/cli/commands/ddn_supergraph) - Perform Supergraph-related operations



==============================



# ddn_supergraph_prune.mdx

URL: https://hasura.io/docs/promptql/reference/cli/commands/ddn_supergraph_prune

# DDN CLI: ddn supergraph prune

Removes unused metadata of the Supergraph.

## Synopsis

Removes unused metadata of the Supergraph

```bash
ddn supergraph prune [flags]
```

## Options

```sass
    --ci                  Disables the use of context
-c, --context string      Name of the context to use. (default <current_context>)
    --dry-run             Perform a dry run only. The changes won't be applied
-h, --help                help for prune
    --supergraph string   Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn supergraph](/reference/cli/commands/ddn_supergraph) - Perform Supergraph-related operations



==============================



# faq.mdx

URL: https://hasura.io/docs/promptql/reference/cli/faq


# Troubleshooting FAQ

## How do I see debug logs from CLI commands?

Add `--log-level=debug` to the command to see debug logs from the CLI.

## How do I see the output in JSON or YAML format instead of the table format that is shown by default?

Use `--out=json` or `--out=yaml` to see the output in the corresponding format instead of JSON.

## How do I output the version of the DDN CLI?

Run `ddn version` to see the CLI version.

## Why does my engine container crash upon running `ddn run docker-start` during local development?

The engine Docker container can crash due to various reasons:

1. The mandatory environment variables required to configure the engine were not provided.
2. The metadata files required by the engine are invalid or non-existent.
3. The engine version is not up to date to handle new metadata objects.

#### 1. The mandatory environment variables required to configure the engine were not provided

```text title="Error output"
error: the following required arguments were not provided:
--metadata-path <PATH>
--authn-config-path <PATH>

Usage: engine --metadata-path <PATH> --authn-config-path < PATH>

For more information, try '--help'
```

In this case, verify that the env vars `AUTHN_CONFIG_PATH`, `INTROSPECTION_METADATA_FILE` and `METADATA_PATH` are passed
to the engine container.

The default docker compose file generated by the CLI passes these values to the engine container:

```
AUTHN_CONFIG_PATH: /md/auth_config.json
ENABLE_CORS: "true"
INTROSPECTION_METADATA_FILE: /md/metadata.json
METADATA_PATH: /md/open_dd.json
OTLP_ENDPOINT: http://local.hasura.dev:4317
```

`AUTHN_CONFIG_PATH`, `INTROSPECTION_METADATA_FILE` and the `METADATA_PATH` are the paths to the files created as a
result of `ddn supergraph build local`. These paths should be configured based on where it is mounted in the engine
container.

#### 2. The metadata files required by the engine are invalid or non-existent {#engine-env-var-config}

```text title="Error output"
Error while starting up the engine: failed to build engine state - No such file or directory (os error 2)
```

These are the files created as a result of `ddn supergraph build local`. If these files do not exist, then it can be
recreated by running the command.

Make sure the files are mounted to the engine container and the paths to it configured using the environment variables
mentioned in [this section](#engine-env-var-config).

By default the CLI generates these files inside `engine/build` of the project directory and this directory is copied
when building the engine image by the following Dockerfile specified in the `dockerfile` field of the default docker
compose file.

```
FROM ghcr.io/hasura/v3-engine
COPY ./build /md/
```

#### 3. The engine version is not up to date to handle new metadata objects

New features introduced in DDN may have new metadata objects or fields associated with it. If your metadata uses any of
these new objects but the engine version you are running is an older version that doesn't support these objects, then
the engine will fail to startup. This can be resolved by using the latest engine version as mentioned in
[this section](#pull-latest-engine).

## `ddn run docker-start` is using a different ddn binary than the one in the current shell. {#docker-start-different-binary}

The `ddn run` command is just a convenient way to run arbitrary commands defined in the `scripts` section of
`.hasura/context.yaml` of the project directory.

The `docker-start` is just the name of the script defined in this file. You can see the definition of the script by
going to `.hasura/context.yaml`.

As these scripts run in a new shell, the DDN CLI must be present in the path for it to be invoked correctly when the
script is run.

## How do I check that all the required dependencies for the CLI are installed?

The DDN CLI provides a `ddn doctor` command that checks that all the required dependencies are installed.

```text title="ddn doctor Output"
ddn doctor
3:23PM INF Evaluating the dependencies of DDN CLI...
DDN CLI location: "/usr/local/bin/ddn"
DDN CLI version: "v2.15.0"
+------------------------------+--------+-----+
| Criteria                     | Result | Fix |
+------------------------------+--------+-----+
| Docker available             | YES    |     |
+------------------------------+--------+-----+
| Docker Engine running        | YES    |     |
+------------------------------+--------+-----+
| Docker Registry reachable    | YES    |     |
+------------------------------+--------+-----+
| Docker Compose available     | YES    |     |
+------------------------------+--------+-----+
| Docker Compose version valid | YES    |     |
+------------------------------+--------+-----+
| Control Plane reachable      | YES    |     |
+------------------------------+--------+-----+
| DDN CLI Authenticated        | YES    |     |
+------------------------------+--------+-----+
```

## How do I specify the latest engine version for local development? {#pull-latest-engine}

The `ddn run docker-start` command uses the `--pull` flag in the `docker-start` script when bringing up docker
containers, and so will use the latest engine version. If you're running a`docker compose` command directly, then add
the `--pull` flag to get the latest engine version.

## Why are traces are not showing up for local development?

The local engine container and all connector containers send traces to the OpenTelemetry Collector container which is
defined by default in the docker compose file initialized by the DDN CLI.

Traces are then exported to a cloud endpoint by the OpenTelemetry container as defined in the
`otel-collector-config.yaml` file initialized by the CLI. For this, the OpenTelemetry Collector requires a PAT token.

If you are not able to see traces during local development:

1. Make sure you are logged in to the DDN CLI. You can check this with `ddn auth print-access-token`. If not, log in by
   running `ddn auth login`.
2. Make sure that your OpenTelemetry Collector container is running.
3. If you are running the docker compose command directly, then make sure that the environment variable `HASURA_DDN_PAT`
   is passed to the OpenTelemetry Collector container. The value for this env var would be the output of the
   `ddn auth print-access-token` command. If you are running `ddn run docker-start`, then this is already handled for
   you.
4. If you are seeing the issue when running `ddn run docker-start`, then it could be due to the `docker-start` script
   using a [different binary to the one defined in the current shell](#docker-start-different-binary).

## I'm unable to use the latest features in my project.

If you are not able to see newly introduced features, then it could be because your metadata is outdated. New features
like subscriptions may need corresponding changes to your metadata. If your metadata was initialized by an older version
of the CLI, then these features may not be available and you [may need to upgrade your
metadata](/project/configuration/overview.mdx



==============================



# Connectors

URL: https://hasura.io/docs/promptql/reference/connectors/

# Connectors

## Introduction

The connector reference section primarily focuses on two areas:

- Connector configuration
- Extending the GraphQL API using native operations

If you're getting started with a connector and want to connect it to your data source as quickly as possible,
[learn how to connect a data source](/data-sources/connect-to-a-source.mdx) to Hasura DDN.

## Connector docs

- [Amazon Athena](/reference/connectors/athena/index.mdx)
- [Amazon Redshift](/reference/connectors/redshift/index.mdx)
- [BigQuery](/reference/connectors/bigquery/index.mdx)
- [Databricks](/reference/connectors/databricks/index.mdx)
- [MySQL](/reference/connectors/mysql/index.mdx)
- [PostgreSQL](/reference/connectors/postgresql/index.mdx)
- [Snowflake](/reference/connectors/snowflake/index.mdx)



==============================



# Amazon Athena

URL: https://hasura.io/docs/promptql/reference/connectors/athena/

# Amazon Athena

## Introduction

Hasura DDN includes a Native Data Connector for Amazon Athena, providing integration with data in Amazon S3. This
connector allows you to leverage Athena's powerful OLAP capabilities while taking advantage of Hasura’s metadata-driven
approach. Here, we’ll explore the key features of the Amazon Athena connector and walk through the configuration process
within a Hasura DDN project.

:::tip Looking to get started?

If you've ended up here and aren't concerned about tweaking your configuration, and rather are looking to get started
with Amazon Athena and Hasura DDN as quickly as possible, check out our
[Amazon Athena tutorial](/how-to-build-with-promptql/with-amazon-athena.mdx) or
[learn how to connect](/data-sources/connect-to-a-source.mdx) to a Amazon Athena.

:::

## Amazon Athena docs

- [Connector configuration](/reference/connectors/athena/configuration.mdx)
- [Troubleshooting](/reference/connectors/athena/troubleshooting.mdx)



==============================



# Configuration

URL: https://hasura.io/docs/promptql/reference/connectors/athena/configuration


## Introduction

The `configuration.json` file is generated whenever you introspect a new connector. The file is located in the
`<connector_name>/connector` sub-directory of its parent subgraph.

The configuration is a metadata object that lists all the database entities — such as tables — that the data connector
has to know about in order to serve queries. When your database schema changes you will have to update the configuration
accordingly.

While the `configuration.json` file is generated and populated for you, you can hand-edit sections (such as
[native queries](#native-queries)) to manipulate what resources are available to your application.

## Structure

The configuration object is a JSON object with the following fields:

```json
{
  "version": "v2",
  "connection_uri": {
    "value": "...",
    "variable": "..."
  },
  "schemas": ["public"],
  "tables": [...],
  "functions": [],
}
```

### Property: connection_uri

The connection URI for the datasource. This is a required field that can be specified either as a direct string value or
as a reference to an environment variable:

```json
{
  "connection_uri": {
    "value": "<connection_uri>"
  }
}
```

:::info JDBC Syntax

This construction differs from source to source. Check out [these docs](/data-sources/connect-to-a-source.mdx) for
examples of connection strings for this and other sources.

:::

Or using an environment variable:

```json
{
  "connection_uri": {
    "variable": "JDBC_URL"
  }
}
```

### Property: schemas

This is an optional array of schema names to include in the introspection process. If not provided, all schemas will be
included. **Any schema passed in the JDBC URL will take precedence.**

Example:

```json
{
  "schemas": ["schema1", "schema2"]
}
```

### Property: tables

An array of table definitions generated automatically during introspection. Each table definition includes metadata
about the table structure, columns, primary keys, and foreign keys.

Example:

```json
{
  "tables": [
    {
      "name": "public.customers",
      "description": "Customer information table",
      "category": "TABLE",
      "columns": [
        {
          "name": "customer_id",
          "description": "Unique customer identifier",
          "type": {
            "scalar_type": "INT64"
          },
          "nullable": false,
          "auto_increment": false,
          "is_primarykey": true
        },
        {
          "name": "name",
          "description": "Customer name",
          "type": {
            "scalar_type": "STRING"
          },
          "nullable": false,
          "auto_increment": false
        },
        {
          "name": "location",
          "description": "Geographic location",
          "type": {
            "scalar_type": "GEOGRAPHY"
          },
          "nullable": true,
          "auto_increment": false
        },
        {
          "name": "tags",
          "description": "Customer tags",
          "type": {
            "array_type": {
              "scalar_type": "STRING"
            }
          },
          "nullable": true,
          "auto_increment": false
        }
      ],
      "primary_keys": ["customer_id"],
      "foreign_keys": {
        "fk_customer_order": {
          "column_mapping": {
            "customer_id": "customer_id"
          },
          "foreign_collection": "public.orders"
        }
      }
    }
  ]
}
```

## Versioning & upgrading

The JDBC connector configuration uses a version field to indicate its schema version:

```json
{
  "version": "v2"
  // other configuration properties
}
```

This version field helps the connector understand how to interpret the rest of the configuration. As the connector
evolves, new configuration versions may be introduced to support new features or changes in behavior.

### Configuration versions

The JDBC connector configuration has gone through the following versions:

- **v1**: Initial configuration format that provides the foundation for JDBC connector configuration
- **v2**: Current configuration format that uses jooq SQLDataType for all sources, providing better type handling and
  compatibility across different database systems

### Upgrading configuration

When a new configuration version is available, you can upgrade your existing configuration using the Hasura CLI plugin
command:

```bash
# Upgrade the configuration to the latest version
ddn connector plugin --connector <subgraph>/connector/<connector>/connector.yaml upgrade --config-file /current/config/file/path --outfile /new/config/file/path
```

The upgrade process will automatically convert your configuration to the latest format while preserving your existing
data source connections and schema information. This versioning system allows for future schema evolution while
maintaining backward compatibility.

### What changes during an upgrade

When upgrading your configuration from v1 to v2:

1. Column type handling is improved with jooq SQLDataType for better cross-database compatibility
2. Configuration structure is refactored for better organization of versioned code
3. Type parameters are properly handled for more robust configuration parsing

The upgrade process is designed to be non-destructive, preserving all your existing data source connections and schema
information while enabling access to new features and improved type handling.

## Native queries {#native-queries}

Native queries allow you to use the SQL syntax of the underlying data source to create custom operations and expose them
as models in your application. This is useful for complex queries, stored procedures, or custom functions that you want
to leverage directly in your API.

### Native query structure

A native query is a single SQL statement that returns results and can take arguments. The JDBC connector supports two
methods for defining native queries:

1. **File-based approach** (recommended): Store SQL queries in separate files
2. **Configuration-based approach**: Define queries directly in the `configuration.json` file

### File-based native queries

To create a file-based native query:

1. Create a directory structure for your native operations:

   ```bash
   mkdir -p <subgraph>/connector/<connector>/native_operations/queries/
   ```

2. Create a SQL file with your query, using `{{parameter}}` syntax for parameters:

   ```sql
   -- <subgraph>/connector/<connector>/native_operations/queries/get_customers_by_region.sql
   SELECT * FROM customers
   WHERE region = :region
   AND sales > :min_sales
   ```

3. Register the query using the CLI:

   ```bash
   ddn connector plugin --connector <subgraph>/connector/<connector>/connector.yaml -- \
     native-queries create --operation-path native_operations/queries/get_customers_by_region.sql --name get_customers_by_region
   ```

4. Update your metadata to track the new native query:
   ```bash
   ddn connector-link update <connector_name> --add-all-resources
   ```

### Configuration-based native queries

You can also define native queries directly in the `configuration.json` file:

```json
{
  "version": "v2",
  "connection_uri": {
    "value": "..."
  },
  "schemas": ["public"],
  "tables": [...],
  "functions": [
    {
      "name": "get_customers_by_region",
      "description": "Get customers filtered by region",
      "sql": "SELECT * FROM customers WHERE region = :region AND sales > :min_sales",
      "parameters": [
        {
          "name": "region",
          "description": "Region to filter by",
          "type": {
            "scalar_type": "STRING"
          }
        },
        {
          "name": "min_sales",
          "description": "Minimum sales amount",
          "type": {
            "scalar_type": "INT64"
          }
        }
      ],
      "result_type": {
        "type": "array",
        "element_type": {
          "type": "named",
          "name": "public.customers"
        }
      }
    }
  ]
}
```

### Parameter syntax

The JDBC connector supports colon prefix syntax to specify parameters. This syntax is translated to parameterized
queries, which helps prevent SQL injection.

### Important syntax rules

When writing native queries, follow these rules:

1. **Parameters as scalar values only**: Parameters can only be used in place of scalar values, not table names, column
   names, or other SQL parts
2. **No quoting of string parameters**: Don't add quotes around parameters (use `:name` not `':name'`)
3. **Single statements only**: Each native query should be a single SQL statement without a semicolon at the end
4. **String patterns with concatenation**: For LIKE patterns, use concatenation (e.g., `LIKE '%' || :search || '%'`)
5. **No "hasura\_" prefixed parameters**: Parameter names starting with `hasura_` are reserved

### Result types

The `result_type` field defines the structure of data returned by the native query:

- **Scalar value**: A single value (string, number, boolean, etc.)
- **Array of values**: A list of scalar values or objects
- **Named type**: References an existing table structure
- **Custom object type**: A custom structure defined for the query result

Once defined, native queries are exposed in your application and made available to PromptQL.



==============================



# Troubleshooting

URL: https://hasura.io/docs/promptql/reference/connectors/athena/troubleshooting


## Introduction

For general troubleshooting help, please see these [docs](/data-sources/troubleshooting.mdx).



==============================



# Amazon Redshift

URL: https://hasura.io/docs/promptql/reference/connectors/redshift/

# Amazon Redshift

## Introduction

Hasura DDN includes a Native Data Connector for Amazon Redshift, providing integration with petabyte-scale data
warehouses. This connector allows you to leverage Redshift's powerful OLAP capabilities while taking advantage of
Hasura’s metadata-driven approach. Here, we’ll explore the key features of the Amazon Redshift connector and walk
through the configuration process within a Hasura DDN project.

:::tip Looking to get started?

If you've ended up here and aren't concerned about tweaking your configuration, and rather are looking to get started
with Amazon Redshift and Hasura DDN as quickly as possible, check out our
[Amazon Redshift tutorial](/how-to-build-with-promptql/with-amazon-redshift.mdx) or
[learn how to connect](/data-sources/connect-to-a-source.mdx) to a Amazon Redshift.

:::

## Amazon Redshift docs

- [Connector configuration](/reference/connectors/redshift/configuration.mdx)
- [Troubleshooting](/reference/connectors/redshift/troubleshooting.mdx)



==============================



# Configuration

URL: https://hasura.io/docs/promptql/reference/connectors/redshift/configuration


## Introduction

The `configuration.json` file is generated whenever you introspect a new connector. The file is located in the
`<connector_name>/connector` sub-directory of its parent subgraph.

The configuration is a metadata object that lists all the database entities — such as tables — that the data connector
has to know about in order to serve queries. When your database schema changes you will have to update the configuration
accordingly.

While the `configuration.json` file is generated and populated for you, you can hand-edit sections (such as
[native queries](#native-queries)) to manipulate what resources are available to your application.

## Structure

The configuration object is a JSON object with the following fields:

```json
{
  "version": "v2",
  "connection_uri": {
    "value": "...",
    "variable": "..."
  },
  "schemas": ["public"],
  "tables": [...],
  "functions": [],
}
```

### Property: connection_uri

The connection URI for the datasource. This is a required field that can be specified either as a direct string value or
as a reference to an environment variable:

```json
{
  "connection_uri": {
    "value": "<connection_uri>"
  }
}
```

:::info JDBC Syntax

This construction differs from source to source. Check out [these docs](/data-sources/connect-to-a-source.mdx) for
examples of connection strings for this and other sources.

:::

Or using an environment variable:

```json
{
  "connection_uri": {
    "variable": "JDBC_URL"
  }
}
```

### Property: schemas

This is an optional array of schema names to include in the introspection process. If not provided, all schemas will be
included. **Any schema passed in the JDBC URL will take precedence.**

Example:

```json
{
  "schemas": ["schema1", "schema2"]
}
```

### Property: tables

An array of table definitions generated automatically during introspection. Each table definition includes metadata
about the table structure, columns, primary keys, and foreign keys.

Example:

```json
{
  "tables": [
    {
      "name": "public.customers",
      "description": "Customer information table",
      "category": "TABLE",
      "columns": [
        {
          "name": "customer_id",
          "description": "Unique customer identifier",
          "type": {
            "scalar_type": "INT64"
          },
          "nullable": false,
          "auto_increment": false,
          "is_primarykey": true
        },
        {
          "name": "name",
          "description": "Customer name",
          "type": {
            "scalar_type": "STRING"
          },
          "nullable": false,
          "auto_increment": false
        },
        {
          "name": "location",
          "description": "Geographic location",
          "type": {
            "scalar_type": "GEOGRAPHY"
          },
          "nullable": true,
          "auto_increment": false
        },
        {
          "name": "tags",
          "description": "Customer tags",
          "type": {
            "array_type": {
              "scalar_type": "STRING"
            }
          },
          "nullable": true,
          "auto_increment": false
        }
      ],
      "primary_keys": ["customer_id"],
      "foreign_keys": {
        "fk_customer_order": {
          "column_mapping": {
            "customer_id": "customer_id"
          },
          "foreign_collection": "public.orders"
        }
      }
    }
  ]
}
```

## Versioning & upgrading

The JDBC connector configuration uses a version field to indicate its schema version:

```json
{
  "version": "v2"
  // other configuration properties
}
```

This version field helps the connector understand how to interpret the rest of the configuration. As the connector
evolves, new configuration versions may be introduced to support new features or changes in behavior.

### Configuration versions

The JDBC connector configuration has gone through the following versions:

- **v1**: Initial configuration format that provides the foundation for JDBC connector configuration
- **v2**: Current configuration format that uses jooq SQLDataType for all sources, providing better type handling and
  compatibility across different database systems

### Upgrading configuration

When a new configuration version is available, you can upgrade your existing configuration using the Hasura CLI plugin
command:

```bash
# Upgrade the configuration to the latest version
ddn connector plugin --connector <subgraph>/connector/<connector>/connector.yaml upgrade --config-file /current/config/file/path --outfile /new/config/file/path
```

The upgrade process will automatically convert your configuration to the latest format while preserving your existing
data source connections and schema information. This versioning system allows for future schema evolution while
maintaining backward compatibility.

### What changes during an upgrade

When upgrading your configuration from v1 to v2:

1. Column type handling is improved with jooq SQLDataType for better cross-database compatibility
2. Configuration structure is refactored for better organization of versioned code
3. Type parameters are properly handled for more robust configuration parsing

The upgrade process is designed to be non-destructive, preserving all your existing data source connections and schema
information while enabling access to new features and improved type handling.

## Native queries {#native-queries}

Native queries allow you to use the SQL syntax of the underlying data source to create custom operations and expose them
as models in your application. This is useful for complex queries, stored procedures, or custom functions that you want
to leverage directly in your API.

### Native query structure

A native query is a single SQL statement that returns results and can take arguments. The JDBC connector supports two
methods for defining native queries:

1. **File-based approach** (recommended): Store SQL queries in separate files
2. **Configuration-based approach**: Define queries directly in the `configuration.json` file

### File-based native queries

To create a file-based native query:

1. Create a directory structure for your native operations:

   ```bash
   mkdir -p <subgraph>/connector/<connector>/native_operations/queries/
   ```

2. Create a SQL file with your query, using `{{parameter}}` syntax for parameters:

   ```sql
   -- <subgraph>/connector/<connector>/native_operations/queries/get_customers_by_region.sql
   SELECT * FROM customers
   WHERE region = :region
   AND sales > :min_sales
   ```

3. Register the query using the CLI:

   ```bash
   ddn connector plugin --connector <subgraph>/connector/<connector>/connector.yaml -- \
     native-queries create --operation-path native_operations/queries/get_customers_by_region.sql --name get_customers_by_region
   ```

4. Update your metadata to track the new native query:
   ```bash
   ddn connector-link update <connector_name> --add-all-resources
   ```

### Configuration-based native queries

You can also define native queries directly in the `configuration.json` file:

```json
{
  "version": "v2",
  "connection_uri": {
    "value": "..."
  },
  "schemas": ["public"],
  "tables": [...],
  "functions": [
    {
      "name": "get_customers_by_region",
      "description": "Get customers filtered by region",
      "sql": "SELECT * FROM customers WHERE region = :region AND sales > :min_sales",
      "parameters": [
        {
          "name": "region",
          "description": "Region to filter by",
          "type": {
            "scalar_type": "STRING"
          }
        },
        {
          "name": "min_sales",
          "description": "Minimum sales amount",
          "type": {
            "scalar_type": "INT64"
          }
        }
      ],
      "result_type": {
        "type": "array",
        "element_type": {
          "type": "named",
          "name": "public.customers"
        }
      }
    }
  ]
}
```

### Parameter syntax

The JDBC connector supports colon prefix syntax to specify parameters. This syntax is translated to parameterized
queries, which helps prevent SQL injection.

### Important syntax rules

When writing native queries, follow these rules:

1. **Parameters as scalar values only**: Parameters can only be used in place of scalar values, not table names, column
   names, or other SQL parts
2. **No quoting of string parameters**: Don't add quotes around parameters (use `:name` not `':name'`)
3. **Single statements only**: Each native query should be a single SQL statement without a semicolon at the end
4. **String patterns with concatenation**: For LIKE patterns, use concatenation (e.g., `LIKE '%' || :search || '%'`)
5. **No "hasura\_" prefixed parameters**: Parameter names starting with `hasura_` are reserved

### Result types

The `result_type` field defines the structure of data returned by the native query:

- **Scalar value**: A single value (string, number, boolean, etc.)
- **Array of values**: A list of scalar values or objects
- **Named type**: References an existing table structure
- **Custom object type**: A custom structure defined for the query result

Once defined, native queries are exposed in your application and made available to PromptQL.



==============================



# Troubleshooting

URL: https://hasura.io/docs/promptql/reference/connectors/redshift/troubleshooting


## Introduction

For general troubleshooting help, please see these [docs](/data-sources/troubleshooting.mdx).



==============================



# BigQuery

URL: https://hasura.io/docs/promptql/reference/connectors/bigquery/

# BigQuery

## Introduction

Hasura DDN includes a Native Data Connector for BigQuery, providing integration your fully-managed enterprise data
warehouse. This connector allows you to leverage BigQuery's powerful OLAP capabilities while taking advantage of
Hasura’s metadata-driven approach. Here, we’ll explore the key features of the BigQuery connector and walk through the
configuration process within a Hasura DDN project.

:::tip Looking to get started?

If you've ended up here and aren't concerned about tweaking your configuration, and rather are looking to get started
with BigQuery and Hasura DDN as quickly as possible, check out our
[BigQuery tutorial](/how-to-build-with-promptql/with-bigquery.mdx) or
[learn how to connect](/data-sources/connect-to-a-source.mdx) to a BigQuery.

:::

## BigQuery docs

- [Connector configuration](/reference/connectors/bigquery/configuration.mdx)
- [Troubleshooting](/reference/connectors/bigquery/troubleshooting.mdx)



==============================



# Configuration

URL: https://hasura.io/docs/promptql/reference/connectors/bigquery/configuration


## Introduction

The `configuration.json` file is generated whenever you introspect a new connector. The file is located in the
`<connector_name>/connector` sub-directory of its parent subgraph.

The configuration is a metadata object that lists all the database entities — such as tables — that the data connector
has to know about in order to serve queries. When your database schema changes you will have to update the configuration
accordingly.

While the `configuration.json` file is generated and populated for you, you can hand-edit sections (such as
[native queries](#native-queries)) to manipulate what resources are available to your application.

## Structure

The configuration object is a JSON object with the following fields:

```json
{
  "version": "v2",
  "connection_uri": {
    "value": "...",
    "variable": "..."
  },
  "schemas": ["public"],
  "tables": [...],
  "functions": [],
}
```

### Property: connection_uri

The connection URI for the datasource. This is a required field that can be specified either as a direct string value or
as a reference to an environment variable:

```json
{
  "connection_uri": {
    "value": "<connection_uri>"
  }
}
```

:::info JDBC Syntax

This construction differs from source to source. Check out [these docs](/data-sources/connect-to-a-source.mdx) for
examples of connection strings for this and other sources.

:::

Or using an environment variable:

```json
{
  "connection_uri": {
    "variable": "JDBC_URL"
  }
}
```

### Property: schemas

This is an optional array of schema names to include in the introspection process. If not provided, all schemas will be
included. **Any schema passed in the JDBC URL will take precedence.**

Example:

```json
{
  "schemas": ["schema1", "schema2"]
}
```

### Property: tables

An array of table definitions generated automatically during introspection. Each table definition includes metadata
about the table structure, columns, primary keys, and foreign keys.

Example:

```json
{
  "tables": [
    {
      "name": "public.customers",
      "description": "Customer information table",
      "category": "TABLE",
      "columns": [
        {
          "name": "customer_id",
          "description": "Unique customer identifier",
          "type": {
            "scalar_type": "INT64"
          },
          "nullable": false,
          "auto_increment": false,
          "is_primarykey": true
        },
        {
          "name": "name",
          "description": "Customer name",
          "type": {
            "scalar_type": "STRING"
          },
          "nullable": false,
          "auto_increment": false
        },
        {
          "name": "location",
          "description": "Geographic location",
          "type": {
            "scalar_type": "GEOGRAPHY"
          },
          "nullable": true,
          "auto_increment": false
        },
        {
          "name": "tags",
          "description": "Customer tags",
          "type": {
            "array_type": {
              "scalar_type": "STRING"
            }
          },
          "nullable": true,
          "auto_increment": false
        }
      ],
      "primary_keys": ["customer_id"],
      "foreign_keys": {
        "fk_customer_order": {
          "column_mapping": {
            "customer_id": "customer_id"
          },
          "foreign_collection": "public.orders"
        }
      }
    }
  ]
}
```

## Versioning & upgrading

The JDBC connector configuration uses a version field to indicate its schema version:

```json
{
  "version": "v2"
  // other configuration properties
}
```

This version field helps the connector understand how to interpret the rest of the configuration. As the connector
evolves, new configuration versions may be introduced to support new features or changes in behavior.

### Configuration versions

The JDBC connector configuration has gone through the following versions:

- **v1**: Initial configuration format that provides the foundation for JDBC connector configuration
- **v2**: Current configuration format that uses jooq SQLDataType for all sources, providing better type handling and
  compatibility across different database systems

### Upgrading configuration

When a new configuration version is available, you can upgrade your existing configuration using the Hasura CLI plugin
command:

```bash
# Upgrade the configuration to the latest version
ddn connector plugin --connector <subgraph>/connector/<connector>/connector.yaml upgrade --config-file /current/config/file/path --outfile /new/config/file/path
```

The upgrade process will automatically convert your configuration to the latest format while preserving your existing
data source connections and schema information. This versioning system allows for future schema evolution while
maintaining backward compatibility.

### What changes during an upgrade

When upgrading your configuration from v1 to v2:

1. Column type handling is improved with jooq SQLDataType for better cross-database compatibility
2. Configuration structure is refactored for better organization of versioned code
3. Type parameters are properly handled for more robust configuration parsing

The upgrade process is designed to be non-destructive, preserving all your existing data source connections and schema
information while enabling access to new features and improved type handling.

## Native queries {#native-queries}

Native queries allow you to use the SQL syntax of the underlying data source to create custom operations and expose them
as models in your application. This is useful for complex queries, stored procedures, or custom functions that you want
to leverage directly in your API.

### Native query structure

A native query is a single SQL statement that returns results and can take arguments. The JDBC connector supports two
methods for defining native queries:

1. **File-based approach** (recommended): Store SQL queries in separate files
2. **Configuration-based approach**: Define queries directly in the `configuration.json` file

### File-based native queries

To create a file-based native query:

1. Create a directory structure for your native operations:

   ```bash
   mkdir -p <subgraph>/connector/<connector>/native_operations/queries/
   ```

2. Create a SQL file with your query, using `{{parameter}}` syntax for parameters:

   ```sql
   -- <subgraph>/connector/<connector>/native_operations/queries/get_customers_by_region.sql
   SELECT * FROM customers
   WHERE region = :region
   AND sales > :min_sales
   ```

3. Register the query using the CLI:

   ```bash
   ddn connector plugin --connector <subgraph>/connector/<connector>/connector.yaml -- \
     native-queries create --operation-path native_operations/queries/get_customers_by_region.sql --name get_customers_by_region
   ```

4. Update your metadata to track the new native query:
   ```bash
   ddn connector-link update <connector_name> --add-all-resources
   ```

### Configuration-based native queries

You can also define native queries directly in the `configuration.json` file:

```json
{
  "version": "v2",
  "connection_uri": {
    "value": "..."
  },
  "schemas": ["public"],
  "tables": [...],
  "functions": [
    {
      "name": "get_customers_by_region",
      "description": "Get customers filtered by region",
      "sql": "SELECT * FROM customers WHERE region = :region AND sales > :min_sales",
      "parameters": [
        {
          "name": "region",
          "description": "Region to filter by",
          "type": {
            "scalar_type": "STRING"
          }
        },
        {
          "name": "min_sales",
          "description": "Minimum sales amount",
          "type": {
            "scalar_type": "INT64"
          }
        }
      ],
      "result_type": {
        "type": "array",
        "element_type": {
          "type": "named",
          "name": "public.customers"
        }
      }
    }
  ]
}
```

### Parameter syntax

The JDBC connector supports colon prefix syntax to specify parameters. This syntax is translated to parameterized
queries, which helps prevent SQL injection.

### Important syntax rules

When writing native queries, follow these rules:

1. **Parameters as scalar values only**: Parameters can only be used in place of scalar values, not table names, column
   names, or other SQL parts
2. **No quoting of string parameters**: Don't add quotes around parameters (use `:name` not `':name'`)
3. **Single statements only**: Each native query should be a single SQL statement without a semicolon at the end
4. **String patterns with concatenation**: For LIKE patterns, use concatenation (e.g., `LIKE '%' || :search || '%'`)
5. **No "hasura\_" prefixed parameters**: Parameter names starting with `hasura_` are reserved

### Result types

The `result_type` field defines the structure of data returned by the native query:

- **Scalar value**: A single value (string, number, boolean, etc.)
- **Array of values**: A list of scalar values or objects
- **Named type**: References an existing table structure
- **Custom object type**: A custom structure defined for the query result

Once defined, native queries are exposed in your application and made available to PromptQL.



==============================



# Troubleshooting

URL: https://hasura.io/docs/promptql/reference/connectors/bigquery/troubleshooting


## Introduction

For general troubleshooting help, please see these [docs](/data-sources/troubleshooting.mdx).



==============================



# Databricks

URL: https://hasura.io/docs/promptql/reference/connectors/databricks/

# Databricks

## Introduction

Hasura DDN includes a Native Data Connector for Databricks, providing integration with Databricks warehouses. This
connector allows you to leverage Databricks' powerful OLAP capabilities while taking advantage of Hasura’s
metadata-driven approach. Here, we’ll explore the key features of the Databricks connector and walk through the
configuration process within a Hasura DDN project.

:::tip Looking to get started?

If you've ended up here and aren't concerned about tweaking your configuration, and rather are looking to get started
with Databricks and Hasura DDN as quickly as possible, check out our
[Databricks tutorial](/how-to-build-with-promptql/with-databricks.mdx) or
[learn how to connect](/data-sources/connect-to-a-source.mdx) to a Databricks instance.

:::

## Databricks docs

- [Connector configuration](/reference/connectors/databricks/configuration.mdx)
- [Troubleshooting](/reference/connectors/databricks/troubleshooting.mdx)



==============================



# configuration.mdx

URL: https://hasura.io/docs/promptql/reference/connectors/databricks/configuration

# Configuration Reference


## Introduction

The `configuration.json` file is generated whenever you introspect a new connector. The file is located in the
`<connector_name>/connector` sub-directory of its parent subgraph.

The configuration is a metadata object that lists all the database entities — such as tables — that the data connector
has to know about in order to serve queries. When your database schema changes you will have to update the configuration
accordingly.

While the `configuration.json` file is generated and populated for you, you can hand-edit sections (such as
[native queries](#native-queries)) to manipulate what resources are available to your application.

## Structure

The configuration object is a JSON object with the following fields:

```json
{
  "version": "v2",
  "connection_uri": {
    "value": "...",
    "variable": "..."
  },
  "schemas": ["public"],
  "tables": [...],
  "functions": [],
}
```

### Property: connection_uri

The connection URI for the datasource. This is a required field that can be specified either as a direct string value or
as a reference to an environment variable:

```json
{
  "connection_uri": {
    "value": "<connection_uri>"
  }
}
```

:::info JDBC Syntax

This construction differs from source to source. Check out [these docs](/data-sources/connect-to-a-source.mdx) for
examples of connection strings for this and other sources.

:::

Or using an environment variable:

```json
{
  "connection_uri": {
    "variable": "JDBC_URL"
  }
}
```

### Property: schemas

This is an optional array of schema names to include in the introspection process. If not provided, all schemas will be
included. **Any schema passed in the JDBC URL will take precedence.**

Example:

```json
{
  "schemas": ["schema1", "schema2"]
}
```

### Property: tables

An array of table definitions generated automatically during introspection. Each table definition includes metadata
about the table structure, columns, primary keys, and foreign keys.

Example:

```json
{
  "tables": [
    {
      "name": "public.customers",
      "description": "Customer information table",
      "category": "TABLE",
      "columns": [
        {
          "name": "customer_id",
          "description": "Unique customer identifier",
          "type": {
            "scalar_type": "INT64"
          },
          "nullable": false,
          "auto_increment": false,
          "is_primarykey": true
        },
        {
          "name": "name",
          "description": "Customer name",
          "type": {
            "scalar_type": "STRING"
          },
          "nullable": false,
          "auto_increment": false
        },
        {
          "name": "location",
          "description": "Geographic location",
          "type": {
            "scalar_type": "GEOGRAPHY"
          },
          "nullable": true,
          "auto_increment": false
        },
        {
          "name": "tags",
          "description": "Customer tags",
          "type": {
            "array_type": {
              "scalar_type": "STRING"
            }
          },
          "nullable": true,
          "auto_increment": false
        }
      ],
      "primary_keys": ["customer_id"],
      "foreign_keys": {
        "fk_customer_order": {
          "column_mapping": {
            "customer_id": "customer_id"
          },
          "foreign_collection": "public.orders"
        }
      }
    }
  ]
}
```

## Versioning & upgrading

The JDBC connector configuration uses a version field to indicate its schema version:

```json
{
  "version": "v2"
  // other configuration properties
}
```

This version field helps the connector understand how to interpret the rest of the configuration. As the connector
evolves, new configuration versions may be introduced to support new features or changes in behavior.

### Configuration versions

The JDBC connector configuration has gone through the following versions:

- **v1**: Initial configuration format that provides the foundation for JDBC connector configuration
- **v2**: Current configuration format that uses jooq SQLDataType for all sources, providing better type handling and
  compatibility across different database systems

### Upgrading configuration

When a new configuration version is available, you can upgrade your existing configuration using the Hasura CLI plugin
command:

```bash
# Upgrade the configuration to the latest version
ddn connector plugin --connector <subgraph>/connector/<connector>/connector.yaml upgrade --config-file /current/config/file/path --outfile /new/config/file/path
```

The upgrade process will automatically convert your configuration to the latest format while preserving your existing
data source connections and schema information. This versioning system allows for future schema evolution while
maintaining backward compatibility.

### What changes during an upgrade

When upgrading your configuration from v1 to v2:

1. Column type handling is improved with jooq SQLDataType for better cross-database compatibility
2. Configuration structure is refactored for better organization of versioned code
3. Type parameters are properly handled for more robust configuration parsing

The upgrade process is designed to be non-destructive, preserving all your existing data source connections and schema
information while enabling access to new features and improved type handling.

## Native queries {#native-queries}

Native queries allow you to use the SQL syntax of the underlying data source to create custom operations and expose them
as models in your application. This is useful for complex queries, stored procedures, or custom functions that you want
to leverage directly in your API.

### Native query structure

A native query is a single SQL statement that returns results and can take arguments. The JDBC connector supports two
methods for defining native queries:

1. **File-based approach** (recommended): Store SQL queries in separate files
2. **Configuration-based approach**: Define queries directly in the `configuration.json` file

### File-based native queries

To create a file-based native query:

1. Create a directory structure for your native operations:

   ```bash
   mkdir -p <subgraph>/connector/<connector>/native_operations/queries/
   ```

2. Create a SQL file with your query, using `{{parameter}}` syntax for parameters:

   ```sql
   -- <subgraph>/connector/<connector>/native_operations/queries/get_customers_by_region.sql
   SELECT * FROM customers
   WHERE region = :region
   AND sales > :min_sales
   ```

3. Register the query using the CLI:

   ```bash
   ddn connector plugin --connector <subgraph>/connector/<connector>/connector.yaml -- \
     native-queries create --operation-path native_operations/queries/get_customers_by_region.sql --name get_customers_by_region
   ```

4. Update your metadata to track the new native query:
   ```bash
   ddn connector-link update <connector_name> --add-all-resources
   ```

### Configuration-based native queries

You can also define native queries directly in the `configuration.json` file:

```json
{
  "version": "v2",
  "connection_uri": {
    "value": "..."
  },
  "schemas": ["public"],
  "tables": [...],
  "functions": [
    {
      "name": "get_customers_by_region",
      "description": "Get customers filtered by region",
      "sql": "SELECT * FROM customers WHERE region = :region AND sales > :min_sales",
      "parameters": [
        {
          "name": "region",
          "description": "Region to filter by",
          "type": {
            "scalar_type": "STRING"
          }
        },
        {
          "name": "min_sales",
          "description": "Minimum sales amount",
          "type": {
            "scalar_type": "INT64"
          }
        }
      ],
      "result_type": {
        "type": "array",
        "element_type": {
          "type": "named",
          "name": "public.customers"
        }
      }
    }
  ]
}
```

### Parameter syntax

The JDBC connector supports colon prefix syntax to specify parameters. This syntax is translated to parameterized
queries, which helps prevent SQL injection.

### Important syntax rules

When writing native queries, follow these rules:

1. **Parameters as scalar values only**: Parameters can only be used in place of scalar values, not table names, column
   names, or other SQL parts
2. **No quoting of string parameters**: Don't add quotes around parameters (use `:name` not `':name'`)
3. **Single statements only**: Each native query should be a single SQL statement without a semicolon at the end
4. **String patterns with concatenation**: For LIKE patterns, use concatenation (e.g., `LIKE '%' || :search || '%'`)
5. **No "hasura\_" prefixed parameters**: Parameter names starting with `hasura_` are reserved

### Result types

The `result_type` field defines the structure of data returned by the native query:

- **Scalar value**: A single value (string, number, boolean, etc.)
- **Array of values**: A list of scalar values or objects
- **Named type**: References an existing table structure
- **Custom object type**: A custom structure defined for the query result

Once defined, native queries are exposed in your application and made available to PromptQL.



==============================



# Troubleshooting

URL: https://hasura.io/docs/promptql/reference/connectors/databricks/troubleshooting


## Introduction

For general troubleshooting help, please see these [docs](/data-sources/troubleshooting.mdx).



==============================



# MySQL

URL: https://hasura.io/docs/promptql/reference/connectors/mysql/

# MySQL

## Introduction

Hasura DDN includes a Native Data Connector for MySQL, providing integration with MySQL databases. This connector allows
you to leverage MySQL’s powerful relational database capabilities while taking advantage of Hasura’s metadata-driven
approach. Here, we’ll explore the key features of the MySQL connector and walk through the configuration process within
a Hasura DDN project.

:::tip Looking to get started?

If you've ended up here and aren't concerned about tweaking your configuration, and rather are looking to get started
with MySQL and Hasura DDN as quickly as possible, check out our
[MySQL tutorial](/how-to-build-with-promptql/with-mysql.mdx) or
[learn how to connect](/data-sources/connect-to-a-source.mdx) to a MySQL instance.

:::

## MySQL docs

- [Connector configuration](/reference/connectors/mysql/configuration.mdx)
- [Troubleshooting](/reference/connectors/mysql/troubleshooting.mdx)



==============================



# configuration.mdx

URL: https://hasura.io/docs/promptql/reference/connectors/mysql/configuration

# Configuration Reference


## Introduction

The `configuration.json` file is generated whenever you introspect a new connector. The file is located in the
`<connector_name>/connector` sub-directory of its parent subgraph.

The configuration is a metadata object that lists all the database entities — such as tables — that the data connector
has to know about in order to serve queries. When your database schema changes you will have to update the configuration
accordingly.

While the `configuration.json` file is generated and populated for you, you can hand-edit sections (such as
[native queries](#native-queries)) to manipulate what resources are available to your application.

## Structure

The configuration object is a JSON object with the following fields:

```json
{
  "version": "v2",
  "connection_uri": {
    "value": "...",
    "variable": "..."
  },
  "schemas": ["public"],
  "tables": [...],
  "functions": [],
}
```

### Property: connection_uri

The connection URI for the datasource. This is a required field that can be specified either as a direct string value or
as a reference to an environment variable:

```json
{
  "connection_uri": {
    "value": "<connection_uri>"
  }
}
```

:::info JDBC Syntax

This construction differs from source to source. Check out [these docs](/data-sources/connect-to-a-source.mdx) for
examples of connection strings for this and other sources.

:::

Or using an environment variable:

```json
{
  "connection_uri": {
    "variable": "JDBC_URL"
  }
}
```

### Property: schemas

This is an optional array of schema names to include in the introspection process. If not provided, all schemas will be
included. **Any schema passed in the JDBC URL will take precedence.**

Example:

```json
{
  "schemas": ["schema1", "schema2"]
}
```

### Property: tables

An array of table definitions generated automatically during introspection. Each table definition includes metadata
about the table structure, columns, primary keys, and foreign keys.

Example:

```json
{
  "tables": [
    {
      "name": "public.customers",
      "description": "Customer information table",
      "category": "TABLE",
      "columns": [
        {
          "name": "customer_id",
          "description": "Unique customer identifier",
          "type": {
            "scalar_type": "INT64"
          },
          "nullable": false,
          "auto_increment": false,
          "is_primarykey": true
        },
        {
          "name": "name",
          "description": "Customer name",
          "type": {
            "scalar_type": "STRING"
          },
          "nullable": false,
          "auto_increment": false
        },
        {
          "name": "location",
          "description": "Geographic location",
          "type": {
            "scalar_type": "GEOGRAPHY"
          },
          "nullable": true,
          "auto_increment": false
        },
        {
          "name": "tags",
          "description": "Customer tags",
          "type": {
            "array_type": {
              "scalar_type": "STRING"
            }
          },
          "nullable": true,
          "auto_increment": false
        }
      ],
      "primary_keys": ["customer_id"],
      "foreign_keys": {
        "fk_customer_order": {
          "column_mapping": {
            "customer_id": "customer_id"
          },
          "foreign_collection": "public.orders"
        }
      }
    }
  ]
}
```

## Versioning & upgrading

The JDBC connector configuration uses a version field to indicate its schema version:

```json
{
  "version": "v2"
  // other configuration properties
}
```

This version field helps the connector understand how to interpret the rest of the configuration. As the connector
evolves, new configuration versions may be introduced to support new features or changes in behavior.

### Configuration versions

The JDBC connector configuration has gone through the following versions:

- **v1**: Initial configuration format that provides the foundation for JDBC connector configuration
- **v2**: Current configuration format that uses jooq SQLDataType for all sources, providing better type handling and
  compatibility across different database systems

### Upgrading configuration

When a new configuration version is available, you can upgrade your existing configuration using the Hasura CLI plugin
command:

```bash
# Upgrade the configuration to the latest version
ddn connector plugin --connector <subgraph>/connector/<connector>/connector.yaml upgrade --config-file /current/config/file/path --outfile /new/config/file/path
```

The upgrade process will automatically convert your configuration to the latest format while preserving your existing
data source connections and schema information. This versioning system allows for future schema evolution while
maintaining backward compatibility.

### What changes during an upgrade

When upgrading your configuration from v1 to v2:

1. Column type handling is improved with jooq SQLDataType for better cross-database compatibility
2. Configuration structure is refactored for better organization of versioned code
3. Type parameters are properly handled for more robust configuration parsing

The upgrade process is designed to be non-destructive, preserving all your existing data source connections and schema
information while enabling access to new features and improved type handling.

## Native queries {#native-queries}

Native queries allow you to use the SQL syntax of the underlying data source to create custom operations and expose them
as models in your application. This is useful for complex queries, stored procedures, or custom functions that you want
to leverage directly in your API.

### Native query structure

A native query is a single SQL statement that returns results and can take arguments. The JDBC connector supports two
methods for defining native queries:

1. **File-based approach** (recommended): Store SQL queries in separate files
2. **Configuration-based approach**: Define queries directly in the `configuration.json` file

### File-based native queries

To create a file-based native query:

1. Create a directory structure for your native operations:

   ```bash
   mkdir -p <subgraph>/connector/<connector>/native_operations/queries/
   ```

2. Create a SQL file with your query, using `{{parameter}}` syntax for parameters:

   ```sql
   -- <subgraph>/connector/<connector>/native_operations/queries/get_customers_by_region.sql
   SELECT * FROM customers
   WHERE region = :region
   AND sales > :min_sales
   ```

3. Register the query using the CLI:

   ```bash
   ddn connector plugin --connector <subgraph>/connector/<connector>/connector.yaml -- \
     native-queries create --operation-path native_operations/queries/get_customers_by_region.sql --name get_customers_by_region
   ```

4. Update your metadata to track the new native query:
   ```bash
   ddn connector-link update <connector_name> --add-all-resources
   ```

### Configuration-based native queries

You can also define native queries directly in the `configuration.json` file:

```json
{
  "version": "v2",
  "connection_uri": {
    "value": "..."
  },
  "schemas": ["public"],
  "tables": [...],
  "functions": [
    {
      "name": "get_customers_by_region",
      "description": "Get customers filtered by region",
      "sql": "SELECT * FROM customers WHERE region = :region AND sales > :min_sales",
      "parameters": [
        {
          "name": "region",
          "description": "Region to filter by",
          "type": {
            "scalar_type": "STRING"
          }
        },
        {
          "name": "min_sales",
          "description": "Minimum sales amount",
          "type": {
            "scalar_type": "INT64"
          }
        }
      ],
      "result_type": {
        "type": "array",
        "element_type": {
          "type": "named",
          "name": "public.customers"
        }
      }
    }
  ]
}
```

### Parameter syntax

The JDBC connector supports colon prefix syntax to specify parameters. This syntax is translated to parameterized
queries, which helps prevent SQL injection.

### Important syntax rules

When writing native queries, follow these rules:

1. **Parameters as scalar values only**: Parameters can only be used in place of scalar values, not table names, column
   names, or other SQL parts
2. **No quoting of string parameters**: Don't add quotes around parameters (use `:name` not `':name'`)
3. **Single statements only**: Each native query should be a single SQL statement without a semicolon at the end
4. **String patterns with concatenation**: For LIKE patterns, use concatenation (e.g., `LIKE '%' || :search || '%'`)
5. **No "hasura\_" prefixed parameters**: Parameter names starting with `hasura_` are reserved

### Result types

The `result_type` field defines the structure of data returned by the native query:

- **Scalar value**: A single value (string, number, boolean, etc.)
- **Array of values**: A list of scalar values or objects
- **Named type**: References an existing table structure
- **Custom object type**: A custom structure defined for the query result

Once defined, native queries are exposed in your application and made available to PromptQL.



==============================



# Troubleshooting

URL: https://hasura.io/docs/promptql/reference/connectors/mysql/troubleshooting


## Introduction

For general troubleshooting help, please see these [docs](/data-sources/troubleshooting.mdx).



==============================



# PostgreSQL

URL: https://hasura.io/docs/promptql/reference/connectors/postgresql/

# PostgreSQL for PromptQL

## Introduction

The Native Data Connector for PostgreSQL is our flagship connector for PromptQL. In the sections below, we'll try to
give an overview of the features of the PostgreSQL connector for PromptQL and how to configure it in a Hasura DDN
project.

:::tip Looking to get started?

If you've ended up here and aren't concerned about tweaking your configuration, and rather are looking to get started
with PostgreSQL and Hasura DDN as quickly as possible, check out our
[PostgreSQL tutorial](/how-to-build-with-promptql/with-postgresql.mdx) or
[learn how to connect](/data-sources/connect-to-a-source.mdx) to a PostgreSQL instance.

:::

## PostgreSQL docs

- [Connector configuration](/reference/connectors/postgresql/configuration.mdx)
- [Troubleshooting](/reference/connectors/postgresql/troubleshooting.mdx)



==============================



# Configuration

URL: https://hasura.io/docs/promptql/reference/connectors/postgresql/configuration


## Introduction

The `configuration.json` file is generated whenever you introspect a new connector. The file is located in the
`<connector_name>/connector` sub-directory of its parent subgraph.

The configuration is a metadata object that lists all the database entities — such as tables — that the data connector
has to know about in order to serve queries. When your database schema changes you will have to update the configuration
accordingly.

While the `configuration.json` file is generated and populated for you, you can hand-edit sections (such as
[native queries](#native-queries)) to manipulate what resources are available to your application.

## Structure

The configuration object is a JSON object with the following fields:

```json
{
  "version": "v2",
  "connection_uri": {
    "value": "...",
    "variable": "..."
  },
  "schemas": ["public"],
  "tables": [...],
  "functions": [],
}
```

### Property: connection_uri

The connection URI for the datasource. This is a required field that can be specified either as a direct string value or
as a reference to an environment variable:

```json
{
  "connection_uri": {
    "value": "<connection_uri>"
  }
}
```

:::info JDBC Syntax

This construction differs from source to source. Check out [these docs](/data-sources/connect-to-a-source.mdx) for
examples of connection strings for this and other sources.

:::

Or using an environment variable:

```json
{
  "connection_uri": {
    "variable": "JDBC_URL"
  }
}
```

### Property: schemas

This is an optional array of schema names to include in the introspection process. If not provided, all schemas will be
included. **Any schema passed in the JDBC URL will take precedence.**

Example:

```json
{
  "schemas": ["schema1", "schema2"]
}
```

### Property: tables

An array of table definitions generated automatically during introspection. Each table definition includes metadata
about the table structure, columns, primary keys, and foreign keys.

Example:

```json
{
  "tables": [
    {
      "name": "public.customers",
      "description": "Customer information table",
      "category": "TABLE",
      "columns": [
        {
          "name": "customer_id",
          "description": "Unique customer identifier",
          "type": {
            "scalar_type": "INT64"
          },
          "nullable": false,
          "auto_increment": false,
          "is_primarykey": true
        },
        {
          "name": "name",
          "description": "Customer name",
          "type": {
            "scalar_type": "STRING"
          },
          "nullable": false,
          "auto_increment": false
        },
        {
          "name": "location",
          "description": "Geographic location",
          "type": {
            "scalar_type": "GEOGRAPHY"
          },
          "nullable": true,
          "auto_increment": false
        },
        {
          "name": "tags",
          "description": "Customer tags",
          "type": {
            "array_type": {
              "scalar_type": "STRING"
            }
          },
          "nullable": true,
          "auto_increment": false
        }
      ],
      "primary_keys": ["customer_id"],
      "foreign_keys": {
        "fk_customer_order": {
          "column_mapping": {
            "customer_id": "customer_id"
          },
          "foreign_collection": "public.orders"
        }
      }
    }
  ]
}
```

## Versioning & upgrading

The JDBC connector configuration uses a version field to indicate its schema version:

```json
{
  "version": "v2"
  // other configuration properties
}
```

This version field helps the connector understand how to interpret the rest of the configuration. As the connector
evolves, new configuration versions may be introduced to support new features or changes in behavior.

### Configuration versions

The JDBC connector configuration has gone through the following versions:

- **v1**: Initial configuration format that provides the foundation for JDBC connector configuration
- **v2**: Current configuration format that uses jooq SQLDataType for all sources, providing better type handling and
  compatibility across different database systems

### Upgrading configuration

When a new configuration version is available, you can upgrade your existing configuration using the Hasura CLI plugin
command:

```bash
# Upgrade the configuration to the latest version
ddn connector plugin --connector <subgraph>/connector/<connector>/connector.yaml upgrade --config-file /current/config/file/path --outfile /new/config/file/path
```

The upgrade process will automatically convert your configuration to the latest format while preserving your existing
data source connections and schema information. This versioning system allows for future schema evolution while
maintaining backward compatibility.

### What changes during an upgrade

When upgrading your configuration from v1 to v2:

1. Column type handling is improved with jooq SQLDataType for better cross-database compatibility
2. Configuration structure is refactored for better organization of versioned code
3. Type parameters are properly handled for more robust configuration parsing

The upgrade process is designed to be non-destructive, preserving all your existing data source connections and schema
information while enabling access to new features and improved type handling.

## Native queries {#native-queries}

Native queries allow you to use the SQL syntax of the underlying data source to create custom operations and expose them
as models in your application. This is useful for complex queries, stored procedures, or custom functions that you want
to leverage directly in your API.

### Native query structure

A native query is a single SQL statement that returns results and can take arguments. The JDBC connector supports two
methods for defining native queries:

1. **File-based approach** (recommended): Store SQL queries in separate files
2. **Configuration-based approach**: Define queries directly in the `configuration.json` file

### File-based native queries

To create a file-based native query:

1. Create a directory structure for your native operations:

   ```bash
   mkdir -p <subgraph>/connector/<connector>/native_operations/queries/
   ```

2. Create a SQL file with your query, using `{{parameter}}` syntax for parameters:

   ```sql
   -- <subgraph>/connector/<connector>/native_operations/queries/get_customers_by_region.sql
   SELECT * FROM customers
   WHERE region = :region
   AND sales > :min_sales
   ```

3. Register the query using the CLI:

   ```bash
   ddn connector plugin --connector <subgraph>/connector/<connector>/connector.yaml -- \
     native-queries create --operation-path native_operations/queries/get_customers_by_region.sql --name get_customers_by_region
   ```

4. Update your metadata to track the new native query:
   ```bash
   ddn connector-link update <connector_name> --add-all-resources
   ```

### Configuration-based native queries

You can also define native queries directly in the `configuration.json` file:

```json
{
  "version": "v2",
  "connection_uri": {
    "value": "..."
  },
  "schemas": ["public"],
  "tables": [...],
  "functions": [
    {
      "name": "get_customers_by_region",
      "description": "Get customers filtered by region",
      "sql": "SELECT * FROM customers WHERE region = :region AND sales > :min_sales",
      "parameters": [
        {
          "name": "region",
          "description": "Region to filter by",
          "type": {
            "scalar_type": "STRING"
          }
        },
        {
          "name": "min_sales",
          "description": "Minimum sales amount",
          "type": {
            "scalar_type": "INT64"
          }
        }
      ],
      "result_type": {
        "type": "array",
        "element_type": {
          "type": "named",
          "name": "public.customers"
        }
      }
    }
  ]
}
```

### Parameter syntax

The JDBC connector supports colon prefix syntax to specify parameters. This syntax is translated to parameterized
queries, which helps prevent SQL injection.

### Important syntax rules

When writing native queries, follow these rules:

1. **Parameters as scalar values only**: Parameters can only be used in place of scalar values, not table names, column
   names, or other SQL parts
2. **No quoting of string parameters**: Don't add quotes around parameters (use `:name` not `':name'`)
3. **Single statements only**: Each native query should be a single SQL statement without a semicolon at the end
4. **String patterns with concatenation**: For LIKE patterns, use concatenation (e.g., `LIKE '%' || :search || '%'`)
5. **No "hasura\_" prefixed parameters**: Parameter names starting with `hasura_` are reserved

### Result types

The `result_type` field defines the structure of data returned by the native query:

- **Scalar value**: A single value (string, number, boolean, etc.)
- **Array of values**: A list of scalar values or objects
- **Named type**: References an existing table structure
- **Custom object type**: A custom structure defined for the query result

Once defined, native queries are exposed in your application and made available to PromptQL.



==============================



# Troubleshooting

URL: https://hasura.io/docs/promptql/reference/connectors/postgresql/troubleshooting


## Introduction

For general troubleshooting help, please see these [docs](/data-sources/troubleshooting.mdx).



==============================



# Snowflake

URL: https://hasura.io/docs/promptql/reference/connectors/snowflake/

# Snowflake

## Introduction

Hasura DDN includes a Native Data Connector for Snowflake, providing integration with Snowflake instances. This
connector allows you to leverage Snowflake’s powerful capabilities while taking advantage of Hasura’s metadata-driven
approach. Here, we’ll explore the key features of the Snowflake connector and walk through the configuration process
within a Hasura DDN project.

:::info Important Considerations for Using Hasura DDN with Snowflake

Snowflake is an analytical data source optimized for large-scale data analysis and complex queries. When integrating
Snowflake with Hasura DDN, it’s essential to recognize that the performance of your API will be directly influenced by
the query performance of Snowflake. As a result, response times may vary depending on the complexity and volume of the
queries executed. **Use this connector and Snowflake for scenarios where analytical query performance is acceptable.**

Snowflake is not designed for transactional use cases that require low-latency operations, high-frequency updates, or
real-time responses. **Avoid using Hasura DDN with Snowflake for applications that require rapid transactional
processing.**

:::

## Snowflake docs

- [Connector configuration](/reference/connectors/snowflake/configuration.mdx)
- [Troubleshooting](/reference/connectors/snowflake/troubleshooting.mdx)



==============================



# configuration.mdx

URL: https://hasura.io/docs/promptql/reference/connectors/snowflake/configuration

# Configuration Reference


## Introduction

The `configuration.json` file is generated whenever you introspect a new connector. The file is located in the
`<connector_name>/connector` sub-directory of its parent subgraph.

The configuration is a metadata object that lists all the database entities — such as tables — that the data connector
has to know about in order to serve queries. When your database schema changes you will have to update the configuration
accordingly.

While the `configuration.json` file is generated and populated for you, you can hand-edit sections (such as
[native queries](#native-queries)) to manipulate what resources are available to your application.

## Structure

The configuration object is a JSON object with the following fields:

```json
{
  "version": "v2",
  "connection_uri": {
    "value": "...",
    "variable": "..."
  },
  "schemas": ["public"],
  "tables": [...],
  "functions": [],
}
```

### Property: connection_uri

The connection URI for the datasource. This is a required field that can be specified either as a direct string value or
as a reference to an environment variable:

```json
{
  "connection_uri": {
    "value": "<connection_uri>"
  }
}
```

:::info JDBC Syntax

This construction differs from source to source. Check out [these docs](/data-sources/connect-to-a-source.mdx) for
examples of connection strings for this and other sources.

:::

Or using an environment variable:

```json
{
  "connection_uri": {
    "variable": "JDBC_URL"
  }
}
```

### Property: schemas

This is an optional array of schema names to include in the introspection process. If not provided, all schemas will be
included. **Any schema passed in the JDBC URL will take precedence.**

Example:

```json
{
  "schemas": ["schema1", "schema2"]
}
```

### Property: tables

An array of table definitions generated automatically during introspection. Each table definition includes metadata
about the table structure, columns, primary keys, and foreign keys.

Example:

```json
{
  "tables": [
    {
      "name": "public.customers",
      "description": "Customer information table",
      "category": "TABLE",
      "columns": [
        {
          "name": "customer_id",
          "description": "Unique customer identifier",
          "type": {
            "scalar_type": "INT64"
          },
          "nullable": false,
          "auto_increment": false,
          "is_primarykey": true
        },
        {
          "name": "name",
          "description": "Customer name",
          "type": {
            "scalar_type": "STRING"
          },
          "nullable": false,
          "auto_increment": false
        },
        {
          "name": "location",
          "description": "Geographic location",
          "type": {
            "scalar_type": "GEOGRAPHY"
          },
          "nullable": true,
          "auto_increment": false
        },
        {
          "name": "tags",
          "description": "Customer tags",
          "type": {
            "array_type": {
              "scalar_type": "STRING"
            }
          },
          "nullable": true,
          "auto_increment": false
        }
      ],
      "primary_keys": ["customer_id"],
      "foreign_keys": {
        "fk_customer_order": {
          "column_mapping": {
            "customer_id": "customer_id"
          },
          "foreign_collection": "public.orders"
        }
      }
    }
  ]
}
```

## Versioning & upgrading

The JDBC connector configuration uses a version field to indicate its schema version:

```json
{
  "version": "v2"
  // other configuration properties
}
```

This version field helps the connector understand how to interpret the rest of the configuration. As the connector
evolves, new configuration versions may be introduced to support new features or changes in behavior.

### Configuration versions

The JDBC connector configuration has gone through the following versions:

- **v1**: Initial configuration format that provides the foundation for JDBC connector configuration
- **v2**: Current configuration format that uses jooq SQLDataType for all sources, providing better type handling and
  compatibility across different database systems

### Upgrading configuration

When a new configuration version is available, you can upgrade your existing configuration using the Hasura CLI plugin
command:

```bash
# Upgrade the configuration to the latest version
ddn connector plugin --connector <subgraph>/connector/<connector>/connector.yaml upgrade --config-file /current/config/file/path --outfile /new/config/file/path
```

The upgrade process will automatically convert your configuration to the latest format while preserving your existing
data source connections and schema information. This versioning system allows for future schema evolution while
maintaining backward compatibility.

### What changes during an upgrade

When upgrading your configuration from v1 to v2:

1. Column type handling is improved with jooq SQLDataType for better cross-database compatibility
2. Configuration structure is refactored for better organization of versioned code
3. Type parameters are properly handled for more robust configuration parsing

The upgrade process is designed to be non-destructive, preserving all your existing data source connections and schema
information while enabling access to new features and improved type handling.

## Native queries {#native-queries}

Native queries allow you to use the SQL syntax of the underlying data source to create custom operations and expose them
as models in your application. This is useful for complex queries, stored procedures, or custom functions that you want
to leverage directly in your API.

### Native query structure

A native query is a single SQL statement that returns results and can take arguments. The JDBC connector supports two
methods for defining native queries:

1. **File-based approach** (recommended): Store SQL queries in separate files
2. **Configuration-based approach**: Define queries directly in the `configuration.json` file

### File-based native queries

To create a file-based native query:

1. Create a directory structure for your native operations:

   ```bash
   mkdir -p <subgraph>/connector/<connector>/native_operations/queries/
   ```

2. Create a SQL file with your query, using `{{parameter}}` syntax for parameters:

   ```sql
   -- <subgraph>/connector/<connector>/native_operations/queries/get_customers_by_region.sql
   SELECT * FROM customers
   WHERE region = :region
   AND sales > :min_sales
   ```

3. Register the query using the CLI:

   ```bash
   ddn connector plugin --connector <subgraph>/connector/<connector>/connector.yaml -- \
     native-queries create --operation-path native_operations/queries/get_customers_by_region.sql --name get_customers_by_region
   ```

4. Update your metadata to track the new native query:
   ```bash
   ddn connector-link update <connector_name> --add-all-resources
   ```

### Configuration-based native queries

You can also define native queries directly in the `configuration.json` file:

```json
{
  "version": "v2",
  "connection_uri": {
    "value": "..."
  },
  "schemas": ["public"],
  "tables": [...],
  "functions": [
    {
      "name": "get_customers_by_region",
      "description": "Get customers filtered by region",
      "sql": "SELECT * FROM customers WHERE region = :region AND sales > :min_sales",
      "parameters": [
        {
          "name": "region",
          "description": "Region to filter by",
          "type": {
            "scalar_type": "STRING"
          }
        },
        {
          "name": "min_sales",
          "description": "Minimum sales amount",
          "type": {
            "scalar_type": "INT64"
          }
        }
      ],
      "result_type": {
        "type": "array",
        "element_type": {
          "type": "named",
          "name": "public.customers"
        }
      }
    }
  ]
}
```

### Parameter syntax

The JDBC connector supports colon prefix syntax to specify parameters. This syntax is translated to parameterized
queries, which helps prevent SQL injection.

### Important syntax rules

When writing native queries, follow these rules:

1. **Parameters as scalar values only**: Parameters can only be used in place of scalar values, not table names, column
   names, or other SQL parts
2. **No quoting of string parameters**: Don't add quotes around parameters (use `:name` not `':name'`)
3. **Single statements only**: Each native query should be a single SQL statement without a semicolon at the end
4. **String patterns with concatenation**: For LIKE patterns, use concatenation (e.g., `LIKE '%' || :search || '%'`)
5. **No "hasura\_" prefixed parameters**: Parameter names starting with `hasura_` are reserved

### Result types

The `result_type` field defines the structure of data returned by the native query:

- **Scalar value**: A single value (string, number, boolean, etc.)
- **Array of values**: A list of scalar values or objects
- **Named type**: References an existing table structure
- **Custom object type**: A custom structure defined for the query result

Once defined, native queries are exposed in your application and made available to PromptQL.



==============================



# Troubleshooting

URL: https://hasura.io/docs/promptql/reference/connectors/snowflake/troubleshooting


## Introduction

For general troubleshooting help, please see these [docs](/data-sources/troubleshooting.mdx).



==============================



# Specification

URL: https://hasura.io/docs/promptql/reference/spec


# PromptQL Spec

## Introduction

PromptQL is a novel agent approach to enable high-trust LLM interaction with business data & systems.

Unlike traditional tool calling & RAG approaches that rely on in-context composition, PromptQL's planning engine
generates and runs [programs](#promptql-programs) that compose tool calls and LLM tasks in a way that provides a high
degree of **explainability**, **accuracy** and **repeatability** for arbitrarily complex tasks.

<Thumbnail src="/img/get-started/promptql-agent.png" alt="A PromptQL agent chat instance" width="1000px" />

**Compared to tool calling**

- _~2x improvement in accuracy._
- _Near-perfect repeatability as complexity of task & size of working set increases._
- _Constant context size as data in working set increases._
- _Lower token consumption for complex tasks._

<Grid />

<Charts />

## Challenges with in-context tool chaining

### Challenge #1

Accuracy & repeatability deteriorate as the instruction complexity increases and/or the amount of data in-context
increases.

<Thumbnail src="/img/get-started/challenge-1.png" alt="The first challenge faced by LLMs" className="thumbnail-small" />

### Challenge #2

In-context approaches risk running into hard LLM limitations around input and output token limits. For example, if
retrieved data from a data tool call needs to be passed to a code execution tool, the entire data has to be printed as a
variable into the source code of the program. Or, the response of a data retrieval tool call might result in crossing
the size of the context window.

<Thumbnail
  src="/img/get-started/challenge-2.png"
  alt="The second challenge faced by LLMs"
  className="thumbnail-small"
/>

## Introducing PromptQL

PromptQL's goal is to ensure that AI connected to data and systems can realistically be introduced into business
operations & workflows. This requires a high degree of accuracy and repeatability.

To do this, PromptQL separates the creation of a query plan that describes the interaction with the business data, from
the execution of the query plan.

PromptQL uses the LLM to first create a program to compose the different tool calls. PromptQL has programmatic
primitives for LLM tasks. This allows retrieval, computational and "cognitive" tasks to be arbitrarily composed.

Once the program is created, it is executed in code.

This approach has a few important implications:

It removes input and output data generated during the execution of the plan from the current context. Programmatic
execution of the desired plan makes it deterministic and repeatable. It allows the user to steer the generation of the
plan.

**There are three key components of PromptQL:**

1. [PromptQL programs](#promptql-programs) are Python programs that read & write data via python functions. PromptQL
   programs are generated by LLMs.
2. [PromptQL primitives](#promptql-primitives) are LLM primitives that are available as python functions in the PromptQL
   program to perform common "AI" tasks on data.
3. [PromptQL artifacts](#promptql-artifacts) are stores of data and can be referenced from PromptQL programs. PromptQL
   programs can create artifacts.

A PromptQL **agent** creates and runs PromptQL programs.

### PromptQL Programs

A PromptQL program is a concrete representation of the user's intended interaction with the business data.

```python title="PromptQL program that fetches the last 10 unread emails:"
# Fetch last 10 unread emails
emails = fetch_emails(limit=10, unread=True)

# Calculate average
average = sum(emails) / len(emails)
```

PromptQL programs can read/write data, or search through data by invoking Python functions.

These _tools_ are implemented outside of PromptQL and should simply be provided as dependencies to the PromptQL program.

#### Examples of Tools

1. Search (vector, attribute, keyword, etc.)
2. Reading and writing data from a database
3. Interacting with an API

The most important factors that determine the effectiveness of PromptQL are:

1. The quality of the tools provided to PromptQL.
2. The ability of the PromptQL agent to consistently generate high-quality PromptQL programs.

### PromptQL Primitives

PromptQL primitives are AI functions that are available as python functions in the PromptQL program to perform common AI
tasks on data. These primitives can create structured information from unstructured and structured data and allow the
composition of "cognitive" tasks with "computational" tasks.

For example, a simple search-based RAG system can be represented as a PromptQL program with a retrieval function
followed by a PromptQL primitive that then generates the result.

These are the four basic PromptQL primitives:

- Classify
- Summarize
- Extract
- Visualize

```python title="Example PromptQL program with primitives:"
# A PromptQL program that uses AI to analyze an email

email_text = """From: prince.nigeria@royalfamily.ng
Date: Thu, 14 Mar 2024 15:23:47 +0000
To: recipient@email.com
Subject: URGENT: Your Assistance Required - $25M Inheritance

Dear Beloved Friend,

I am Prince Mohammed Ibrahim, the son of late King Ibrahim of Nigeria. I am writing to request your urgent assistance in transferring the sum of $25,000,000 (Twenty-Five Million United States Dollars) from my father's account to your account.

As the sole heir to the throne and my father's fortune, I need a trusted foreign partner to help move these funds out of the country due to political instability. In return for your assistance, I am prepared to offer you 25% of the total sum.

To proceed, I only require:
1. Your full name
2. Bank account details
3. A small processing fee of $1,000

Please treat this matter with utmost confidentiality and respond urgently.

Best regards,
Prince Mohammed Ibrahim
Royal Family of Nigeria
Tel: +234 801 234 5678
"""

# Extract the sender's email using the extract primitive
json_schema = {
    "type": "object",
    "properties": {
        "sender_email": {
            "type": "string",
            "description": "The email address of the sender"
        }
    }
}

#highlight-start
extracted_info = primitives_extract(
#highlight-end
    json_schema=json_schema,
    instructions="Extract the sender's email address from the email header (From: field)",
    input=email_text
)

# Classify if it's spam
#highlight-start
classification = primitives_classify(
#highlight-end
    instructions="Determine if this email is likely to be spam/scam based on its content, tone, and characteristics",
    inputs_to_classify=[email_text],
    categories=['Likely Spam', 'Legitimate Email'],
    allow_multiple=False
)

# Store results in an artifact
result = [{
    'email_content': email_text,
    'extracted_sender': extracted_info.get('sender_email'),
    'spam_classification': classification[0]
}]

print(result)
```

### PromptQL Artifacts

PromptQL artifacts are stores of data and can be referenced from PromptQL programs. PromptQL programs can create
artifacts. These are the two necessary artifacts for PromptQL:

- Text artifacts
- Table artifacts
- Visualization artifacts

```python title="Example PromptQL program that fetches the last 10 unread emails and stores them in an artifact:"
# Fetch last 10 unread emails
emails = fetch_emails(limit=10, unread=True)

# Create a list with one dictionary containing the emails
result = []
for email in emails:
    result.append({
        'email': email
    })

# Store as an artifact
#highlight-start
store_table_artifact(
#highlight-end
    'emails',
    'Last 10 unread emails',
    'table',
    result
)
```

PromptQL programs can load previously generated artifacts, and this creates a form of _structured memory_ for PromptQL
programs, allowing them to surpass limitations introduced by passing data in context.

```python title="Example PromptQL program with artifacts:"
# Create a function to extract the sender's email address from an email
def get_sender_email(email):
  extracted_info = primitives_extract(
      json_schema=json_schema,
      instructions="Extract the sender's email address from the email header (From: field)",
      input=email
  )
  return extracted_info.get('sender_email')

# Create a function to classify an email as spam/scam
def classify_email(email):
  classification = primitives_classify(
    instructions="Determine if this email is likely to be spam/scam based on its content, tone, and characteristics",
    inputs_to_classify=[email],
    categories=['Likely Spam', 'Legitimate Email'],
    allow_multiple=False
  )
  return classification[0]

#highlight-start
emails = get_table_artifact('emails')
#highlight-end

# For each email, extract the sender's email address and classify if it's spam
result = []
for email in emails:
    result.append({
        'email': email,
        'sender_email': get_sender_email(email),
        'spam_classification': classify_email(email)
    })

# Run an action to mark the email as spam
for email in result:
    if email['spam_classification'] == 'Likely Spam':
        mark_email_as_spam(email['email'])
```



==============================



# Benchmark

URL: https://hasura.io/docs/promptql/reference/benchmark


# Case-study I - PromptQL vs MCP

:::info Work in progress

This case-study will gradually be formalized as a benchmark as we open-source more real-word datasets and user tasks.

**This was last updated on December 13th, 2024.**

:::

## Introduction

In contrast to traditional "in-context" approaches to connect LLMs to data, PromptQL takes a programmatic approach. A
PromptQL agent creates a query plan on the fly to compose retrieval, tool calling and generative tasks.

[Read the PromptQL Specification →](/reference/spec.mdx)

In this post, we compare PromptQL with [Claude.ai + MCP](https://www.anthropic.com/news/model-context-protocol),
connected to the same set of tools.

## 1. Data Domain & Architecture

The benchmark focuses on a customer success domain, integrating data from Zendesk and a user-product transactional
database. This setup simulates a real-world scenario where customer support interactions are linked with product usage
and purchase history, providing a comprehensive view of customer interactions and behavior.

<Architecture />

## 2. Evaluation set

The case-study focuses on interactions derived from 2 user-stories. Both these user stories involve multiple steps of
retrieval, transformation and intelligence. They are intended to represent typical high-value operations with business
data.

1. **A day to day task: Prioritize current open support tickets using a standard operating procedure.**

- Prioritization rules are gradually increased in complexity.
- The set of open tickets used is gradually increased in size.

2. **A planning task: Analyze underlying cause behind slow processing of support tickets.**

- The complexity of selecting which tickets to focus on is gradually increased.
- The set of tickets used is gradually increased in size.

## 3. Observations

We run each prompt five times, and record the accuracy of the result. Each result is a list of data points. In the heat
maps that you see below, columns represent a particular "run" and the rows represent the data points in the output.

<Results />

## 4. Examining failure of in-context approaches

### 4.1 Failure #1: Inability to follow a "query plan"

A query plan here refers to the underlying decision tree that describes the process of how to retrieve, process and
interact with data over multiple steps.

In-context approaches cannot separate the creation of a query plan from the execution of a query plan.

This results in a lack of repeatability. Increasing amount of data and information in the context window deteriorates
accuracy at each step of the query plan and hence further reduces repeatability.

<Gallery />

### 4.2 Failure #2: Natural query plans don't work

In-context approaches require that data is exchanged between tools in context, or processed in context. Natural sounding
query plans often require data to flow between steps with 100% reliability, but this runs against the LLMs natural
limitations, and in particular against output token limits.

<Thumbnail src="/img/get-started/challenge-2.png" alt="A limitation of query plans" className="thumbnail-small" />

<Thumbnail src="/img/get-started/failure2-computation-in-context.png" alt="A failure during computation" />

### 4.3 PromptQL approach: Decouple creation of the query plan from the execution of the query plan

PromptQL's key insight is that intelligence is required to create a query plan, but is not required to execute the query
plan. The execution of the query plan should be mechanical and 100% repeatable.

PromptQL's query plans are composed of steps that are computational and cognitive. These ensures intelligence can be
precisely used only in the isolated context of the step (eg: "extract project_id from the ticket description").

```mermaid
flowchart TD
  subgraph "Create query plan (LLM task)"
    A[Fetch last 20 open tickets]
    B[Extract project_id]
    C[Enrich tickets with more project information]
    D[Fetch ticket comments]
    E[Classify ticket criticality]
    F[Enrich ticket with all the required information]
    G[Run ranking algorithm]

    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
  end

  UI[User input] --> A

  G --> H["Run query plan<br/><i>programmatically</i>"]

  subgraph Inputs
    I[Tools]
    J[LLM]
  end

  I --> H
  J --> H

  H --> R[Result]
```

## 5. Evaluation Methodology

To measure usefulness of introducing an AI system into business processes, we calculate a usefulness score that is a
composite of two key measures:

1. Accuracy: The correctness of the result.

- **Computational accuracy**: The correctness of the result on a task that was possible to solve programmatically
- **Cognitive accuracy**: The correctness of the result on a task needing AI

2. Repeatability: How often is the result the same, across multiple runs of the same prompt.

- **Computational repeatability**: The repeatability of the computational part of the result.
- **Cognitive repeatability**: The repeatability of the AI part of the result.

### 5.1 Computational vs Cognitive

When working with business data and systems, computational accuracy and repeatability are usually more important than
"cognitive" accuracy and repeatability. That is to say, there is higher sensitivity and mistrust when an AI system fails
on an objective task, vs a subjective task.

For example: Imagine working with a colleague or an intern who is a "data access agent". This person can interact with
data for you and take actions based on your instructions. You would provide a lower performance rating to this agent if
they missed data or calculated something incorrectly. However, you would be more forgiving if they made a judgement call
on a subjective task.

### 5.2 A composite usefulness score

We factor these into a formula to calculate a usefulness score:

$$
\text{Usefulness} = (\text{CompAcc}^{w} \times \text{CogAcc}^{1-w}) \times (\text{CompRep}^{w} \times \text{CogRep}^{1-w})
$$

Where:

- $\text{CompAcc} (Computational Accuracy)$
  - A user-assigned score between 0 and 1 indicating how accurately the system handles computational (algorithmic)
    tasks.
- $\text{CogAcc} (Cognitive Accuracy)$
  - A user-assigned score between 0 and 1 indicating the system's accuracy on more interpretive, cognitive tasks.
- $\text{CompRep} (Computational Repeatability)$

$$
\text{CompRep} = \exp\left(-\alpha_C (1 - C)\right)
$$

This transforms the raw computational repeatability measure (C, ranging from 0 to 1) into a value between 0 and 1. A
higher αC means that any deviation from perfect computational repeatability (C=1) is penalized more severely. In plain
English: This score heavily rewards systems that deliver the same computational results every time.

- $\text{CogRep} (Cognitive Repeatability)$

$$
\text{CogRep} = \exp\left(-\alpha_K (1 - K)\right)
$$

Similar to CompRep, this takes a raw cognitive repeatability measure (K, 0 to 1) and maps it to a 0-to-1 scale. Here, αK
controls how harshly deviations from perfect repeatability are penalized for cognitive tasks. In other words, this score
measures how consistently the system provides similar cognitive outputs across multiple runs, typically allowing for a
bit more variation than computational tasks if α<sub>K</sub> is smaller than α<sub>C</sub>.

- $\text{w}$

A weighting factor between 0 and 1 that determines the relative importance of computational versus cognitive factors in
both accuracy and repeatability.

### 5.3 Evaluation parameters

- Humans label accuracy against a known correct answer with a numerical score between 0 and 1.
- Humans label repeatability of relevant outputs across multiple runs, with a numerical score between 0 and 1.
- αc: 8
- αk: 2
- w: 0.7

### 5.4 Prompting and Prompt input

We evaluate the usefulness of PromptQL and Claude.ai + MCP on the same user goal. This allows flexibility in optimizing
the prompt for each system in order to achieve the best possible score. In practice, this often meant that for Claude.ai
the prompt had to stress on ensuring that the input guidelines are followed strictly. For PromptQL, the prompt needed
basic awareness of the available AI primitives.

### 5.5 Evaluation results

<ScoreCompareCharts />

## 6. Coming Up

In the near future, we plan to expand our benchmark to include:

- Additional data domains such as e-commerce and financial services
- Comparison with more AI systems and traditional database querying methods
- Evaluation of data privacy and security aspects

## 7. Contribute & Test

Code and data for this comparison is available at:
[https://github.com/hasura/mcp-vs-promptql](https://github.com/hasura/mcp-vs-promptql).

We welcome contributions to our benchmark efforts. You can access our open-source benchmark toolkit, contribute test
cases, or run the benchmark on your own systems by visiting our
[Hasura Agentic Data Access Benchmark GitHub repository](https://github.com/hasura/business-data-benchmark). Your
participation helps us continually improve and validate the effectiveness of PromptQL and other agentic data access
methods.



==============================



# 100% accuracy

URL: https://hasura.io/docs/promptql/reference/rag-hundred-percent-accuracy


# Achieving 100% Accuracy with RAG with Agentic Planning

## RAG vs RAG + agentic planning

We take 10 hard [questions](#questions) from the
[FRAMES benchmark](https://huggingface.co/datasets/google/frames-benchmark). These questions test the AI system's
ability to handle complex reasoning tasks, and to handle multi-hop retrieval. The knowledge base is Wikipedia articles.

We see that a simple RAG pipeline with Claude 3.5 Sonnet achieves 40% accuracy. On the same retrieval stack, PromptQL
achieves 100% accuracy.

<FRAMESAccuracyChart />

#### The setup

- **Naive RAG**: A simple RAG pipeline with vector search that retrieves the top 5 articles that match the question and
  then provides that to the LLM
- **Agentic RAG**: A vector search tool is provided to the LLM. The tool retrieves the top 5 articles that match a given
  query. The LLM uses the tool to create search queries on its own or as guided by the user.
- **PromptQL**: Same retrieval function as used in the setups above, but instead of retrieving data and processing in
  context, PromptQL uses the LLM to create a plan and then executes it programmatically.

The LLM used in all the 3 setups above is Claude 3.5 Sonnet.

**PromptQL achieves this accuracy by providing a _planning engine_ with a novel approach to processing data in
context.**

PromptQL uses the LLM to author a program representing the overall plan. This program contains data retrieval,
computational and even LLM operations with dynamically created contexts. This decoupling of context-heavy plan
generation from context-limited plan execution allows for indefinite iteration over arbitrarily large or numerous data
items, and allows the task to be solved successfully.

Read the discussions below for more details on the failure modes of RAG and how PromptQL addresses them.

## Benchmark question results {#questions}

<FRAMESQuestionTable />

## Discussion: RAG failure modes

#### Extraction errors in processing

The following question failed with Agentic RAG because of errors in processing the data in context:

> If my future wife has the same first name as the 15th first lady of the United States' mother and her surname is the
> same as the second assassinated president's mother's maiden name, what is my future wife's name?

Correct answer: Jane Ballou

Agentic RAG final response:

> Let me process all this information:
>
> 1. The 15th First Lady was Mary Todd Lincoln, and I need to find her mother's first name
> 2. The second assassinated president was James Garfield, and I need to find his mother's maiden name
>
> From searching through the results:
>
> 1. Mary Todd Lincoln's mother's name Eliza Parker
> 2. James Garfield's mother was Eliza Ballou
>
> Therefore, if your future wife has the first name "Eliza" (like Mary Todd Lincoln's mother) and the surname "Ballou"
> (like James Garfield's mother's maiden name), her name would be Eliza Ballou.
>
> So your future wife's name would be Eliza Ballou.

The problem here is the wrong extraction of the 15th First Lady from the Wikipedia article. Mary Todd Lincoln is
actually the 16th First Lady, while Harriet Lane is the 15th First Lady. This is a common problem with errors in
processing when dealing with large data in-context.

#### Context size limitations

The following question failed on Agentic RAG due to context size limits:

> I have an element in mind and would like you to identify the person it was named after. Here's a clue: The element's
> atomic number is 9 higher than that of an element discovered by the scientist who discovered Zirconium in the same
> year.

Correct answer: Mendelevium is named after Dmitri Mendeleev.

Agentic RAG final response

> Error code: 400 -
> `{'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'prompt is too long: 208600 tokens > 200000 maximum'}}`

Since we are retrieving top-5 chunks per tool call, and there can be multiple tool calls requested in the course of one
question - it’s not uncommon to see context size limit issues.

#### Inexhaustive computation

The following question failed on Agentic RAG because it preemptively responded with a plausible answer instead of
exhaustively verifying the answer.

> A general motors vehicle is named after the largest ward in the country of Monaco. How many people had walked on the
> moon as of the first model year of the vehicle? Note: the model year is not the same as the year the model was first
> produced.

Correct answer: 4

Agentic RAG response:

> By July 1969, when the first 1970 model year Monte Carlo would have been introduced, two people had walked on the
> Moon - Neil Armstrong and Buzz Aldrin who landed on July 20, 1969 as part of the Apollo 11 mission. They were the
> first and only people to have walked on the Moon when the first 1970 model year Monte Carlos went on sale. The Apollo
> 12 mission also happened in 1969 but in-context processing simply skips processing it.

#### Errors in computational reasoning

The following question failed on Agentic RAG because it couldn’t reason and compute the user intent correctly:

> As of July 1, 2024, if I wanted to give my daughter the middle name of the American woman who is the most decorated
> female in the history of American gymnastics as her first name and the full first name of the American woman who holds
> the world record in the 800-meter freestyle as her middle name, what would I name my daughter?

Correct answer: Arianne Kathleen

Agentic RAG answer:

> Based on the search results, I can determine that as of July 1, 2024, the name would be:
>
> The first name would be "Simone", as Simone Biles is clearly the most decorated American female gymnast in history,
> with 32 Olympic and World Championship gold medals among her many accomplishments.
>
> The middle name would be "Katie", as Katie Ledecky is the world record holder in the 800m freestyle with a time of
> 8:04.79, which she set at the 2016 Rio Olympics and still stands as of 2024.
>
> Therefore, the name would be "Simone Katie".

Although the athletes determined are correct, in-context processing fails to put the right computational plan for the
final answer. We needed to get the middle name for Simone Biles and the full first name of Katie Ledecky.

## Discussion: Adding planning to RAG

PromptQL does not process data in-context. It uses the LLM to first create a query plan and then executes the plan in a
programmatic environment. It stores relevant outputs in referenceable artifacts to create "structured memory" across
multiple turns. See the details of PromptQL design here: https://promptql.hasura.io/spec

With this approach, a user can ask PromptQL to create a plan according to some guidelines, or even provide a specific
plan which allows for far more accuracy.

### Planning prompt

Here's an example of how a user can provide a planning prompt to help tackle FRAMES style questions with the given
retrieval stack:

> This problem requires the collection of many facts step by step and creating an algorithm to solve the problem.
> Don{"'"}t rely on your own knowledge of facts, but instead try to collect facts step by step. To collect facts,
> retrieve relevant articles by making search queries. From a retrieved article use the extract function to extract the
> precise fact if its present in the article.
>
> [..question here...]

Here's another example of how a user can even provide a more specific plan to steer the AI agent:

> Find the article that contains the S&P 500 list.
>
> Extract the company name of the alphabetically first ticker from the article.
>
> Then, extract the company name of the alphabetically second to last ticker.
>
> Get articles about these two companies and try to extract the employee count of these companies from those articles if
> present.
>
> Tell me the employee count difference.

### Example

Here's an example of how PromptQL's planner breaks down the retrieval and reasoning into a series of steps to determine
the answer for one of the questions.

<Thumbnail src="/img/get-started/promptql-rag-planner.png" alt="PromptQL planning a RAG sequence" />

[View shared thread](https://promptql.console.hasura.io/share/a1e707cc-ece4-42f4-8c3d-925514d75d9b)

- Read more about PromptQL's design [here](/reference/spec.mdx).
- Add PromptQL to your existing RAG set up [here](/recipes/tutorials/add-vector-search-to-postgresql.mdx).

## Code

The code that was used to try out the different approaches is available
[on github](https://github.com/hasura/rag-benchmark).



==============================



# Overview

URL: https://hasura.io/docs/promptql/help/overview

# Help and Support Overview

## Self-Serve Options

### Explore the docs

Start by exploring our [quickstart guide](/quickstart.mdx) to get up and running with PromptQL or dive into the
[tutorials](/how-to-build-with-promptql/overview.mdx) for a step-by-step approach to connect PromptQL to your data
sources. Once you're comfortable, expand your knowledge with sections like [data sources](data-sources/overview.mdx),
[data modeling](data-modeling/overview.mdx), [auth](auth/overview.mdx), and [deployment](deployment/overview.mdx) to
enhance how PromptQL interacts with your data.

### Ask the community

- We have a forum where you can ask questions and get help from the community and Hasura employees. [Join here](#).
- Additionally, you can create a GitHub issue for any bugs or feature requests
  [here](https://github.com/hasura/promptql/issues).

## Enterprise clients

If you have a paid plan or a dedicated Enterprise contract, you can
[ask the support team for help](https://hasurahelp.zendesk.com/hc/en-us/requests/new).

For details on our Service-Level Agreements (SLAs), version support, data privacy, or submitting feature requests and
bug fixes, please refer to our [policies documentation](/help/policies/index.mdx). This comprehensive resource outlines
everything you need to know about our commitments, processes, and how we handle updates for Enterprise clients.



==============================



# glossary.mdx

URL: https://hasura.io/docs/promptql/help/glossary

# PromptQL and Hasura DDN Glossary

## PromptQL

PromptQL is a novel agent approach to enable high-trust LLM interaction with business data and systems. It composes tool
calls and LLM tasks in a way that provides a high degree of explainability, accuracy, and repeatability for arbitrarily
complex tasks.

Unlike traditional tool calling & RAG approaches that rely on in-context composition, PromptQL's planning engine
generates and runs programs that compose tool calls and LLM tasks, providing:

- ~2x improvement in accuracy over traditional approaches
- Near-perfect repeatability as complexity increases
- Constant context size regardless of data volume
- Detailed execution plans for complete explainability
- Fine-grained authorization to protect sensitive data

PromptQL uses Hasura DDN's data layer which provides out-of-the-box connectors to a wide range of data sources,
introspecting your data and helping you build a high-quality realtime data product with semantic metadata and
fine-grained entitlements.

## Hasura Data Delivery Network (DDN)

Hasura DDN is a managed service that powers PromptQL. It provides the infrastructure needed to connect, unify, and serve
data from multiple sources — securely and at scale.

Hasura DDN delivers a production-ready platform that removes the need to manage your own API or application server. It
handles everything under the hood so that PromptQL can focus on understanding your questions and delivering accurate,
real-time answers.

Hasura DDN is globally-distributed and always available. Its new runtime engine accesses metadata on a per-request
basis, enabling better isolation, high scalability, and faster response times.

The managed control plane includes tools for metadata authoring, CI/CD, infrastructure management, and team
collaboration. This separation of control and data planes (unlike earlier versions, where they were bundled together)
allows you to manage your metadata in version control and deploy confidently—so PromptQL always has the context it needs
to reason across your data sources and respond reliably.

## PromptQL Programs

PromptQL programs are Python programs that read & write data via python functions. They are generated by LLMs and
represent the concrete implementation of a user's intended interaction with their business data. PromptQL programs can
create, read, update, and analyze data across multiple sources.

## PromptQL Primitives

PromptQL primitives are AI functions available as Python functions within PromptQL programs. They perform common AI
tasks on data, such as classification, summarization, and extraction, allowing the composition of cognitive tasks with
computational tasks.

## PromptQL Artifacts

PromptQL artifacts are stores of data that can be referenced from PromptQL programs. PromptQL programs can create
artifacts as structured memory, which helps overcome LLM context window limitations and ensures information consistency
across interactions.

## Query Plans

PromptQL builds dynamic query plans that can be modified during execution based on intermediate results or explicit
instructions from the user. These plans incorporate control structures to handle different scenarios and can adjust
retrieval strategies dynamically, overcoming limitations of rigid retrieval-based approaches.

## Control plane

The control plane manages the configuration, orchestration, and coordination of the data plane elements along with
providing tools to author metadata. It is responsible for setting up and managing the behavior, policies, and rules that
govern how data is processed and forwarded in the data plane. It also oversees the overall behavior of the system,
manages the endpoints, maintains the configurations, handles authentication and access control mechanisms, and gathers
analytics or metrics related to application usage.

## Data Plane

The data plane manages the actual handling of application requests and responses. It deals with the execution of
application operations, the transmission of data between clients and endpoints, and the processing of data payloads. The
following are the main components of a data plane:

**Hasura v3 GraphQL Engine:** The engine takes in an API request (e.g., a question for PromptQL), converts it into an
intermediate representation that the connectors can handle, and creates a plan for the query execution across various
data sources.

**Hasura connectors:** These handle the actual API execution. They accept the intermediate representation from the
engine and use the most efficient mechanism to execute the query and fetch/mutate data from the underlying data source.

## Supergraph

A supergraph is how PromptQL sees the big picture.

It connects multiple subgraphs - each representing a specific data domain - into a single graph that PromptQL can query
across. This lets you organize your data by team or domain, without giving up the ability to ask questions that span
everything.

The supergraph balances the benefits of modular development (independent teams, separate repositories, clear ownership)
with a unified interface for querying. It keeps everything loosely coupled under the hood, but tightly integrated when
you need answers.

PromptQL uses the supergraph to reason across connected domains and return accurate results - even when the data lives
in different systems or is owned by different teams.

[Learn more](https://hasura.io/supergraph).

## Subgraphs

A subgraph is a building block of the supergraph - and the unit PromptQL uses to understand each part of your data.

Each subgraph is a standalone module that includes metadata, permissions, and one or more data connectors. It defines a
clear boundary around a specific data domain, like users, orders, or content. Subgraphs are independently developed,
tested, and versioned - so teams can work in parallel without stepping on each other.

Fields in a subgraph can be scoped or prefixed to avoid conflicts. Relationships between subgraphs let PromptQL follow
connections across domains when answering questions.

## Globals subgraph

When you run the `ddn supergraph init` command, a `globals` subgraph is created by default.

This subgraph holds shared configuration that applies across your entire supergraph - things like API settings, auth
rules, and compatibility options. It gives PromptQL the context it needs to understand how to route, secure, and execute
your conversations correctly.

By default, these objects live in the `globals` subgraph, but you can move them to another subgraph if needed.

## Hasura metadata

Hasura metadata defines the structure of your supergraph; it gives PromptQL the context it needs to ask the right
questions, enforce permissions, and return meaningful answers.

Metadata is written declaratively and describes how to connect to your data sources, what the API should look like, and
how different parts of your system relate to each other. It introduces key modeling constructs such as `Types`,
`Models`, `Commands`, `Permissions`, and `Relationships` to represent your real-world domain in a way PromptQL can
understand.

Beyond that, metadata also includes configuration for API security, caching, CI/CD, and deployment - giving your team
full control over how the application behaves.

[Learn more](/reference/metadata-reference/index.mdx).

## .hml (Hasura Metadata Language)

`.hml` is the file extension used for writing Hasura metadata. It shares the same syntax as `.yaml`, and offers the same
benefits: it's readable, supports nested data structures, allows for inline comments, and is portable across
environments and teams.

PromptQL relies on `.hml` files to understand how your data is structured and how it can be queried and secured.

## Supergraph modeling

Supergraph modeling is the process of defining how your data, operations, and relationships are represented in metadata.

By identifying and structuring the elements of your system - like types, models, permissions, and relationships - you
create a clear, declarative blueprint. PromptQL uses this blueprint to answer questions, enforce rules, and coordinate
across connected data sources.

## Immutable-Build runtime system

Hasura DDN uses an immutable build system for your metadata. Each time you make a change, a new build is created. This
means PromptQL always runs against a specific, versioned snapshot of your supergraph.

There's no shared state to manage, no long-running config to refresh. Every request is served using a clean, isolated
runtime - making rollbacks, previews, and CI/CD workflows straightforward and reliable.

You can confidently ship changes to your API, knowing PromptQL will always use the exact version you deployed - and if
something goes wrong, you can roll back just as easily.

## Data Sources

Any external data source, database, or service that can be connected to Hasura DDN using a Data Connector agent.

[Learn more](/reference/metadata-reference/data-connector-links.mdx).

## Native Data Connector Specification (NDC Spec)

A standardized specification that allows you to extend the functionality of the Hasura server by providing web services
that resolve new sources of data and business logic and help define the metadata structure for APIs in Hasura DDN. The
specification defines types such as `collections`, `functions`, and `procedures` that help in describing the behavior of
agents or connectors that connect to the underlying data source. It provides a framework and guidelines on the types of
web service endpoints that a connector needs to implement.

[Learn more](/reference/metadata-reference/data-connector-links.mdx).

## Native Data Connectors

Data connectors are specialized agents that connect subgraphs to data sources, enabling communication in the data
source's native language.

They integrate Hasura with various external data sources and services based on the native data connector specification.

Each subgraph can have multiple data connectors, and the same data source can be connected to different subgraphs via
separate connector instances - allowing teams to work with the same source from different domain perspectives.

Data connectors are available for a wide variety of sources including databases, business logic functions, REST APIs,
and GraphQL APIs. They can be official Hasura-verified connectors or custom-built connectors to integrate with other
data sources.

[Learn more](/data-sources/overview.mdx).

## Push-Down Capabilities

The ability in Hasura DDN to delegate certain query operations including Authorization to the underlying data source.
This can improve query optimization and performance and is the reason why data connectors in Hasura DDN are called
'native'.

## Connector Hub

Refers to the public site where all Native Data Connectors for Hasura DDN are listed. Users can discover connectors, get
more information about their specific features and find documentation on how to use each connector with Hasura DDN.

[Learn more](https://hasura.io/connectors/).

## Model

A model defines an entity in your API that maps directly to something in your underlying data source - like a table,
collection, or resource exposed by a native data connector.

Models are central to how PromptQL understands your data. Each model includes a reference to its data type, API
configuration, available arguments, and optional global ID settings. Models support select, insert, update, and delete
operations.

Within a select operation, PromptQL can handle filtering, aggregation, pagination, and limiting - giving you full
flexibility when asking questions.

[Learn more](/reference/metadata-reference/models.mdx)

## Command

A command defines a piece of business logic that PromptQL can call as an action. It represents something you can do - a
procedure, mutation, or custom operation - and returns a result.

Commands map directly to functions or procedures defined in your connected data sources. PromptQL uses commands when you
want to go beyond querying and take meaningful action.

[Learn more](/reference/metadata-reference/commands.mdx)

## Relationships

A relationship connects two models - or a model and a command - and tells PromptQL how different pieces of your data are
linked.

With relationships in place, PromptQL can traverse from one part of your system to another, combining data as it answers
questions. Relationships can be defined within a single subgraph or across subgraphs - even if those subgraphs live in
different repositories.

When working across subgraphs, there are a few structural differences to keep in mind. You can read more about those
[here](project-configuration/subgraphs/working-with-multiple-subgraphs.mdx#cross-repo-relationships).

To make working with relationships easier, the
[Hasura VS Code extension](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura) offers auto-complete and
validation as you author metadata.

## Authorization Permissions

Authentication is managed at the supergraph level in Hasura DDN as defined by the `AuthConfig` object and cannot be
customized at the subgraph level.

Authorization however, which is a metadata object in Hasura DDN that defines the access control or authorization rules
on models and commands. [Learn more](/reference/metadata-reference/permissions.mdx), is managed at the subgraph level in
Hasura DDN. This means that the permissions for models and commands are defined within the context of those objects in
their respective subgraphs, and do not affect other subgraphs.

Authorization rules in one subgraph can also be defined to reference data in a foreign subgraph even if that subgraph is
in another repository.

## Collection

A collection is a Native Data Connector Spec object, which encapsulates part of a data source, providing standard
querying capabilities.

Each collection is defined by its name, any collection arguments (need to parametrize the collections), the object type
(a collection of fields) of its rows, and some additional metadata related to constraints.

Tracking a collection results in the creation of a 'model' object in Hasura metadata.

[Learn more](https://hasura.github.io/ndc-spec/specification/schema/collections.html).

## Function

A function is a Native Data Connector Specification object that can be invoked with arguments to get a result, and which
doesn't have side-effects and is thus "read only". Unlike collections, functions do not describe constraints and do not
have object types.

Tracking a function results in the creation of a `Command` Supergraph object in Hasura metadata.

[Learn more](https://hasura.github.io/ndc-spec/specification/schema/functions.html).

## Procedure

A procedure is a native data connector specification object, and it defines an action that the data connector implements
and can mutate data and have other side-effects. Each procedure has arguments and a return type.

Tracking a procedure results in the creation of a `Command` object in Hasura metadata.

[Learn more](https://hasura.github.io/ndc-spec/specification/schema/procedures.html).

## Supergraph config

Supergraph config tells Hasura DDN how to construct your supergraph. A config will contain information such as which
subgraphs to include and which resources to use for the build.

[Learn more](/project-configuration/overview.mdx).

## Connector config

Connector config tells Hasura DDN how to build your connector. It will contain information such as the type of connector
and the location to the context files needed to build the connector.

[Learn more](/project-configuration/overview.mdx).

## DDN CLI (Command-Line Interface)

A tool in Hasura DDN that enables developers to interact with DDN from the command line. It supports various commands
for creating builds, tracking objects, and deploying projects.

[Learn more](/reference/cli/index.mdx).

## VS Code Extension

The Hasura VS Code extension enables features like inline validation of Hasura DDN metadata without having to create
builds, code scaffolding snippets which can be used to quickly scaffold DDN metadata objects, intelligent autocomplete
which shows autocomplete suggestions based on the current state of the DDN project, and other features like
go-to-definition for inter-related DDN metadata objects, documentation on hover of any DDN metadata object, a project
tree on sidebar which shows all DDN projects and metadata objects present in the currently opened folder.

It is strongly recommended to download the VS code extension while working with DDN projects, it can be downloaded from
the [VS Code extension marketplace](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura).

## Metadata build service

Creates builds from the metadata and makes it available to Hasura v3 GraphQL Engine at the edge for it to serve the
application. Provides essential error handling for fast debugging and troubleshooting.

## Control plane cloud API

This component is the underlying cloud service, which enables creating builds, applying builds and testing your API. We
consider this the brain of DDN Console and CLI – the component that drives most of the DX functionalities.

## PromptQL Console and Playground

An interface that provides tools for interacting with PromptQL, visualizing metadata, team collaboration, documentation,
traces, and analytics.

The Playground is your primary method of interacting with and testing conversations before integrating the PromptQL API
into your customer-facing application(s). It allows you to experiment with different queries, view the generated query
plans, and observe how PromptQL interacts with your data sources in real-time.

## Cloud PAT

This refers to a personal authentication token that Hasura Cloud creates automatically for you on every new project
creation. This ensures that your application always has a security mechanism. The auto-generated PAT is included in the
API header `cloud_pat`.



==============================



# Security

URL: https://hasura.io/docs/promptql/help/security

# Security in PromptQL and Hasura DDN

## Introduction

Hasura DDN is designed with a strong emphasis on security, ensuring that your data and APIs are protected through
various measures. This document outlines the key security features and practices implemented within Hasura DDN, covering
data encryption, access control, network security, and authentication.

## Data Encryption

### Encryption at Rest

All metadata stored within Hasura DDN is encrypted at rest using the **AES-GCM-256** algorithm. This industry-standard
encryption ensures that your data is secure even if the underlying storage is compromised. Hasura also uses **Envelope
encryption** for the keys themselves.

### Encryption in Transit

All communication between your client applications, the Hasura DDN engine, and configured data connectors is secured
using **HTTPS/TLS 1.2** or higher. This ensures that your data is encrypted during transmission, protecting it from
interception or tampering.

### Network Connectivity Requirements

When using Hasura public DDN connectors to connect to your data sources, these sources must be accessible from the
internet (allowing connections from `0.0.0.0/0`). This is because Hasura DDN operates from dynamic IP addresses and does
not provide static IP addresses for connections.

If your security requirements necessitate the use of static IP addresses or private network connectivity, consider using
[Hasura Private DDN](/private-ddn/overview.mdx), which allows you to host the data plane within your own infrastructure.

## Metadata Storage and Handling

### Types of Metadata Stored

Hasura DDN's control plane stores metadata related to your project's configuration. This metadata includes:

- **HML (Hasura Metadata Language) files:** These files define the structure of your API, including data source
  information such as table names, column names, relationships, and permissions.
- **Subgraph information:** Details about your project's subgraphs.
- **User access information:** RBAC rules and policies that determines supergraph and subgraph access for users.

:::note

Connection string or data source credentials used by the connectors are managed as secrets.

:::

### Metadata Immutability and Secret Protection

Once a supergraph build is created, the metadata associated with that build, including any resolved values from
environment variables, is immutable and encrypted using envelope encryption. This design allows Private DDNs to use
their own master encryption keys for enhanced security. Critically, **secrets incorporated into the build via
environment variables are not directly exposed or retrievable from the built metadata once on the control plane**.

### Data Retention

Metadata is retained for as long as a build is available on the system. When a build is deleted, the associated metadata
is also deleted. Read more about deleting builds [here](/reference/cli/commands/ddn_supergraph_build_delete.mdx).

## Access Control

Hasura DDN employs a robust role-based access control (RBAC) system. You can define granular permissions to control
which users or roles can access specific data and operations. This ensures that only authorized users can perform
actions based on their assigned roles.

[Learn more about permissions](/data-modeling/permissions.mdx).

## Authentication

Hasura DDN offers flexible authentication options utilising JWT tokens or webhooks, that allow you to integrate with
many auth services or use your own.

### Authentication Modes

Hasura DDN offers the following authentication modes:

- **JWT Mode:** Integrate with popular authentication providers like Auth0, AWS Cognito, Firebase, and Clerk.
- **Webhook Mode:** Use a custom webhook for authentication in unique scenarios.
- **NoAuth Mode:** For testing or development, or for fully public APIs, you can use NoAuth mode, which requires no
  authentication.

### Multi-Factor Authentication (MFA)

Currently, the console does not natively support MFA for login. However, the recommended approach for enterprise clients
is to integrate with a corporate Identity Provider (IDP) via SAML, allowing you to leverage your existing identity and
multi-factor authentication infrastructure, and ensuring a consistent, and secure, access management process.

If you use a corporate email address to sign up, the password you choose is tied only to our system and you can set it
to whatever you'd like.

## Self-Hosted Data Plane

For enhanced security and control, Hasura Private DDN allows the data plane to be hosted within your own infrastructure.
This ensures that the data processing and execution occur within your controlled environment, providing an added layer
of security. [Learn more](/private-ddn/overview.mdx).

## Access Tokens

Hasura DDN provides two types of access tokens for supergraph management: Personal Access Tokens (PATs) and Service
Account Tokens. Service Account Tokens (as apposed to PATs) are recommended for interacting with the control plane,
CI/CD pipelines and automated processes.

### Service Account Tokens

Service Account Tokens are designed for applications, services, or automated processes that need to interact with the
Hasura DDN Control Plane.

- **Usage:** Ideal for continuous integration and continuous deployment (CI/CD) pipelines, automated scripts, and
  backend services.
- **Security:** Associated with a project, not an individual user.
- **Generation:** You can create new service accounts and manage tokens for existing ones via your project's
  `Settings` > `Service Accounts` tab of the console.

### Personal Access Tokens (PATs)

Personal Access Tokens (PATs) are designed for individual users to interact with the Hasura DDN Control Plane. These
tokens are tied to your user account and are ideal for development, testing, and accessing the Hasura Console.

- **Usage:** Primarily used for accessing the Hasura Console and authenticating the DDN CLI during development and
  testing.
- **Security:** Linked to a user account; not to be shared or used in production environments.
- **Generation:** Create and manage PATs through your
  [account settings](https://cloud.hasura.io/account-settings/access-tokens) on the Hasura Cloud dashboard or via the
  DDN CLI with the `ddn auth login` command and `ddn auth print-pat` command.

## Securely Managing Secrets

The PromptQL ecosystem strongly recommends using environment variables to manage sensitive information, such as API
keys, database connection strings, and other credentials. This practice enhances security by keeping secrets out of your
codebase and metadata files.

The platform employs envelope encryption to secure all sensitive information, including secrets and metadata.

### Best Practices

1.  **Use Environment Variables:** Store sensitive information in environment variables rather than hardcoding them
    directly into your configuration files.

2.  **Local Development:** For local development, use `.env` files to manage environment variables. The DDN CLI
    automatically recognizes and loads these files.

3.  **Cloud Deployment:** When deploying to the cloud, use the project's `.env.cloud` file.

4.  **Connector Configuration:** When configuring data connectors, use the `valueFromEnv` option to reference
    environment variables. This ensures that sensitive values are loaded from the environment rather than being stored
    directly in the configuration.

```yaml
# Example of using valueFromEnv in a connector configuration
kind: DataConnectorLink
version: v1
definition:
  name: my_connector
  url:
    readWriteUrls:
      read:
        valueFromEnv: MY_CONNECTOR_READ_URL # URL is stored as an env var
      write:
        valueFromEnv: MY_CONNECTOR_WRITE_URL # URL is stored as an env var
  headers:
    Authorization:
      valueFromEnv: MY_CONNECTOR_AUTHORIZATION_HEADER # API Key/Secret is stored as an env var
  schema: ...
```

5.  **Gitignore:** Ensure that your `.env` files, and any other files that might hold secrets, are added to your
    `.gitignore` file. This will prevent these files from being committed to version control.

## Native Operations and Security

You can extend your application's capabilities using native queries and mutations, providing direct access to the
underlying data source's capabilities. However, this comes with the responsibility of preventing vulnerabilities.

**Important Considerations:**

- **Code Injection:** When using native operations with user-provided input, _you are responsible for preventing code
  injection vulnerabilities_. Carefully sanitize and validate all inputs to avoid potential security risks.
- **Connector-Specific Guidance:** Review the documentation for your specific data connector for detailed information on
  secure usage of native operations.

## Disable APIs

Instead of exposing all APIs in your supergraph, assess your requirements and disable any APIs that are not required for
PromptQL's functionality.

## Need more information?

Check out these common questions, and if you still need help, reach out to us on
[The Hasura Community Forum](https://forum.hasura.io) or through our
[support portal](https://hasurahelp.zendesk.com/hc/en-us/requests/new).

Join the [Hasura Security Announcements](https://groups.google.com/forum/#!forum/hasura-security-announce) group for
emails about security announcements.



==============================



# Hasura policies

URL: https://hasura.io/docs/promptql/help/policies/

# Hasura Policies

This section contains documents and strategies which outline Hasura's operational policies.

- [Version Support Policy](/help/policies/versioning.mdx)



==============================



# versioning.mdx

URL: https://hasura.io/docs/promptql/help/policies/versioning

# Hasura Version Support Policy

## Releases and support

Hasura DDN releases its software via independently versioned components, such as:

- **Engine**
- **Connectors**
- **CLI**

Each of these components is released independently. While they do not have "versioned" releases as a unified product,
their individual versions are critical for ensuring compatibility and understanding what to use and upgrade. Versions
for the CLI and connectors follow semantic versioning:

- **Major versions** may include incompatible or breaking changes.
- **Minor versions** introduce new functionality and bug fixes in a backwards-compatible manner.
- **Patch versions** provide backwards-compatible bug fixes.

:::info calver vs. semver

The **Engine** utilizes [calver (calendar versioning)](https://calver.org/) instead of semver, with versions aligned to
the year, month, and date of the release. This approach provides clarity on the recency of releases.

:::

### When to upgrade a component

Managing independently versioned components offers flexibility but requires careful planning to ensure compatibility and
functionality. Follow these guidelines to decide when and how to upgrade:

1. **Engine**:

   - **When to upgrade**: Upgrade when new features, performance improvements, or critical bug fixes are introduced in
     the release notes.
   - **Compatibility details**: The Engine utilizes a `CompatibilityConfig` object that defines the compatibility
     configuration of the Hasura metadata. Learn more [here](/reference/metadata-reference/compatibility-config.mdx).
   - **Why it matters**: The Engine is the core of your GraphQL API, and updates may introduce new capabilities or
     optimizations.

2. **Connectors**:

   - **When to upgrade**: Upgrade when your current connector version does not support features or updates in the
     Engine. Verify compatibility using the connector release notes.
   - **Why it matters**: Connectors ensure seamless integration with your data sources, and outdated versions may lead
     to errors or limited functionality.

3. **CLI**:
   - **When to upgrade**: Upgrade when new commands, workflows, or fixes are introduced, or when compatibility with the
     Engine or connectors is required.
   - **Why it matters**: The CLI is your primary tool for automation and deployment. Staying up-to-date ensures you can
     use all supported features effectively.

#### Example: Coordinating component upgrades

Imagine the Engine releases a new feature in version `2024.07.10`. Before upgrading, check the release notes for:

- Supported connectors for this version of the Engine.
- Required CLI versions to use the new Engine capabilities.

**Step-by-step upgrade process:**

1. Review the [Hasura Changelog](https://hasura.io/changelog) for details about the new Engine version.
2. Cross-check connector and CLI release notes to confirm compatibility with the new Engine version.
3. Upgrade the Engine first, followed by connectors and CLI, in that order, to maintain compatibility and minimize
   disruptions.

## Reporting bugs and requesting fixes

Hasura is committed to providing enterprise-grade reliability and support to ensure your business operates seamlessly.
For bug reporting and resolution, the following process ensures efficiency and transparency:

1. **Reporting Bugs**:

   - Bugs can be reported through the [Hasura Support Portal](https://support.hasura.io/hc/en-us/requests/new).
   - Customers are encouraged to provide detailed descriptions, logs, and replication steps to help expedite resolution.

2. **Bug Triage and Prioritization**:

   - Reported issues are triaged based on severity and impact, prioritizing critical bugs that affect system security,
     availability, or functionality.
   - Issues impacting production environments are escalated for immediate attention.

3. **Bug Fix Releases**:
   - Fixes for critical issues are rolled out as patch updates. Non-critical issues are included in the subsequent minor
     or major releases.
   - Customers are notified through release notes, the [Hasura Changelog](https://hasura.io/changelog), and direct
     communication for mission-critical updates.

## Security updates and communication

Security is a top priority for Hasura. To maintain the integrity of your systems, we adhere to the following practices:

1. **Proactive Security Patching**:

   - Vulnerabilities are addressed promptly with targeted patch updates.
   - Security patches are backward-compatible and applied to supported versions.

2. **Customer Communication**:

   - Security updates are communicated directly to enterprise customers through our Customer Success team and email
     notifications.
   - Detailed documentation and guidance are provided to ensure smooth implementation of critical updates.

## Rolling out updates

Hasura ensures a seamless update experience for enterprise customers by following these principles:

1. **Pre-Release Testing**:

   - Updates are rigorously tested across supported versions and environments to minimize disruption.

2. **Documentation and Support**:
   - Comprehensive release notes and upgrade guides accompany every update.
   - Our support team is available to assist with planning and executing updates, ensuring minimal impact on operations.



==============================



